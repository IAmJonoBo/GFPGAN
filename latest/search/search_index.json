{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GFPGAN (Fork) \u2014 Documentation","text":"<p>Note: We are introducing the new Restoria CLI (<code>restoria</code>) as the primary entry point going forward. Legacy commands remain available during the transition. See the migration guide: guides/migration.md.</p> <p>Welcome to the fork documentation. This fork focuses on modern developer ergonomics, a smoother Colab experience, and practical inference.</p> <ul> <li>What\u2019s new vs upstream:</li> <li>Modern CI and linting</li> <li>Colab with interactive UI and compatibility fixes (BasicSR master + modern torchvision)</li> <li>CLI quality-of-life flags and console entrypoint</li> <li>Safer repository defaults and optional light tests</li> </ul> <p>Quick links</p> <ul> <li>Getting started</li> <li>Quickstart: getting-started/quickstart.md</li> <li>Troubleshooting: troubleshooting.md</li> <li>Reference</li> <li>CLI Usage: usage/cli.md</li> <li>Backend Matrix: BACKEND_MATRIX.md</li> <li>Compatibility: COMPATIBILITY.md</li> <li>Contribute</li> <li>Contributing: governance/contributing.md</li> </ul> <p>Upstream reference: https://github.com/TencentARC/GFPGAN</p>"},{"location":"#quick-tiles","title":"Quick tiles","text":"<ul> <li> <p>:material-rocket: Quickstart \u2014 Start here to install and   restore your first image. [:material-arrow-right: Open quickstart]   (getting-started/quickstart.md)</p> </li> <li> <p>:material-console: CLI usage \u2014 Learn the <code>restoria</code> and <code>gfpup</code>   commands, flags, and outputs. [:material-arrow-right: Explore CLI]   (usage/cli.md)</p> </li> <li> <p>:material-book-open-variant: User guides \u2014 Deep dives: choose a   backend, metrics, performance, and troubleshooting.   [:material-arrow-right: Browse guides] (guides/choose-backend.md)</p> </li> <li> <p>:material-api: API \u2014 Run the FastAPI server or integrate via   Python APIs. [:material-arrow-right: REST API] (usage/api.md)</p> </li> </ul>"},{"location":"ACCESSIBILITY/","title":"Accessibility (WCAG 2.2 AA)","text":"<ul> <li>Visible focus outlines with adequate contrast</li> <li>Keyboard-first navigation and shortcuts</li> <li>Before/after slider must be usable without a mouse</li> <li>Motion kept subtle (ease-out 160\u2013220ms)</li> <li>Dark/light themes with system preference sync</li> </ul>"},{"location":"BACKEND_MATRIX/","title":"Backend Matrix","text":"Engine Torch 2.x ONNX Runtime TensorRT MPS Notes GFPGAN \u2713 planned planned \u2713 Baseline CodeFormer \u2713 (extra) planned planned \u2713 Optional RestoreFormer++ \u2713 (extra) planned planned \u2713 Optional Ensemble \u2713 \u2013 \u2013 \u2713 Weighted blend Guided \u2713 \u2013 \u2013 \u2713 Reference\u2011aware DiffBIR EXPERIMENTAL \u2013 \u2013 \u2013 Heavy HYPIR EXPERIMENTAL \u2013 \u2013 \u2013 Heavy <p>Background upsamplers:</p> Upsampler Torch ONNX Notes RealESRGAN \u2713 \u2013 Default on CUDA SwinIR planned \u2013 Optional"},{"location":"BACKEND_MATRIX/#orchestrator-routing-signals","title":"Orchestrator Routing Signals","text":"<p>When <code>--auto-backend</code> is enabled, routing currently considers:</p> <ul> <li>NIQE / BRISQUE (no\u2011ref quality) thresholds:</li> <li><code>few_artifacts</code>: niqe &lt; 7.5 or brisque &lt; 35 \u2192 stay on GFPGAN (weight preserved)</li> <li><code>heavy_degradation</code>: niqe \u2265 12 or brisque \u2265 55 \u2192 switch to CodeFormer     (weight \u2265 0.6)</li> <li><code>moderate_degradation</code>: values in between \u2192 GFPGAN with standardized weight 0.6</li> <li>Face stats (if detector available): recorded (<code>face_count</code>, size stats)   but not yet influencing routing (planned).</li> </ul> <p>All signals and applied rule are embedded per image under <code>plan.quality</code>, <code>plan.faces</code>, and <code>plan.detail.routing_rules</code> in the manifest.</p>"},{"location":"COMPATIBILITY/","title":"Compatibility Matrix (Fork)","text":"<p>This fork targets a smooth developer and Colab experience with modern Torch/Torchvision while remaining compatible with upstream behavior where practical.</p> <ul> <li>Python 3.10</li> <li>Preferred: Torch 1.x track (<code>-E torch1</code>)</li> <li>Example pins: <code>torch==1.13.1</code>, <code>torchvision==0.14.1</code></li> <li>Python 3.11</li> <li>Preferred: Torch 2.x track (<code>-E torch2</code>)</li> <li>Example range: <code>torch&gt;=2.2,&lt;3</code>, <code>torchvision&gt;=0.17,&lt;0.20</code></li> <li>Basicsr</li> <li>Colab and CI install Basicsr from <code>master</code> to match modern <code>torchvision</code> functional API.</li> <li>On older Torch/Torchvision (1.x), the PyPI Basicsr release is typically fine.</li> </ul> <p>Notes - Colab installs: Torch defaults to CPU wheels (GPU optional via CUDA 12.1 index); Basicsr is installed from <code>master</code> for compatibility. - Real-ESRGAN background upsampling is disabled on CPU in this fork for performance\u2014use GPU runtime in Colab for best results. - If you need fully deterministic runs, use <code>--seed</code> and prefer CPU (or set CUDA deterministic/env flags).</p>"},{"location":"DECISION_TABLE/","title":"Engine Decision Table","text":"<p>This table summarizes when to prefer each engine and the trade\u2011offs involved.</p> <ul> <li>GFPGAN (default)</li> <li>Strengths: Natural outputs, good identity retention on most inputs, fast with Torch 2.x and <code>torch.compile</code>.</li> <li>Use when: Mixed quality inputs, portraits, multi\u2011face scenes.</li> <li> <p>Notes: Background upsampling via RealESRGAN improves whole\u2011image perception.</p> </li> <li> <p>CodeFormer</p> </li> <li>Strengths: Robust on severely degraded faces (blur, compression); fidelity control from 0..1.</li> <li>Use when: Very low\u2011quality inputs, surveillance\u2011like frames.</li> <li> <p>Notes: Higher fidelity values can preserve more original structure at the cost of fine detail.</p> </li> <li> <p>RestoreFormer / RestoreFormer++</p> </li> <li>Strengths: Identity\u2011faithful restoration; stable on higher\u2011quality inputs.</li> <li>Use when: Faces are mostly sharp; you want minimal identity drift.</li> <li> <p>Notes: \"++\" variant uses updated weights when available.</p> </li> <li> <p>GFPGAN (ONNX Runtime)</p> </li> <li>Strengths: Portable runtime; can leverage CUDA/TensorRT/DirectML/CoreML providers.</li> <li>Use when: You need deployment flexibility or lower latency with hardware\u2011accelerated EPs.</li> <li> <p>Notes: Provide <code>--model-path-onnx</code> or JobSpec <code>model_path_onnx</code>.</p> </li> <li> <p>DiffBIR / HYPIR (experimental)</p> </li> <li>Strengths: Diffusion\u2011prior and heavy models for challenging scenes.</li> <li>Use when: Research or advanced users; expect longer runtimes.</li> <li>Notes: Behind feature flags; not included in CI.</li> </ul> <p>Trade\u2011offs - Identity vs Detail: CodeFormer (with fidelity) and RestoreFormer++ bias toward identity; GFPGAN often yields more natural detail. - Speed vs Quality: Use the Quality preset to map tiling/precision; ORT providers can improve latency. The optimizer can evaluate several weights and pick the best under a time budget.</p> <p>Metrics - ArcFace cosine (identity): Higher is better; used for identity lock and optimizer. - LPIPS/DISTS (perceptual): Lower is better; optional in fast/full metrics modes. - NIQE/BRISQUE (no\u2011ref): Optional for auto backend heuristics.</p>"},{"location":"DEV_ENV/","title":"Development Environment","text":"<p>Development Environment (uv)</p> <p>Overview - Uses uv for fast, reproducible Python environments and locking. - Default target: Python 3.10 with a CPU-only stack compatible with BasicSR 1.4.2.</p> <p>Prereqs - Install uv: macOS <code>brew install uv</code>, Linux <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code>. - Python 3.10 recommended. 3.11+ is experimental (see below).</p> <p>Quick Start (3.11, Torch 2.x default) 1) Sync deps (with dev tools):    - <code>scripts/setup_uv.sh --python 3.11 --track torch2</code> 2) Run tests:    - <code>scripts/test.sh</code>    - Optional progress during tests:      - Console: <code>scripts/test.sh --progress-console</code>      - JSONL file: <code>scripts/test.sh --progress-log .pytest-progress.jsonl</code> 3) Lint / format:    - <code>scripts/lint.sh</code>    - <code>scripts/fmt.sh</code></p> <p>Notes - The runtime depends on <code>torch</code> and <code>torchvision</code>. For compatibility with <code>basicsr==1.4.2</code>, we supply a Torch 1.x extra (<code>torch1</code>) and version markers to keep NumPy/Skimage/OpenCV in a compatible range for Python 3.10. - If you install via pip directly, you can use the constraints file: <code>pip install -r requirements.txt -c constraints-3.10-compat.txt</code>.</p> <p>Python 3.10 (Torch 1.x track; stable) - For maximum compatibility with <code>basicsr==1.4.2</code> and CPU-only usage:   - <code>scripts/setup_uv.sh --python 3.10 --track torch1</code>   - Or with pip + constraints: <code>pip install -r requirements.txt -c constraints-3.10-compat.txt</code></p> <p>Torch 2.x notes - Torch 2.x (3.11+) may require Basicsr master:   - <code>uv pip install --no-cache-dir --upgrade \"git+https://github.com/xinntao/BasicSR@master\"</code>   - Alternatively use <code>constraints-3.11-experimental.txt</code> with pip.</p> <p>Apple Silicon - All constraints are chosen to have prebuilt wheels on macOS arm64 where possible. If you hit build issues, ensure you\u2019re on Python 3.10 and use the 3.10 constraints.</p> <p>Metrics extras (BRISQUE availability) - The <code>metrics</code> extra installs perceptual metrics (LPIPS, DISTS, PIQ). BRISQUE is best-effort:   - Linux: <code>imquality[brisque]</code> is enabled.   - Windows: <code>pybrisque</code> is enabled.   - macOS: BRISQUE is not installed by default to avoid resolver/build issues on newer Python tracks. The code gracefully skips BRISQUE if unavailable.</p> <p>Headless environments (servers/CI) - For containerized or server environments where GUI libs are unnecessary, consider installing <code>opencv-python-headless</code> instead of <code>opencv-python</code>. This reduces GUI-related dependencies while keeping functionality required by the CLI and metrics.</p>"},{"location":"HARDWARE_GUIDE/","title":"Hardware &amp; Performance Guide","text":"<ul> <li>CUDA GPUs (recommended): enables RealESRGAN background upsampling and torch.compile.</li> <li>Apple Silicon (MPS): GFPGAN works; ONNX CoreML EP planned.</li> <li>CPU-only: Works for small images; disable background upsampling for speed.</li> </ul> <p>Tips: - Use <code>--compile default</code> to JIT-compile models on Torch 2.x. - Adjust tile size and precision (<code>fp16</code>) based on VRAM. - Set <code>--seed</code> and <code>--deterministic</code> for reproducible runs.</p>"},{"location":"ORT_GUIDE/","title":"ONNX Runtime Guide (GFPP)","text":"<p>This guide explains how to run GFPGAN with ONNX Runtime.</p> <ul> <li>Install ORT:</li> <li>CPU: <code>pip install onnxruntime</code></li> <li> <p>CUDA/TensorRT: <code>pip install onnxruntime-gpu</code> (and ensure CUDA/TensorRT installed)</p> </li> <li> <p>Export ONNX graph (outline):</p> </li> <li>The CLI has a stub:     <code>gfpup export-onnx --version 1.4 --model-path /path/GFPGANv1.4.pth --out gfpgan.onnx</code>     (alias: <code>--output gfpgan.onnx</code>)</li> <li> <p>Implement export by initializing <code>restorer.gfpgan</code> and calling <code>torch.onnx.export</code>     with a 512x512 BCHW normalized dummy input. Validate with ORT.</p> </li> <li> <p>Run ORT backend:</p> </li> <li>CLI: <code>gfpup run --input inputs/whole_imgs --backend gfpgan-ort --model-path-onnx     /path/gfpgan.onnx --output out/</code></li> <li> <p>API: pass <code>{\\\"backend\\\":\\\"gfpgan-ort\\\",\\\"model_path_onnx\\\":\\\"/path/gfpgan.onnx\\\"}</code>     in the JobSpec.</p> </li> <li> <p>Providers:</p> </li> <li> <p>The runtime logs available providers and the selected provider; metrics include     <code>ort_provider</code> and <code>ort_init_sec</code>.</p> </li> <li> <p>Fallback:</p> </li> <li>If ORT initialization or inference is not available, the system falls back to     the Torch backend transparently and records <code>backend: onnxruntime+torch-fallback</code>.</li> </ul>"},{"location":"TORCH2/","title":"Torch 2 Notes","text":"<p>Torch 2.x Migration Path (Experimental)</p> <p>Goals - Support Python 3.11+ and Torch/Torchvision 2.x while keeping the stable 3.10 + Torch 1.x path intact.</p> <p>Status - GFPGAN code imports from <code>torchvision.ops</code> and <code>torchvision.transforms.functional</code>, which are compatible with 0.17\u20130.19. - Known blocker was in <code>basicsr==1.4.2</code> using <code>torchvision.transforms.functional_tensor</code> removed in torchvision 0.15+. Upstream master addresses this.</p> <p>How to Try 1) Create a uv env for Python 3.11 with Torch 2.x extra:    - <code>uv sync -p 3.11 -E dev -E torch2</code> 2) If you hit Basicsr import errors, install master:    - <code>uv pip install --no-cache-dir --upgrade \"git+https://github.com/xinntao/BasicSR@master\"</code> 3) Run tests:    - <code>uv run pytest -q</code></p> <p>Notes - Some torchvision ops (e.g., <code>roi_align</code>) rely on compiled C++/CUDA ops. CPU wheels include C++ kernels; CUDA requires matching CUDA wheels. - If you need CUDA, install the appropriate Torch/Torchvision CUDA wheels and re-sync the environment. - If downstream regressions appear (numerics, tolerance), we can add conditional branches or loosen test tolerances for the Torch 2.x track.</p> <p>Next Steps (if adopting Torch 2.x by default) - Change the default scripts to <code>--track torch2</code> and set CI matrix to Python 3.11. - Remove Torch 1.x constraints when Basicsr master (or a tagged release) is required and stable.</p>"},{"location":"UI_GUIDE/","title":"UI Guide (GFPP Web)","text":"<p>The web app (Next.js 15 + React 19) provides a one-screen flow:</p> <ul> <li>Controls: Backend (Torch/ORT/CodeFormer/RF++), ONNX model path (for ORT), Preset, Metrics, Background, Quality, Auto Backend, Identity Lock.</li> <li>Submit: \"Submit Dry-Run Job\" sends a job to the local API and opens a WebSocket stream for progress.</li> <li>Queue: shows job list with progress bars, a ZIP download link when finished, and a \"Re-run (dry)\" button.</li> <li>Results: a gallery of before/after comparisons and per-image metrics cards. Identity lock retries display a green badge.</li> <li>Per-image Re-run: \"Re-run this (dry)\" submits a rerun for that specific input using the current controls.</li> </ul> <p>Dev setup:</p> <pre><code>uvicorn services.api.main:app --reload --port 3000\ncd apps/web &amp;&amp; pnpm i &amp;&amp; pnpm dev\n</code></pre> <p>Images are proxied via <code>/file?path=</code> for convenience in dev; in production, serve static files via a proper file server.</p> <p>Feedback (optional):</p> <ul> <li>The local Gradio demo supports lightweight feedback logging to     <code>results/feedback.jsonl</code> when enabled.</li> <li>Enable by setting an environment variable before launch:</li> </ul> <pre><code>export GFPP_FEEDBACK=1\npython -m gfpgan.gradio_app\n</code></pre>"},{"location":"faq/","title":"FAQ","text":"<ul> <li>Which Python/Torch should I use?</li> <li> <p>Python 3.11 with Torch 2.x is recommended. See docs/COMPATIBILITY.md for details.</p> </li> <li> <p>Why is background upsampling disabled on CPU?</p> </li> <li> <p>Real-ESRGAN is heavy on CPU and often slower than desired; this fork disables it by default on CPU for a smoother UX. Use GPU for best results.</p> </li> <li> <p>How can I run inference without downloads?</p> </li> <li> <p>Use <code>--no-download</code> and pass <code>--model-path /path/to/GFPGANv1.4.pth</code>.</p> </li> <li> <p>How do I quickly validate the CLI works locally?</p> </li> <li> <p><code>gfpgan-infer --dry-run -v 1.4</code> parses arguments and exits.</p> </li> <li> <p>Where is the docs site?</p> </li> <li>Once enabled via GitHub Pages on gh-pages, the site will be available at the repo Pages URL.</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<ul> <li>Basicsr import error with <code>functional_tensor</code></li> <li>Symptom: <code>ModuleNotFoundError: No module named 'torchvision.transforms.functional_tensor'</code></li> <li> <p>Fix: Install Basicsr from master (works with modern torchvision)</p> <ul> <li>Pip: <code>pip uninstall -y basicsr &amp;&amp; pip install --no-cache-dir --force-reinstall \"git+https://github.com/xinntao/BasicSR@master\"</code></li> <li>Colab: already handled in the install cell.</li> </ul> </li> <li> <p>Colab is slow or runs out of memory</p> </li> <li>Use a GPU runtime (Runtime \u2192 Change runtime type \u2192 GPU)</li> <li>Keep background upsampling disabled on CPU (default in this fork)</li> <li> <p>Reduce <code>--upscale</code> or batch size (fewer/lighter images)</p> </li> <li> <p>Heavy installs on CI/Colab</p> </li> <li>Prefer pinned Python (3.11) with Torch 2.x track</li> <li> <p>The tests (light) job avoids heavyweight tests and validates CLI and imports</p> </li> <li> <p>Deterministic results</p> </li> <li>Use <code>--seed</code> for reproducibility (random, numpy, torch)</li> <li> <p>Consider CPU for stricter determinism (or set CUDA deterministic options)</p> </li> <li> <p>Missing model weights</p> </li> <li>Use <code>--model-path</code> to point to local weights</li> <li> <p>Or remove <code>--no-download</code> to allow fetching from the release URLs</p> </li> <li> <p>NIQE/BRISQUE not showing up</p> </li> <li>These are optional no\u2011reference metrics; if their dependencies aren\u2019t installed, the run will continue and values will be omitted or null.</li> <li>To enable: install the <code>metrics</code> extra or the specific libraries used by your environment.</li> <li> <p>Dry\u2011run computes NIQE/BRISQUE too when <code>--metrics</code> is enabled, which helps validate setup without model weights.</p> </li> <li> <p>OpenCV import errors</p> </li> <li>The CLI and tests fall back to PIL for image I/O when OpenCV isn\u2019t available.</li> <li> <p>If you need strict OpenCV behavior, install <code>opencv-python</code> or <code>opencv-python-headless</code> explicitly.</p> </li> <li> <p>Warning: falling back from <code>weights_only=True</code></p> </li> <li>Symptom: A warning like <code>torch.load(weights_only=True) unsupported by this torch version; falling back to full deserialization.</code></li> <li>Context: Safer deserialization uses <code>weights_only=True</code> (PyTorch &gt;= 2.0). On older Torch, GFPGAN will warn and proceed with full deserialization to preserve compatibility.</li> <li>Fix: Prefer upgrading to PyTorch 2.x when possible. Example: <code>pip install --upgrade 'torch&gt;=2'</code> (match CUDA version as needed), or keep current Torch if upgrading isn\u2019t feasible (the warning is informational).</li> </ul>"},{"location":"about/acknowledgements/","title":"Acknowledgements","text":"<p>We express our gratitude to the researchers, developers, and communities whose foundational work and contributions made GFPGAN possible.</p>"},{"location":"about/acknowledgements/#research-foundations","title":"Research Foundations","text":""},{"location":"about/acknowledgements/#original-research","title":"Original Research","text":"<p>This project builds upon the foundational research and implementations from:</p> <p>GFPGAN: Towards Real-World Blind Face Restoration with Generative Facial Prior Xintao Wang, Yu Li, Honglun Zhang, Ying Shan IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021 Paper | Original Implementation</p> <p>We acknowledge and thank the original authors for their groundbreaking research in generative facial priors and their open-source contribution that enabled this project. This project has been completely unforked and operates independently while preserving attribution to the original research.</p>"},{"location":"about/acknowledgements/#core-dependencies","title":"Core Dependencies","text":""},{"location":"about/acknowledgements/#essential-libraries","title":"Essential Libraries","text":"<p>BasicSR - Super-Resolution Framework Xintao Wang and contributors GitHub Provides the fundamental training and inference framework for super-resolution models.</p> <p>FaceXLib - Face Detection and Analysis Xintao Wang and contributors GitHub Essential for face detection, alignment, and facial feature analysis.</p>"},{"location":"about/acknowledgements/#model-architectures","title":"Model Architectures","text":"<p>StyleGAN2 - Generative Adversarial Networks Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila Provides the core generative architecture for high-quality face synthesis.</p> <p>Real-ESRGAN - Real-World Image Super-Resolution Xintao Wang and contributors Background enhancement and general image upscaling capabilities.</p>"},{"location":"about/acknowledgements/#technical-infrastructure","title":"Technical Infrastructure","text":""},{"location":"about/acknowledgements/#deep-learning-frameworks","title":"Deep Learning Frameworks","text":"<p>PyTorch - Machine Learning Framework Facebook AI Research and contributors The foundation for all model training and inference operations.</p> <p>OpenCV - Computer Vision Library Intel Corporation and contributors Essential for image processing, manipulation, and I/O operations.</p>"},{"location":"about/acknowledgements/#web-and-api-framework","title":"Web and API Framework","text":"<p>Gradio - Machine Learning Web Interfaces Abubakar Abid and the Gradio team Enables the intuitive web interface for interactive face restoration.</p> <p>FastAPI - Modern Web Framework Sebasti\u00e1n Ram\u00edrez and contributors Powers the REST API for programmatic access to restoration capabilities.</p>"},{"location":"about/acknowledgements/#community-and-development","title":"Community and Development","text":""},{"location":"about/acknowledgements/#development-tools","title":"Development Tools","text":"<p>MkDocs Material - Documentation Framework Martin Donath and contributors Provides the beautiful and functional documentation site.</p> <p>Ruff - Python Linting and Formatting Charlie Marsh and contributors Ensures code quality and consistency throughout the project.</p> <p>Pre-commit - Git Hook Framework Anthony Sottile and contributors Maintains code quality standards and automated checks.</p>"},{"location":"about/acknowledgements/#testing-and-quality-assurance","title":"Testing and Quality Assurance","text":"<p>pytest - Testing Framework Holger Krekel and contributors Comprehensive testing infrastructure for reliability and quality.</p> <p>GitHub Actions - CI/CD Platform GitHub and contributors Automated testing, building, and deployment workflows.</p>"},{"location":"about/acknowledgements/#data-and-evaluation","title":"Data and Evaluation","text":""},{"location":"about/acknowledgements/#evaluation-datasets","title":"Evaluation Datasets","text":"<p>CelebA-HQ - High-Quality Celebrity Faces Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen High-resolution celebrity face dataset for evaluation and benchmarking.</p> <p>FFHQ - Flickr-Faces-HQ Dataset Tero Karras, Samuli Laine, Timo Aila Diverse, high-quality face dataset for comprehensive evaluation.</p>"},{"location":"about/acknowledgements/#metrics-and-evaluation","title":"Metrics and Evaluation","text":"<p>LPIPS - Learned Perceptual Image Patch Similarity Richard Zhang, Phillip Isola, Alexei A. Efros Perceptual similarity measurement for image quality assessment.</p> <p>ArcFace - Additive Angular Margin Loss Jiankang Deng, Jia Guo, Stefanos Zafeiriou Identity preservation evaluation through facial recognition embeddings.</p>"},{"location":"about/acknowledgements/#inspiration-and-related-work","title":"Inspiration and Related Work","text":""},{"location":"about/acknowledgements/#face-restoration-research","title":"Face Restoration Research","text":"<p>DFDNet - Deep Face Dictionary Network Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, Lei Zhang Pioneering work in dictionary-based face restoration.</p> <p>CodeFormer - Learning to Restore Face Images Shangchen Zhou, Kelvin C.K. Chan, Chongyi Li, Chen Change Loy Advanced transformer-based approach to face restoration.</p> <p>RestoreFormer - High-Quality Blind Face Restoration Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, Ping Luo State-of-the-art transformer architecture for face enhancement.</p>"},{"location":"about/acknowledgements/#community-contributions","title":"Community Contributions","text":""},{"location":"about/acknowledgements/#contributors","title":"Contributors","text":"<p>We thank all community members who have contributed to this project through:</p> <ul> <li>Code contributions and bug fixes</li> <li>Documentation improvements</li> <li>Issue reporting and testing</li> <li>Feature suggestions and feedback</li> <li>Community support and discussions</li> </ul>"},{"location":"about/acknowledgements/#special-recognition","title":"Special Recognition","text":"<ul> <li>Beta testers: Early adopters who provided valuable feedback</li> <li>Documentation reviewers: Contributors who improved clarity and accuracy</li> <li>Accessibility advocates: Those who helped improve usability for all users</li> <li>Security researchers: Responsible disclosure of security considerations</li> </ul>"},{"location":"about/acknowledgements/#institutional-support","title":"Institutional Support","text":""},{"location":"about/acknowledgements/#research-community","title":"Research Community","text":"<ul> <li>Computer Vision research community: For advancing the field of face restoration</li> <li>Open-source community: For fostering collaboration and knowledge sharing</li> <li>AI ethics researchers: For guidance on responsible AI development</li> </ul>"},{"location":"about/acknowledgements/#standards-and-guidelines","title":"Standards and Guidelines","text":"<p>Model Cards for Model Reporting Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru Framework for transparent model documentation.</p> <p>Datasheets for Datasets Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III, Kate Crawford Guidelines for comprehensive dataset documentation.</p>"},{"location":"about/acknowledgements/#license-and-legal","title":"License and Legal","text":""},{"location":"about/acknowledgements/#open-source-licenses","title":"Open Source Licenses","text":"<p>This project is made possible by the generous open-source licenses of our dependencies:</p> <ul> <li>Apache License 2.0: Core project license ensuring freedom to use,   modify, and distribute</li> <li>MIT License: Many utility libraries and tools</li> <li>BSD Licenses: Scientific computing and computer vision libraries</li> <li>Creative Commons: Documentation and educational content</li> </ul>"},{"location":"about/acknowledgements/#patent-considerations","title":"Patent Considerations","text":"<p>We acknowledge that some techniques used in this project may be covered by patents. Users should ensure compliance with applicable patent laws in their jurisdiction.</p>"},{"location":"about/acknowledgements/#disclaimer","title":"Disclaimer","text":"<p>While we strive to acknowledge all contributors and influences, this list may not be exhaustive. If you believe your work should be acknowledged here, please contact us at acknowledgements@gfpgan.ai.</p> <p>The inclusion of any work, dataset, or contribution in these acknowledgements does not imply endorsement of this project by the original authors or institutions.</p> <p>Contributing to acknowledgements: If you've contributed to this project or believe your work should be acknowledged, please open an issue or contact us directly. Alternatively, you can reach the maintainers via email if GitHub is not an option.</p> <p>Citation: When citing this project, please also consider citing the foundational research and key dependencies that make this work possible.</p>"},{"location":"about/changelog/","title":"Changelog","text":"<p>For detailed version history, see the repository CHANGELOG:</p> <ul> <li>GitHub: https://github.com/IAmJonoBo/Restoria/blob/main/CHANGELOG.md</li> </ul> <p>This page will be expanded with curated highlights per release.</p>"},{"location":"about/license/","title":"License","text":"<p>Root license: LICENSE   files. A machine-readable copy of the license is included at   <code>LICENSES/Apache-2.0.txt</code>.</p> <p>For third-party components and their licenses, see the notices in <code>LICENSE</code>.</p>"},{"location":"api/","title":"API Documentation","text":"<p>This project provides two integration surfaces:</p> <ul> <li>REST API (FastAPI) for HTTP-based usage</li> <li>Python API via the new modular layer under <code>src/gfpp/</code> (preferred), with     a legacy GFPGAN API kept for backward compatibility</li> </ul>"},{"location":"api/#rest-api","title":"REST API","text":""},{"location":"api/#auto-generated-documentation","title":"Auto-generated Documentation","text":"<p>GFPGAN includes a FastAPI-based REST API with automatic documentation:</p> <ul> <li>Interactive docs:     http://localhost:8000/docs (Swagger UI)</li> <li>Alternative docs:     http://localhost:8000/redoc (ReDoc)</li> <li>OpenAPI spec: http://localhost:8000/openapi.json</li> </ul>"},{"location":"api/#starting-the-api-server","title":"Starting the API Server","text":"<pre><code># Production server\nuvicorn services.api.main:app --host 0.0.0.0 --port 8000\n\n# Development server with auto-reload\nuvicorn services.api.main:app --reload --port 8000\n</code></pre>"},{"location":"api/#authentication","title":"Authentication","text":"<p>Currently, the API runs without authentication for simplicity. For production deployments, implement authentication:</p> <pre><code># Example: API key authentication\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer\n\nsecurity = HTTPBearer()\n\ndef verify_api_key(token: str = Depends(security)):\n    if token.credentials != \"your-api-key\":\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid API key\"\n        )\n    return token\n</code></pre>"},{"location":"api/#example-api-usage","title":"Example API Usage","text":""},{"location":"api/#single-image-restoration","title":"Single Image Restoration","text":"<p>Request:</p> <pre><code>curl -X POST \"http://localhost:8000/restore\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F \"file=@damaged_photo.jpg\" \\\n  -F \"version=1.4\" \\\n  -F \"upscale=2\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"status\": \"success\",\n  \"restoration_id\": \"12345-abcde\",\n  \"original_size\": [800, 600],\n  \"restored_size\": [1600, 1200],\n  \"processing_time\": 2.3,\n  \"download_url\": \"/download/12345-abcde\"\n}\n</code></pre>"},{"location":"api/#batch-processing","title":"Batch Processing","text":"<p>Request:</p> <pre><code>curl -X POST \"http://localhost:8000/batch\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"image_urls\": [\n      \"https://example.com/photo1.jpg\",\n      \"https://example.com/photo2.jpg\"\n    ],\n    \"settings\": {\n      \"version\": \"1.4\",\n      \"upscale\": 2,\n      \"background_enhance\": true\n    }\n  }'\n</code></pre> <p>Response:</p> <pre><code>{\n  \"status\": \"accepted\",\n  \"batch_id\": \"batch-67890\",\n  \"estimated_completion\": \"2024-01-15T10:30:00Z\",\n  \"status_url\": \"/batch/batch-67890/status\"\n}\n</code></pre>"},{"location":"api/#quality-metrics","title":"Quality Metrics","text":"<p>Request:</p> <pre><code>curl -X POST \"http://localhost:8000/metrics\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F \"original=@original.jpg\" \\\n  -F \"restored=@restored.jpg\" \\\n  -F \"metrics[]=lpips\" \\\n  -F \"metrics[]=dists\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"metrics\": {\n    \"lpips\": 0.234,\n    \"dists\": 0.156,\n    \"arcface_similarity\": 0.892\n  },\n  \"analysis\": {\n    \"quality_score\": 8.5,\n    \"identity_preservation\": \"excellent\",\n    \"texture_realism\": \"high\"\n  }\n}\n</code></pre>"},{"location":"api/#python-api-gfpp","title":"Python API (gfpp)","text":"<p>Prefer the modular <code>gfpp</code> API for new integrations. It exposes:</p> <ul> <li>Orchestrator: produce a deterministic Plan for an image and options</li> <li>Registry: list/resolve backends without importing heavy deps</li> <li>IO: run manifest helpers and centralized weight resolution</li> <li>Restorers: lightweight adapters that implement a shared protocol</li> </ul>"},{"location":"api/#orchestrator-and-plan","title":"Orchestrator and Plan","text":"<pre><code>from gfpp.core.orchestrator import plan\n\npl = plan(\n    \"samples/portrait.jpg\",\n    {\"backend\": \"gfpgan\", \"weight\": 0.6, \"experimental\": False},\n)\nprint(pl.backend, pl.params, pl.reason, pl.confidence)\nprint(pl.quality)  # may include niqe/brisque (best-effort)\n</code></pre> <p>Plan fields (dataclass):</p> <ul> <li>backend: str</li> <li>params: dict[str, any]</li> <li>postproc: dict[str, any]</li> <li>reason: str</li> <li>confidence: float (0..1)</li> <li>quality: dict[str, float | None]</li> <li>faces: dict[str, any]</li> <li>detail: dict[str, any] (routing explanation and inputs)</li> </ul>"},{"location":"api/#registry","title":"Registry","text":"<pre><code>from gfpp.core.registry import list_backends, get\n\navail = list_backends()           # {'gfpgan': True, 'codeformer': False, ...}\nRestorerClass = get('gfpgan')     # returns a class, import deferred until here\n</code></pre>"},{"location":"api/#restorer-protocol","title":"Restorer Protocol","text":"<pre><code>from gfpp.restorers.base import Restorer, RestoreResult\n\ndef run_restoration(img):\n    RestorerClass = get('gfpgan')\n    restorer: Restorer = RestorerClass()\n    restorer.prepare({\"device\": \"auto\"})  # lazy import heavy deps\n    result: RestoreResult = restorer.restore(img, {\"weight\": 0.6})\n    return result.restored_image, result.metrics\n</code></pre> <p>RestoreResult fields:</p> <ul> <li>input_path: str | None</li> <li>restored_path: str | None</li> <li>restored_image: any | None (e.g., numpy.ndarray)</li> <li>cropped_faces: list[str]</li> <li>restored_faces: list[str]</li> <li>metrics: dict[str, any]</li> </ul>"},{"location":"api/#io-helpers","title":"IO helpers","text":"<pre><code>from gfpp.io.manifest import RunManifest, write_manifest\nfrom gfpp.io.weights import ensure_weight\n\nman = RunManifest(args={\"backend\": \"gfpgan\", \"metrics\": \"fast\"}, device=\"cpu\")\nensure_weight(\"GFPGANv1.4\")  # delegates to centralized resolver\nwrite_manifest(\"out/manifest.json\", man)\n</code></pre>"},{"location":"api/#integration-examples-legacy-gfpgan-api","title":"Integration Examples (legacy GFPGAN API)","text":""},{"location":"api/#flask-application","title":"Flask Application","text":"<pre><code>from flask import Flask, request, send_file\nfrom gfpgan import GFPGANer\nimport cv2\nimport tempfile\n\napp = Flask(__name__)\nrestorer = GFPGANer(model_path='GFPGANv1.4.pth', upscale=2)\n\n@app.route('/restore', methods=['POST'])\ndef restore_image():\n    file = request.files['image']\n\n    # Save uploaded file\n    with tempfile.NamedTemporaryFile(suffix='.jpg') as tmp_input:\n        file.save(tmp_input.name)\n        img = cv2.imread(tmp_input.name)\n\n        # Restore image\n        _, restored_imgs, _ = restorer.enhance(img)\n\n        # Save result\n        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp_output:\n            cv2.imwrite(tmp_output.name, restored_imgs[0])\n            return send_file(tmp_output.name, as_attachment=True)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"api/#streamlit-application","title":"Streamlit Application","text":"<pre><code>import streamlit as st\nfrom gfpgan import GFPGANer\nimport cv2\nimport numpy as np\n\n@st.cache_resource\ndef load_restorer():\n    return GFPGANer(model_path='GFPGANv1.4.pth', upscale=2)\n\nst.title(\"GFPGAN Face Restoration\")\n\nuploaded_file = st.file_uploader(\"Choose an image\", type=['jpg', 'jpeg', 'png'])\n\nif uploaded_file is not None:\n    # Convert uploaded file to OpenCV format\n    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n    image = cv2.imdecode(file_bytes, 1)\n\n    col1, col2 = st.columns(2)\n\n    with col1:\n        st.header(\"Original\")\n        st.image(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    with col2:\n        st.header(\"Restored\")\n        restorer = load_restorer()\n        _, restored_imgs, _ = restorer.enhance(image)\n        st.image(cv2.cvtColor(restored_imgs[0], cv2.COLOR_BGR2RGB))\n</code></pre>"},{"location":"api/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code># Check GPU availability\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"MPS available: {torch.backends.mps.is_available()}\")\n\n# Use specific device\nrestorer = GFPGANer(\n    model_path='GFPGANv1.4.pth',\n    device='cuda:0'  # or 'cpu', 'mps'\n)\n</code></pre>"},{"location":"api/#memory-management","title":"Memory Management","text":"<pre><code># For processing large images or batches\nrestorer = GFPGANer(\n    model_path='GFPGANv1.4.pth',\n    upscale=1,  # Reduce upscaling to save memory\n    bg_upsampler=None  # Disable background upsampling\n)\n\n# Clear GPU cache after processing\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"api/#error-codes","title":"Error Codes","text":"Code Description Resolution 400 Invalid input format Check image format and size 404 Model not found Verify model path or download 413 Image too large Reduce image size or increase limits 500 Processing failed Check GPU memory and input validity 503 Service overloaded Implement rate limiting or retry <p>Need help? Check our guides or create an issue.</p>"},{"location":"api/core/","title":"API: Core","text":"<p>Core modules behind the modular gfpp library and CLI.</p> <ul> <li>Orchestrator and Plan (deterministic planning)</li> <li>Registry and Restorer protocol (pluggable backends)</li> <li>Manifest IO (run manifests and metrics)</li> </ul>"},{"location":"api/core/#orchestrator","title":"Orchestrator","text":"<p>The orchestrator computes a deterministic Plan for a given input and options. It aims to be pure and repeatable for identical inputs and the same seed.</p> <p>Plan dataclass fields:</p> <ul> <li>backend: str</li> <li>params: dict[str, any]</li> <li>postproc: dict[str, any]</li> <li>reason: str</li> <li>confidence: float (0..1)</li> <li>quality: dict[str, float | None]</li> <li>faces: dict[str, any]</li> <li>detail: dict[str, any]</li> </ul> <p>Example:</p> <pre><code>from gfpp.core import orchestrator\n\nplan = orchestrator.plan(\n    \"samples/portrait.jpg\",\n    {\"backend\": \"gfpgan\", \"compile\": False, \"ort_providers\": []},\n)\nprint(plan.backend, plan.params, plan.reason)\n</code></pre>"},{"location":"api/core/#registry-and-restorer-protocol","title":"Registry and Restorer protocol","text":"<p>Backends are resolved via a registry (<code>gfpp.core.registry</code>). Each backend implements:</p> <ul> <li>prepare(cfg: dict) -&gt; None (lazy import heavy deps)</li> <li>restore(image: np.ndarray, cfg: dict) -&gt; RestoreResult</li> </ul> <p>The result contains at least:</p> <ul> <li>restored_image: np.ndarray | None</li> <li>metrics: dict (optional, can be empty)</li> </ul>"},{"location":"api/core/#manifest-io","title":"Manifest IO","text":"<p>Runs typically write two files into the output directory by default:</p> <ul> <li>metrics.json</li> <li>manifest.json</li> </ul> <p>Example manifest shape:</p> <pre><code>{\n  \"args\": {\"backend\": \"gfpgan\", \"metrics\": \"fast\", \"device\": \"auto\"},\n  \"device\": \"cpu\",\n  \"results\": [\n    {\n      \"input\": \"inputs/a.jpg\",\n      \"restored_img\": \"out/a.png\",\n      \"metrics\": {\"arcface_cosine\": 0.87}\n    }\n  ],\n  \"metrics_file\": \"metrics.json\",\n  \"env\": {\n    \"runtime\": {\"compile\": false, \"ort_providers\": []},\n    \"git\": \"abc123\",\n    \"torch\": \"2.x\",\n    \"cuda\": false\n  }\n}\n</code></pre> <p>Notes:</p> <ul> <li>Optional features degrade gracefully; missing metrics return <code>None</code>.</li> <li>Heavy dependencies (torch, cv2, metrics) are imported lazily inside   functions and guarded with try/except.</li> </ul>"},{"location":"api/models/","title":"API: Models","text":"<p>Model- and backend-related APIs and configuration.</p> <ul> <li>Listing available backends</li> <li>Resolving weights via centralized logic</li> <li>Notes on per-backend options (overview)</li> </ul>"},{"location":"api/models/#backends-and-availability","title":"Backends and availability","text":"<pre><code>from gfpp.core.registry import list_backends, get\n\navail = list_backends()  # {'gfpgan': True, 'codeformer': False, ...}\n\n# Include experimental ones\navail_all = list_backends(include_experimental=True)\n\n# Resolve a backend class (imports on first use)\nRestorerClass = get('gfpgan')\nrestorer = RestorerClass()\n</code></pre> <p>Aliases: some names map to canonical implementations, e.g., <code>restoreformer -&gt; restoreformerpp</code>.</p>"},{"location":"api/models/#weight-resolution","title":"Weight resolution","text":"<p>Use the IO helper to ensure a model weight is present locally and obtain its path and optional sha256. This delegates to the centralized resolver to avoid duplication.</p> <pre><code>from gfpp.io.weights import ensure_weight\n\npath, sha256 = ensure_weight(\"GFPGANv1.4\")\nprint(path, sha256)\n</code></pre> <p>Notes:</p> <ul> <li>If <code>no_download=True</code>, this will not attempt network access and may raise   from the underlying resolver if the weight is missing.</li> <li>The central resolver supports local caches and may fetch from configured   sources when permitted.</li> </ul>"},{"location":"api/models/#per-backend-options-overview","title":"Per-backend options (overview)","text":"<ul> <li>gfpgan: <code>weight</code> (blend), <code>upscale</code>, optional background upsampler</li> <li>codeformer: fidelity <code>weight</code> typically &gt;= 0.6 for strong restoration</li> <li>restoreformerpp: similar to GFPGAN with different artifact trade-offs</li> <li>ensemble: combines outputs; expect higher latency</li> </ul> <p>Tip: When using the orchestrator, backend parameters may be normalized for determinism (e.g., weight defaults to 0.6 for moderate degradation).</p>"},{"location":"api/utils/","title":"API: Utilities","text":"<p>General helpers and public types exposed by the modular layer.</p>"},{"location":"api/utils/#public-types","title":"Public types","text":"<pre><code>from gfpp.restorers.base import RestoreResult\n\n# Fields\n# input_path: str | None\n# restored_path: str | None\n# restored_image: any | None\n# cropped_faces: list[str]\n# restored_faces: list[str]\n# metrics: dict[str, any]\n</code></pre>"},{"location":"api/utils/#manifest-helpers","title":"Manifest helpers","text":"<pre><code>from gfpp.io.manifest import RunManifest, write_manifest\n\nman = RunManifest(args={\"backend\": \"gfpgan\", \"metrics\": \"fast\"}, device=\"cpu\")\nman.results.append({\n  \"input\": \"inputs/a.jpg\",\n  \"restored_img\": \"out/a.png\",\n  \"metrics\": {\"arcface_cosine\": 0.87}\n})\nwrite_manifest(\"out/manifest.json\", man)\n</code></pre> <p>Notes:</p> <ul> <li><code>RunManifest.env</code> includes runtime info such as git SHA and torch version   when available.</li> <li>Manifests and metrics are JSON files intended to be stable and machine-readable.</li> </ul>"},{"location":"api/utils/#image-io-helpers","title":"Image IO helpers","text":"<p>The project favors lazy imports for heavy dependencies (cv2, PIL, torch) and permits multiple image array formats as <code>restored_image</code> in <code>RestoreResult</code>. When building higher-level pipelines, normalize to numpy arrays in BGR or RGB as appropriate for your stack.</p>"},{"location":"api/utils/#logging-and-progress","title":"Logging and progress","text":"<p>The CLI emits structured logs and writes run manifests. For library usage, prefer passing a <code>progress</code> callback in your integration code and capture timings around <code>prepare</code> and <code>restore</code> calls.</p>"},{"location":"getting-started/install/","title":"Installation","text":""},{"location":"getting-started/install/#quick-install","title":"Quick install","text":"<p>The fastest way to get started:</p> <pre><code>pip install gfpgan\n</code></pre> <p>This installs the core GFPGAN package with basic dependencies.</p>"},{"location":"getting-started/install/#platform-specific-setup","title":"Platform-specific setup","text":""},{"location":"getting-started/install/#windows","title":"Windows","text":"NVIDIA GPUDirectML (AMD/Intel)CPU only <pre><code># Install PyTorch with CUDA support first\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n\n# Install GFPGAN\npip install gfpgan\n</code></pre> <pre><code># Install PyTorch with DirectML support\npip install torch-directml\n\n# Install GFPGAN\npip install gfpgan\n</code></pre> <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\npip install gfpgan\n</code></pre>"},{"location":"getting-started/install/#macos","title":"macOS","text":"Apple Silicon (M1/M2/M3)Intel Mac <pre><code># Metal Performance Shaders (MPS) support included\npip install gfpgan\n</code></pre> <pre><code># CPU-only installation\npip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\npip install gfpgan\n</code></pre>"},{"location":"getting-started/install/#linux","title":"Linux","text":"NVIDIA GPUAMD GPU (ROCm)CPU only <pre><code># Install PyTorch with CUDA support\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n\n# Install GFPGAN\npip install gfpgan\n</code></pre> <pre><code># Install PyTorch with ROCm support\npip install torch torchvision --index-url https://download.pytorch.org/whl/rocm5.6\n\n# Install GFPGAN\npip install gfpgan\n</code></pre> <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\npip install gfpgan\n</code></pre>"},{"location":"getting-started/install/#development-installation","title":"Development installation","text":"<p>For contributing or using the latest features:</p> <pre><code>git clone https://github.com/IAmJonoBo/Restoria.git\ncd Restoria\npip install -e \".[dev,metrics,web]\"\n</code></pre>"},{"location":"getting-started/install/#optional-extras","title":"Optional extras","text":"<p>Install additional features as needed:</p> <pre><code># Web interface and API\npip install -e \".[web]\"\n\n# Quality metrics\npip install -e \".[metrics]\"\n\n# Development tools\npip install -e \".[dev]\"\n\n# All extras\npip install -e \".[dev,metrics,web]\"\n</code></pre>"},{"location":"getting-started/install/#verify-installation","title":"Verify installation","text":"<p>Test your installation:</p> <pre><code># Check if GFPGAN is installed\ngfpgan-infer --help\n\n# Test with a simple command (dry run)\ngfpgan-infer --input test.jpg --dry-run\n</code></pre>"},{"location":"getting-started/install/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/install/#common-issues","title":"Common issues","text":"<p>CUDA out of memory</p> <p>Try CPU mode or reduce batch size: <pre><code>gfpgan-infer --input photo.jpg --device cpu\n</code></pre></p> <p>ModuleNotFoundError: No module named 'cv2'</p> <p>Install OpenCV: <pre><code>pip install opencv-python\n</code></pre></p> <p>No module named 'basicsr'</p> <p>Install BasicSR: <pre><code>pip install basicsr\n</code></pre></p>"},{"location":"getting-started/install/#getting-help","title":"Getting help","text":"<p>If you encounter issues:</p> <ol> <li>Check our troubleshooting guide</li> <li>Search existing issues</li> <li>Create a new issue with:</li> <li>Your platform (OS, GPU model)</li> <li>Python version (<code>python --version</code>)</li> <li>Error message and full traceback</li> </ol>"},{"location":"getting-started/install/#system-requirements","title":"System requirements","text":"Component Minimum Recommended Python 3.8+ 3.10+ RAM 4GB 8GB+ GPU Memory N/A (CPU) 4GB+ Storage 2GB 5GB+ (for models) <p>Next: Restore your first photo \u2192</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Note: The new Restoria CLI (<code>restoria</code>) is the primary entry point going forward. Legacy commands (<code>gfpgan-infer</code>, <code>gfpup</code>) remain available during the transition. See the migration guide: guides/migration.md.</p> <ul> <li>Install (editable):</li> <li><code>pip install -e .[dev]</code></li> <li>Inference on CPU or GPU (recommended):</li> <li><code>restoria run --input inputs/whole_imgs --output out/ --backend gfpgan --metrics fast</code></li> <li><code>gfpup run --input inputs/whole_imgs --backend gfpgan --metrics fast --output out/</code></li> <li>Legacy: <code>gfpgan-infer --input inputs/whole_imgs --version 1.4 --upscale 2 --device auto</code></li> <li>Helpful flags:</li> <li><code>--dry-run</code> (validate args), <code>--no-download</code> (require local weights), <code>--model-path</code> (override weights)</li> <li>Colab notebook (UI):</li> <li>Open in Colab</li> </ul> <p>See also:</p> <ul> <li>CLI Usage (./usage/cli.md)</li> <li>Colab Guide (./usage/colab.md)</li> </ul>"},{"location":"getting-started/quickstart/#listing-available-backends","title":"Listing available backends","text":"<p>Check which backends are available in your environment without heavy downloads:</p> <pre><code>gfpup list-backends\n# or\nrestoria list-backends\n</code></pre> <p>Add <code>--verbose</code> for per-backend availability, and <code>--all</code> to include experimental ones. For machine-readable output:</p> <pre><code>gfpup list-backends --json\n# or\nrestoria list-backends --json\n</code></pre> <p>The JSON output includes a <code>schema_version</code> field (currently \"1\") and a <code>backends</code> dictionary mapping backend name to availability, with an <code>experimental</code> flag indicating whether experimental backends are included.</p>"},{"location":"getting-started/quickstart/#environment-check-doctor","title":"Environment check (doctor)","text":"<pre><code>gfpup doctor\n# or\nrestoria doctor\n</code></pre> <p>For machine-readable output:</p> <pre><code>gfpup doctor --json\n# or\nrestoria doctor --json\n</code></pre>"},{"location":"getting-started/quickstart/#optional-perceptual-metrics","title":"Optional perceptual metrics","text":"<p>Install optional metrics for identity/perceptual scoring:</p> <pre><code>pip install -e .[metrics]\n</code></pre> <p>Notes:</p> <ul> <li>Linux: BRISQUE via <code>imquality[brisque]</code> is enabled.</li> <li>Windows: BRISQUE via <code>pybrisque</code> is enabled.</li> <li>macOS: BRISQUE is not installed by default to avoid resolver/build issues on newer Python versions; metrics gracefully degrade if BRISQUE is unavailable.</li> </ul>"},{"location":"governance/code-of-conduct/","title":"Code of Conduct","text":"<p>This project adheres to our Code of Conduct.</p> <ul> <li>Repository version: https://github.com/IAmJonoBo/Restoria/blob/main/CODE_OF_CONDUCT.md</li> </ul> <p>Contact maintainers if you see unacceptable behavior.</p>"},{"location":"governance/contributing/","title":"Contributing to GFPGAN","text":"<p>We welcome contributions to GFPGAN! This guide will help you get started with developing and contributing to the project.</p>"},{"location":"governance/contributing/#getting-started","title":"Getting started","text":""},{"location":"governance/contributing/#development-environment-setup","title":"Development environment setup","text":"<ol> <li>Fork and clone the repository</li> </ol> <pre><code>git clone https://github.com/YOUR_USERNAME/Restoria.git\ncd Restoria\n</code></pre> <ol> <li>Create a virtual environment</li> </ol> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre> <ol> <li>Install in development mode</li> </ol> <pre><code>pip install -e \".[dev,metrics,web]\"\n</code></pre> <p>This installs:    - dev: Linting, formatting, and testing tools    - metrics: Quality evaluation dependencies    - web: Web interface dependencies</p> <ol> <li>Set up pre-commit hooks</li> </ol> <pre><code>pre-commit install\n</code></pre>"},{"location":"governance/contributing/#development-workflow","title":"Development workflow","text":"<ol> <li>Create a feature branch</li> </ol> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <ol> <li> <p>Make your changes</p> </li> <li> <p>Write clear, well-documented code</p> </li> <li>Follow existing code style and patterns</li> <li> <p>Add tests for new functionality</p> </li> <li> <p>Test your changes</p> </li> </ol> <pre><code># Run linting and formatting\nruff check .\nblack .\n\n# Run tests\npytest tests_light/  # Quick tests\npytest tests/        # Full test suite\n</code></pre> <ol> <li>Commit and push</li> </ol> <pre><code>git add .\ngit commit -m \"feat: add your feature description\"\ngit push origin feature/your-feature-name\n</code></pre> <ol> <li> <p>Create a pull request</p> </li> <li> <p>Use a clear, descriptive title</p> </li> <li>Explain what your changes do and why</li> <li>Reference any related issues</li> <li>Ensure all checks pass</li> </ol>"},{"location":"governance/contributing/#code-style-and-standards","title":"Code style and standards","text":""},{"location":"governance/contributing/#python-code-style","title":"Python code style","text":"<p>We use automated formatting and linting:</p> <ul> <li>Black: Code formatting</li> <li>Ruff: Fast linting and import sorting</li> <li>Type hints: Required for public APIs</li> </ul> <pre><code># Format code\nblack .\n\n# Check linting\nruff check .\n\n# Fix auto-fixable issues\nruff check --fix .\n</code></pre>"},{"location":"governance/contributing/#commit-message-format","title":"Commit message format","text":"<p>We follow Conventional Commits:</p> <pre><code>type(scope): description\n\n[optional body]\n\n[optional footer(s)]\n</code></pre> <p>Types: - <code>feat</code>: New feature - <code>fix</code>: Bug fix - <code>docs</code>: Documentation changes - <code>refactor</code>: Code refactoring - <code>test</code>: Adding or updating tests - <code>chore</code>: Maintenance tasks</p> <p>Examples: <pre><code>feat(cli): add dry-run mode for batch processing\nfix(web): handle missing face detection gracefully\ndocs(guides): update hardware requirements\n</code></pre></p>"},{"location":"governance/contributing/#documentation","title":"Documentation","text":"<p>All user-facing features should include documentation:</p> <ul> <li>API functions: Docstrings with examples</li> <li>CLI commands: Help text and guide updates</li> <li>New features: Usage guides and examples</li> </ul>"},{"location":"governance/contributing/#testing","title":"Testing","text":""},{"location":"governance/contributing/#test-structure","title":"Test structure","text":"<pre><code>tests/                 # Full test suite\n\u251c\u2500\u2500 test_gfpgan_arch.py\n\u251c\u2500\u2500 test_models.py\n\u2514\u2500\u2500 ...\n\ntests_light/           # Quick tests for CI\n\u251c\u2500\u2500 test_basic.py\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"governance/contributing/#running-tests","title":"Running tests","text":"<pre><code># Quick tests (recommended for development)\npytest tests_light/ -v\n\n# Full test suite (for comprehensive validation)\npytest tests/ -v\n\n# Test specific functionality\npytest tests/test_gfpgan_model.py -v\n\n# Test with coverage\npytest tests_light/ --cov=gfpgan --cov-report=html\n</code></pre>"},{"location":"governance/contributing/#writing-tests","title":"Writing tests","text":"<ul> <li>Test both success and failure cases</li> <li>Use clear, descriptive test names</li> <li>Include edge cases and boundary conditions</li> <li>Mock external dependencies when needed</li> </ul> <p>Example: <pre><code>def test_gfpgan_infer_with_valid_image():\n    \"\"\"Test that GFPGAN inference works with valid input image.\"\"\"\n    # Test implementation\n    pass\n\ndef test_gfpgan_infer_with_invalid_format():\n    \"\"\"Test that GFPGAN handles invalid image formats gracefully.\"\"\"\n    # Test implementation\n    pass\n</code></pre></p>"},{"location":"governance/contributing/#pull-request-checklist","title":"Pull request checklist","text":"<p>Before submitting a pull request, ensure:</p> <ul> <li>[ ] Code quality</li> <li>[ ] Code follows style guidelines (Black + Ruff)</li> <li>[ ] All tests pass (<code>pytest tests_light/</code>)</li> <li> <p>[ ] No linting errors (<code>ruff check .</code>)</p> </li> <li> <p>[ ] Documentation</p> </li> <li>[ ] Docstrings added for new functions/classes</li> <li>[ ] User guides updated if needed</li> <li> <p>[ ] CHANGELOG.md updated for user-facing changes</p> </li> <li> <p>[ ] Testing</p> </li> <li>[ ] New tests added for new functionality</li> <li>[ ] Existing tests still pass</li> <li> <p>[ ] Edge cases considered</p> </li> <li> <p>[ ] Compatibility</p> </li> <li>[ ] Changes don't break existing API</li> <li>[ ] Backward compatibility maintained</li> <li>[ ] Cross-platform compatibility verified</li> </ul>"},{"location":"governance/contributing/#development-tasks","title":"Development tasks","text":""},{"location":"governance/contributing/#running-the-documentation-site-locally","title":"Running the documentation site locally","text":"<pre><code># Install docs dependencies\npip install -e \".[docs]\"\n\n# Serve docs locally\nmkdocs serve\n\n# Build static site\nmkdocs build\n</code></pre>"},{"location":"governance/contributing/#building-and-testing-packages","title":"Building and testing packages","text":"<pre><code># Build wheel\npython -m build\n\n# Test installation\npip install dist/gfpgan-*.whl\n</code></pre>"},{"location":"governance/contributing/#benchmarking-changes","title":"Benchmarking changes","text":"<pre><code># Run benchmark suite\npython bench/run_bench.py --input inputs/samples/ --output bench_results/\n\n# Compare with baseline\npython bench/compare_results.py --baseline bench_baseline.json --current bench_results.json\n</code></pre>"},{"location":"governance/contributing/#release-process","title":"Release process","text":"<ol> <li>Update version numbers</li> <li><code>VERSION</code> file</li> <li><code>pyproject.toml</code></li> <li> <p><code>gfpgan/version.py</code></p> </li> <li> <p>Update CHANGELOG.md</p> </li> <li>Move items from \"Unreleased\" to new version section</li> <li> <p>Follow Keep a Changelog format</p> </li> <li> <p>Create release PR</p> </li> <li>Title: \"Release v1.2.3\"</li> <li>Include changelog highlights</li> <li> <p>Ensure all tests pass</p> </li> <li> <p>Tag and release <pre><code>git tag v1.2.3\ngit push origin v1.2.3\n</code></pre></p> </li> <li> <p>Deploy documentation <pre><code>mike deploy --push --update-aliases 1.2.3 latest\n</code></pre></p> </li> </ol>"},{"location":"governance/contributing/#getting-help","title":"Getting help","text":"<ul> <li>Questions: Open a discussion</li> <li>Bugs: Create an issue</li> <li>Features: Start with a discussion, then create an issue</li> <li>Security: See our security policy</li> </ul>"},{"location":"governance/contributing/#code-of-conduct","title":"Code of conduct","text":"<p>Please read and follow our Code of Conduct. We're committed to providing a welcoming and inclusive environment for all contributors.</p> <p>Thank you for contributing to GFPGAN! \ud83c\udf89</p>"},{"location":"governance/docs-conventions/","title":"Documentation Conventions","text":"<p>This project uses MkDocs Material with markdownlint. Follow these conventions to keep docs consistent and warning\u2011free.</p> <ul> <li>Link styles</li> <li>Prefer relative links for pages inside the docs (e.g., <code>../guides/face-enhancement.md</code>).</li> <li>Use absolute GitHub links for repository files outside <code>docs/</code>     (e.g., LICENSE) to avoid broken relative paths.</li> <li>Use section anchors with <code>#heading-text</code> and ensure the target heading     exists and is unique.</li> <li>Headings</li> <li>Avoid duplicate headings at the same nesting level in a single page.</li> <li>Keep titles concise; use sentence case (e.g., \"Quick start\").</li> <li>Code blocks</li> <li>Surround fenced code blocks with a blank line above and below.</li> <li>Use language hints (<code>bash,</code>python,```json) for syntax highlighting.</li> <li>Line length</li> <li>Try to keep lines \u2264 100 chars. For long URLs, break the sentence across     lines if needed.</li> <li>Images &amp; assets</li> <li>Place images under <code>docs/assets/</code> and refer relatively from the page.</li> <li>Include alt text; keep images light.</li> <li>Admonitions</li> <li>Use for important notes and warnings (e.g., <code>!!! note</code>, <code>!!! warning</code>).</li> <li>Cross\u2011refs to CLIs</li> <li>The primary CLI is <code>restoria</code>; legacy shims (<code>gfpup</code>, <code>gfpgan-infer</code>)     remain but should be called out as legacy only when relevant.</li> <li>Stability</li> <li>If a feature is experimental, label it clearly and prefer soft\u2011failure     language in examples.</li> </ul> <p>If a link or build warning appears, prefer fixing the source page/link rather than suppressing the warning.</p>"},{"location":"governance/ethics/","title":"Ethics Guidelines (Draft)","text":"<p>This is a placeholder for ethics and responsible AI guidance.</p> <p>Areas:</p> <ul> <li>Consent and intended use</li> <li>Bias considerations and limitations</li> </ul> <p>TODO: Add concrete do/don't examples.</p>"},{"location":"governance/maintainers/","title":"Maintainers","text":"<p>This document outlines the maintainer structure, responsibilities, and processes for GFPGAN.</p>"},{"location":"governance/maintainers/#current-maintainers","title":"Current Maintainers","text":""},{"location":"governance/maintainers/#lead-maintainer","title":"Lead Maintainer","text":"<ul> <li>@IAmJonoBo - Project lead, releases, infrastructure</li> </ul>"},{"location":"governance/maintainers/#core-maintainers","title":"Core Maintainers","text":"<ul> <li>@IAmJonoBo - General maintenance, code review, documentation</li> </ul>"},{"location":"governance/maintainers/#area-maintainers","title":"Area Maintainers","text":"Area Maintainer Responsibilities Core Engine @IAmJonoBo Model architectures, inference pipeline Documentation @IAmJonoBo Docs site, guides, API documentation CI/CD @IAmJonoBo Build, test, and deployment automation Web Interface @IAmJonoBo Gradio app, API server"},{"location":"governance/maintainers/#responsibilities","title":"Responsibilities","text":""},{"location":"governance/maintainers/#lead-maintainer_1","title":"Lead Maintainer","text":"<ul> <li>Release management: Planning, coordination, and execution</li> <li>Project direction: Technical roadmap and architecture decisions</li> <li>Community management: Issue triage, contributor onboarding</li> <li>Security: Vulnerability response and security policy enforcement</li> </ul>"},{"location":"governance/maintainers/#core-maintainers_1","title":"Core Maintainers","text":"<ul> <li>Code review: Review and approve pull requests</li> <li>Issue triage: Label, prioritize, and route issues</li> <li>Documentation: Maintain and improve project documentation</li> <li>Testing: Ensure comprehensive test coverage</li> </ul>"},{"location":"governance/maintainers/#area-maintainers_1","title":"Area Maintainers","text":"<ul> <li>Domain expertise: Deep knowledge of specific components</li> <li>Feature development: Lead development in their area</li> <li>Code review: Review PRs affecting their domain</li> <li>Documentation: Maintain area-specific documentation</li> </ul>"},{"location":"governance/maintainers/#processes","title":"Processes","text":""},{"location":"governance/maintainers/#issue-triage","title":"Issue Triage","text":"<p>Issues are triaged using these labels:</p>"},{"location":"governance/maintainers/#priority-labels","title":"Priority Labels","text":"<ul> <li><code>priority/critical</code> - Security issues, data loss, crashes</li> <li><code>priority/high</code> - Significant functionality problems</li> <li><code>priority/medium</code> - General bugs and improvements</li> <li><code>priority/low</code> - Minor enhancements, cleanup</li> </ul>"},{"location":"governance/maintainers/#type-labels","title":"Type Labels","text":"<ul> <li><code>type/bug</code> - Confirmed bugs</li> <li><code>type/feature</code> - New feature requests</li> <li><code>type/enhancement</code> - Improvements to existing features</li> <li><code>type/docs</code> - Documentation improvements</li> <li><code>type/question</code> - Support questions</li> </ul>"},{"location":"governance/maintainers/#area-labels","title":"Area Labels","text":"<ul> <li><code>area/core</code> - Core inference engine</li> <li><code>area/models</code> - Model architectures and weights</li> <li><code>area/cli</code> - Command-line interface</li> <li><code>area/web</code> - Web interface and API</li> <li><code>area/docs</code> - Documentation</li> <li><code>area/ci</code> - Continuous integration</li> </ul>"},{"location":"governance/maintainers/#status-labels","title":"Status Labels","text":"<ul> <li><code>status/needs-info</code> - Waiting for more information</li> <li><code>status/needs-reproduction</code> - Waiting for reproduction steps</li> <li><code>status/blocked</code> - Blocked by external dependencies</li> <li><code>status/wontfix</code> - Will not be fixed (with explanation)</li> </ul>"},{"location":"governance/maintainers/#pull-request-review","title":"Pull Request Review","text":"<p>All PRs require:</p> <ol> <li>Automated checks: All CI checks must pass</li> <li>Code review: At least one maintainer approval</li> <li>Area review: Area maintainer approval for specialized changes</li> <li>Documentation: Updated docs for user-facing changes</li> </ol>"},{"location":"governance/maintainers/#review-guidelines","title":"Review Guidelines","text":"<p>Reviewers should check for:</p> <ul> <li>Functionality: Does the change work as intended?</li> <li>Code quality: Is the code readable and maintainable?</li> <li>Testing: Are there appropriate tests?</li> <li>Documentation: Are user-facing changes documented?</li> <li>Compatibility: Does it maintain backward compatibility?</li> <li>Performance: Does it impact performance significantly?</li> </ul>"},{"location":"governance/maintainers/#release-process","title":"Release Process","text":"<p>Releases follow this process:</p> <ol> <li>Version planning: Determine scope and version number</li> <li>Feature freeze: No new features for patch releases</li> <li>Testing: Run full test suite and manual testing</li> <li>Documentation: Update changelog and version docs</li> <li>Release creation: Tag and create GitHub release</li> <li>Distribution: Deploy to PyPI and update docs site</li> <li>Announcement: Communicate release to community</li> </ol>"},{"location":"governance/maintainers/#becoming-a-maintainer","title":"Becoming a Maintainer","text":"<p>Path to maintainership:</p> <ol> <li>Contribution history: Consistent, quality contributions</li> <li>Community involvement: Help with issues and reviews</li> <li>Domain expertise: Deep knowledge of a specific area</li> <li>Nomination: Current maintainer nomination</li> <li>Consensus: Agreement from lead maintainer</li> </ol>"},{"location":"governance/maintainers/#maintainer-emeritus","title":"Maintainer Emeritus","text":"<p>Former maintainers who have stepped back but contributed significantly:</p> <ul> <li>Recognition in project documentation</li> <li>Optional advisory role for major decisions</li> <li>Credit in release notes and acknowledgments</li> </ul>"},{"location":"governance/maintainers/#decision-making","title":"Decision Making","text":""},{"location":"governance/maintainers/#consensus-model","title":"Consensus Model","text":"<ul> <li>Minor changes: Any maintainer can approve</li> <li>Major changes: Require lead maintainer approval</li> <li>Breaking changes: Require consensus from core maintainers</li> <li>Architecture changes: Require community discussion</li> </ul>"},{"location":"governance/maintainers/#conflict-resolution","title":"Conflict Resolution","text":"<ol> <li>Discussion: Open dialogue in issues or discussions</li> <li>Technical review: Evaluate technical merits</li> <li>Community input: Gather broader community feedback</li> <li>Final decision: Lead maintainer makes final call if needed</li> </ol>"},{"location":"governance/maintainers/#communication","title":"Communication","text":""},{"location":"governance/maintainers/#channels","title":"Channels","text":"<ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>GitHub Discussions: General questions and ideas</li> <li>Pull Requests: Code changes and reviews</li> <li>Email: Security issues and sensitive matters</li> </ul>"},{"location":"governance/maintainers/#response-times","title":"Response Times","text":"<p>Target response times for maintainers:</p> <ul> <li>Security issues: 48 hours</li> <li>Critical bugs: 72 hours</li> <li>General issues: 1 week</li> <li>Pull requests: 1 week</li> <li>Feature requests: 2 weeks</li> </ul>"},{"location":"governance/maintainers/#meetings","title":"Meetings","text":"<ul> <li>Monthly sync: Core maintainer coordination (as needed)</li> <li>Release planning: Before major releases</li> <li>Ad-hoc: For urgent issues or major decisions</li> </ul>"},{"location":"governance/maintainers/#support","title":"Support","text":""},{"location":"governance/maintainers/#community-support","title":"Community Support","text":"<ul> <li>Documentation: Comprehensive guides and API docs</li> <li>Examples: Working code examples and tutorials</li> <li>FAQ: Common questions and solutions</li> <li>Discussions: Community Q&amp;A forum</li> </ul>"},{"location":"governance/maintainers/#professional-support","title":"Professional Support","text":"<p>For organizations requiring professional support:</p> <ul> <li>Consulting: Custom integration and optimization</li> <li>Training: Team training and best practices</li> <li>SLA: Service level agreements for critical applications</li> </ul> <p>Contact: support@gfpgan.ai</p> <p>Want to become a maintainer? Start by contributing and engaging with the community. See our contributing guide for details.</p>"},{"location":"governance/privacy/","title":"Privacy Policy (Draft)","text":"<p>This is a placeholder for the project privacy guidelines.</p> <p>Scope:</p> <ul> <li>Data handling for samples and metrics</li> <li>Telemetry: none by default</li> </ul> <p>TODO: Provide clear guidance and procedures.</p>"},{"location":"governance/security/","title":"Security Policy","text":""},{"location":"governance/security/#reporting-security-vulnerabilities","title":"Reporting Security Vulnerabilities","text":"<p>We take the security of GFPGAN seriously. If you discover a security vulnerability, please follow these guidelines for responsible disclosure.</p>"},{"location":"governance/security/#private-reporting","title":"Private Reporting","text":"<p>For security issues, please do not create public GitHub issues. Instead, report vulnerabilities privately:</p> <ol> <li>Email: Send details to security@gfpgan.ai</li> <li>GitHub: Use private vulnerability reporting</li> </ol>"},{"location":"governance/security/#what-to-include","title":"What to Include","text":"<p>When reporting a vulnerability, please provide:</p> <ul> <li>Description: Clear explanation of the issue</li> <li>Impact: What an attacker could accomplish</li> <li>Reproduction: Step-by-step instructions to reproduce</li> <li>Environment: Operating system, Python version, GFPGAN version</li> <li>Proof of concept: Code or screenshots (if applicable)</li> </ul>"},{"location":"governance/security/#our-response-process","title":"Our Response Process","text":"<ol> <li>Acknowledgment: We'll confirm receipt within 48 hours</li> <li>Assessment: Initial assessment within 5 business days</li> <li>Updates: Regular status updates every 5 business days</li> <li>Resolution: Coordinated disclosure once fixed</li> </ol>"},{"location":"governance/security/#supported-versions","title":"Supported Versions","text":"<p>We provide security updates for:</p> Version Supported 1.4.x \u2705 Full support 1.3.x \u2705 Security fixes &lt; 1.3 \u274c No support"},{"location":"governance/security/#security-best-practices","title":"Security Best Practices","text":"<p>When using GFPGAN:</p>"},{"location":"governance/security/#input-validation","title":"Input Validation","text":"<ul> <li>Validate file types: Only process trusted image formats</li> <li>Size limits: Implement reasonable file size restrictions</li> <li>Sanitize paths: Validate input/output file paths</li> <li>Network isolation: Run processing in isolated environments</li> </ul> <pre><code># Example: Safe file handling\ndef safe_process_image(file_path):\n    # Validate file extension\n    allowed_extensions = {'.jpg', '.jpeg', '.png', '.webp'}\n    if not any(file_path.lower().endswith(ext) for ext in allowed_extensions):\n        raise ValueError(\"Unsupported file format\")\n\n    # Validate file size (e.g., max 50MB)\n    if os.path.getsize(file_path) &gt; 50 * 1024 * 1024:\n        raise ValueError(\"File too large\")\n\n    # Process with GFPGAN\n    return gfpgan_infer(file_path)\n</code></pre>"},{"location":"governance/security/#api-security","title":"API Security","text":"<ul> <li>Rate limiting: Implement request rate limits</li> <li>Authentication: Secure API endpoints appropriately</li> <li>Input sanitization: Validate all API inputs</li> <li>Output filtering: Don't expose internal paths or errors</li> </ul>"},{"location":"governance/security/#model-security","title":"Model Security","text":"<ul> <li>Verify checksums: Validate model file integrity</li> <li>Trusted sources: Only download models from official sources</li> <li>Isolation: Run inference in sandboxed environments</li> <li>Resource limits: Set memory and computation limits</li> </ul>"},{"location":"governance/security/#known-security-considerations","title":"Known Security Considerations","text":""},{"location":"governance/security/#model-files","title":"Model Files","text":"<ul> <li>GFPGAN uses PyTorch <code>.pth</code> model files</li> <li>These files can contain arbitrary Python code</li> <li>Only use models from trusted sources</li> <li>Verify file checksums when available</li> </ul>"},{"location":"governance/security/#dependencies","title":"Dependencies","text":"<ul> <li>Regular dependency updates via Dependabot</li> <li>Security scanning with CodeQL</li> <li>Vulnerability monitoring for third-party packages</li> </ul>"},{"location":"governance/security/#processing-risks","title":"Processing Risks","text":"<ul> <li>Memory exhaustion: Large images can cause OOM</li> <li>Path traversal: Malicious file paths could access system files</li> <li>Model poisoning: Malicious models could execute arbitrary code</li> </ul>"},{"location":"governance/security/#security-features","title":"Security Features","text":""},{"location":"governance/security/#built-in-protections","title":"Built-in Protections","text":"<ul> <li>Input validation: File format and size checking</li> <li>Error handling: Graceful failure without information leakage</li> <li>Resource management: Memory and GPU memory cleanup</li> <li>Dependency pinning: Locked dependency versions for reproducibility</li> </ul>"},{"location":"governance/security/#optional-security-enhancements","title":"Optional Security Enhancements","text":"<pre><code># Run in restricted container\ndocker run --rm -it --security-opt=no-new-privileges \\\n  --cap-drop=ALL --user=1000:1000 gfpgan:latest\n\n# Process with resource limits\ntimeout 300 gfpgan-infer --input photo.jpg --device cpu\n</code></pre>"},{"location":"governance/security/#disclosure-timeline","title":"Disclosure Timeline","text":"<ul> <li>Day 0: Vulnerability reported privately</li> <li>Day 2: Acknowledgment sent to reporter</li> <li>Day 7: Initial assessment and triage</li> <li>Day 14-30: Fix development and testing</li> <li>Day 30-60: Coordinated disclosure and release</li> <li>Day 60+: Public disclosure (if fix is available)</li> </ul>"},{"location":"governance/security/#security-updates","title":"Security Updates","text":"<p>Security updates are released as:</p> <ol> <li>Patch versions: Critical security fixes (e.g., 1.4.1 \u2192 1.4.2)</li> <li>GitHub Security Advisories: Public disclosure with details</li> <li>CVE assignments: For significant vulnerabilities</li> <li>Release notes: Clear security fix descriptions</li> </ol>"},{"location":"governance/security/#contact-information","title":"Contact Information","text":"<ul> <li>Security Email: security@gfpgan.ai</li> <li>PGP Key: Available at https://gfpgan.ai/.well-known/pgp-key.txt</li> <li>GitHub Security: https://github.com/IAmJonoBo/Restoria/security</li> </ul>"},{"location":"governance/security/#acknowledgments","title":"Acknowledgments","text":"<p>We appreciate responsible security researchers and will acknowledge contributions in:</p> <ul> <li>Security advisories</li> <li>Release notes</li> <li>Hall of Fame (with permission)</li> </ul> <p>Questions about security? Contact us at security@gfpgan.ai or review our contributing guidelines.</p>"},{"location":"governance/versioning/","title":"Versioning Policy","text":"<p>GFPGAN follows Semantic Versioning (SemVer) for releases and maintains clear backward compatibility guarantees.</p>"},{"location":"governance/versioning/#version-format","title":"Version Format","text":"<p>Versions follow the format: <code>MAJOR.MINOR.PATCH</code></p> <ul> <li>MAJOR: Breaking changes that require user action</li> <li>MINOR: New features that are backward compatible</li> <li>PATCH: Bug fixes and security updates</li> </ul>"},{"location":"governance/versioning/#examples","title":"Examples","text":"<ul> <li><code>1.4.0</code> \u2192 <code>1.4.1</code>: Patch release (bug fixes)</li> <li><code>1.4.0</code> \u2192 <code>1.5.0</code>: Minor release (new features)</li> <li><code>1.4.0</code> \u2192 <code>2.0.0</code>: Major release (breaking changes)</li> </ul>"},{"location":"governance/versioning/#release-types","title":"Release Types","text":""},{"location":"governance/versioning/#patch-releases-xyz","title":"Patch Releases (x.y.Z)","text":"<p>What's included:</p> <ul> <li>Bug fixes</li> <li>Security updates</li> <li>Documentation improvements</li> <li>Performance optimizations (without API changes)</li> </ul> <p>Backward compatibility: \u2705 Fully compatible</p> <p>Example: <code>1.4.0</code> \u2192 <code>1.4.1</code></p> <pre><code># Safe to upgrade\npip install --upgrade gfpgan\n</code></pre>"},{"location":"governance/versioning/#minor-releases-xyz","title":"Minor Releases (x.Y.z)","text":"<p>What's included:</p> <ul> <li>New features</li> <li>New CLI options</li> <li>New API methods</li> <li>Model improvements</li> <li>Dependency updates (compatible)</li> </ul> <p>Backward compatibility: \u2705 Fully compatible</p> <p>Example: <code>1.4.0</code> \u2192 <code>1.5.0</code></p> <pre><code># Safe to upgrade, new features available\npip install --upgrade gfpgan\n</code></pre>"},{"location":"governance/versioning/#major-releases-xyz","title":"Major Releases (X.y.z)","text":"<p>What's included:</p> <ul> <li>Breaking API changes</li> <li>Removed deprecated features</li> <li>Incompatible dependency updates</li> <li>Major architecture changes</li> </ul> <p>Backward compatibility: \u274c May require code changes</p> <p>Example: <code>1.4.0</code> \u2192 <code>2.0.0</code></p> <pre><code># Review migration guide before upgrading\npip install --upgrade gfpgan\n</code></pre>"},{"location":"governance/versioning/#public-api-definition","title":"Public API Definition","text":"<p>Our public API includes:</p>"},{"location":"governance/versioning/#command-line-interface","title":"Command Line Interface","text":"<pre><code># Stable CLI commands\ngfpgan-infer --input photo.jpg --version 1.4\ngfpgan-doctor\n</code></pre>"},{"location":"governance/versioning/#python-api","title":"Python API","text":"<pre><code># Core classes and functions\nfrom gfpgan import GFPGANer\nfrom gfpgan.utils import restore_image\n\n# Public methods and parameters\nrestorer = GFPGANer(model_path='...', upscale=2)\nresult = restorer.enhance(image, has_aligned=False)\n</code></pre>"},{"location":"governance/versioning/#configuration-files","title":"Configuration Files","text":"<pre><code># Model registry format\nmodels:\n  gfpgan_v1.4:\n    url: \"...\"\n    sha256: \"...\"\n</code></pre>"},{"location":"governance/versioning/#compatibility-guarantees","title":"Compatibility Guarantees","text":""},{"location":"governance/versioning/#within-major-versions","title":"Within Major Versions","text":"<ul> <li>CLI commands: Existing commands continue to work</li> <li>API methods: Public methods maintain signatures</li> <li>Configuration: Existing configs remain valid</li> <li>Model files: Compatible model formats</li> </ul>"},{"location":"governance/versioning/#deprecation-policy","title":"Deprecation Policy","text":"<p>Before removing features:</p> <ol> <li>Deprecation warning: Added in minor release</li> <li>Documentation: Updated with alternatives</li> <li>Migration guide: Provided for complex changes</li> <li>Removal: In next major release (minimum 6 months)</li> </ol> <p>Example deprecation: <pre><code># v1.4.0 - deprecation warning\nwarnings.warn(\"restore_face() is deprecated, use enhance() instead\")\n\n# v2.0.0 - removal\n# restore_face() method removed\n</code></pre></p>"},{"location":"governance/versioning/#release-schedule","title":"Release Schedule","text":""},{"location":"governance/versioning/#regular-releases","title":"Regular Releases","text":"<ul> <li>Patch releases: As needed for critical fixes</li> <li>Minor releases: Every 2-3 months</li> <li>Major releases: Every 12-18 months</li> </ul>"},{"location":"governance/versioning/#security-releases","title":"Security Releases","text":"<ul> <li>Critical vulnerabilities: Within 48 hours</li> <li>High severity: Within 1 week</li> <li>Medium/Low severity: Next regular release</li> </ul>"},{"location":"governance/versioning/#support-windows","title":"Support Windows","text":""},{"location":"governance/versioning/#active-support","title":"Active Support","text":"Version Release Date End of Support Security Fixes 1.4.x 2024-Q4 2025-Q4 \u2705 Yes 1.3.x 2024-Q2 2024-Q4 \u2705 Yes"},{"location":"governance/versioning/#legacy-support","title":"Legacy Support","text":"<ul> <li>Bug fixes: Current major version only</li> <li>Security fixes: Current + previous major version</li> <li>New features: Current major version only</li> </ul>"},{"location":"governance/versioning/#pre-release-versions","title":"Pre-release Versions","text":""},{"location":"governance/versioning/#alpha-releases","title":"Alpha Releases","text":"<ul> <li>Format: <code>1.5.0a1</code>, <code>1.5.0a2</code></li> <li>Purpose: Early feature testing</li> <li>Stability: Unstable, may have breaking changes</li> <li>Availability: GitHub releases only</li> </ul>"},{"location":"governance/versioning/#beta-releases","title":"Beta Releases","text":"<ul> <li>Format: <code>1.5.0b1</code>, <code>1.5.0b2</code></li> <li>Purpose: Feature-complete testing</li> <li>Stability: More stable, API frozen</li> <li>Availability: GitHub releases and PyPI</li> </ul>"},{"location":"governance/versioning/#release-candidates","title":"Release Candidates","text":"<ul> <li>Format: <code>1.5.0rc1</code>, <code>1.5.0rc2</code></li> <li>Purpose: Final testing before release</li> <li>Stability: Production-ready candidate</li> <li>Availability: GitHub releases and PyPI</li> </ul>"},{"location":"governance/versioning/#migration-support","title":"Migration Support","text":""},{"location":"governance/versioning/#migration-guides","title":"Migration Guides","text":"<p>For major releases, we provide:</p> <ul> <li>Step-by-step instructions: How to update code</li> <li>Breaking change summaries: What changed and why</li> <li>Compatibility shims: Temporary compatibility layers</li> <li>Example migrations: Before/after code examples</li> </ul>"},{"location":"governance/versioning/#tools","title":"Tools","text":"<pre><code># Check compatibility with new version\ngfpgan-doctor --check-compatibility 2.0.0\n\n# Automated migration assistance\ngfpgan-migrate --from 1.4 --to 2.0 --scan ./my_project\n</code></pre>"},{"location":"governance/versioning/#version-information","title":"Version Information","text":""},{"location":"governance/versioning/#runtime-version-check","title":"Runtime Version Check","text":"<pre><code>import gfpgan\nprint(gfpgan.__version__)  # \"1.4.1\"\n\n# Programmatic version comparison\nfrom packaging import version\nif version.parse(gfpgan.__version__) &gt;= version.parse(\"1.4.0\"):\n    # Use new features\n    pass\n</code></pre>"},{"location":"governance/versioning/#cli-version-check","title":"CLI Version Check","text":"<pre><code>gfpgan-infer --version\n# GFPGAN 1.4.1\n</code></pre>"},{"location":"governance/versioning/#documentation-versioning","title":"Documentation Versioning","text":""},{"location":"governance/versioning/#docs-site-versioning","title":"Docs Site Versioning","text":"<ul> <li>Latest: Current stable release</li> <li>Development: Main branch (unreleased)</li> <li>Historical: Previous major versions</li> </ul> <p>Access via: <code>https://gfpgan.ai/docs/v1.4/</code></p>"},{"location":"governance/versioning/#api-documentation","title":"API Documentation","text":"<ul> <li>Stable: Generated from release tags</li> <li>Development: Generated from main branch</li> <li>Legacy: Maintained for supported versions</li> </ul>"},{"location":"governance/versioning/#change-communication","title":"Change Communication","text":""},{"location":"governance/versioning/#changelog","title":"Changelog","text":"<p>All releases include detailed changelogs:</p> <ul> <li>Added: New features</li> <li>Changed: Modifications to existing features</li> <li>Deprecated: Features marked for removal</li> <li>Removed: Deleted features</li> <li>Fixed: Bug fixes</li> <li>Security: Security improvements</li> </ul>"},{"location":"governance/versioning/#release-notes","title":"Release Notes","text":"<p>Major releases include:</p> <ul> <li>Upgrade guide: Step-by-step instructions</li> <li>Highlights: Key new features</li> <li>Breaking changes: What requires code changes</li> <li>Performance improvements: Benchmark comparisons</li> </ul>"},{"location":"governance/versioning/#notifications","title":"Notifications","text":"<ul> <li>GitHub releases: Automatic notifications</li> <li>PyPI: Package update notifications</li> <li>Documentation: Version-specific announcements</li> </ul> <p>Questions about versioning? See our FAQ or contributing guide.</p>"},{"location":"guides/auto-mode/","title":"Auto mode (experimental, opt-in)","text":"<p>Auto mode selects a backend and parameters per image using lightweight probes. It is disabled by default and can be enabled with:</p> <ul> <li>CLI: <code>gfpup run --auto --input &lt;path&gt; --output out/</code></li> <li>API/UI: toggle the auto backend option</li> </ul> <p>What it does (transparent rules):</p> <ul> <li>Probes image quality (NIQE, BRISQUE where available)</li> <li>Routes to GFPGAN (clean), CodeFormer (more fidelity on strong degradation),   or keeps your choice</li> <li>Suggests background upsampling (Real-ESRGAN by default) when global quality   remains low</li> </ul> <p>Advanced metrics (optional)</p> <ul> <li>If installed, advanced no-reference IQA (e.g., MANIQA/CONTRIQUE) may be   recorded by the planner.</li> <li>These are purely informational today and do not change routing unless we add   explicit opt-in rules.</li> </ul> <p>Ensemble mode</p> <ul> <li>Auto mode never selects ensemble implicitly. Use <code>--backend ensemble</code>   explicitly to enable blending.</li> </ul> <p>Notes</p> <ul> <li>Probes are best-effort and skipped if dependencies are missing. Behavior   gracefully falls back to your chosen backend.</li> <li>Identity guardrails are part of the metrics/identity lock path and remain opt-in.</li> </ul> <p>TODO</p> <ul> <li>Incorporate face size/count signals and richer thresholds</li> <li>Show the plan explanation in the UI</li> </ul>"},{"location":"guides/batch-processing/","title":"Batch processing","text":"<p>Process entire folders of images with consistent settings and quality tracking.</p>"},{"location":"guides/batch-processing/#quick-start","title":"Quick start","text":"<p>Restore all images in a folder:</p> <pre><code>gfpgan-infer --input photos/ --output results/ --version 1.4\n</code></pre> <p>GFPGAN automatically processes all supported image formats (JPG, PNG, WEBP) in the input folder.</p>"},{"location":"guides/batch-processing/#basic-batch-operations","title":"Basic batch operations","text":""},{"location":"guides/batch-processing/#process-a-folder","title":"Process a folder","text":"Default settingsCustom output folderWith quality metrics <pre><code>gfpgan-infer --input photos/ --version 1.4\n</code></pre> <pre><code>gfpgan-infer --input photos/ --output restored_photos/ --version 1.4\n</code></pre> <pre><code>gfpgan-infer --input photos/ --metrics fast --report-path batch_report.json\n</code></pre>"},{"location":"guides/batch-processing/#supported-file-formats","title":"Supported file formats","text":"<p>GFPGAN processes these image formats: - JPEG/JPG - Most common format - PNG - Lossless format, good for high quality - WEBP - Modern format with good compression - BMP - Uncompressed bitmap - TIFF - High quality, often used professionally</p>"},{"location":"guides/batch-processing/#advanced-batch-processing","title":"Advanced batch processing","text":""},{"location":"guides/batch-processing/#choose-processing-backend","title":"Choose processing backend","text":"Quality-focusedSpeed-focusedBalanced <pre><code># Best quality, slower processing\ngfpgan-infer --input photos/ --backend restoreformer --metrics detailed\n</code></pre> <pre><code># Faster processing, good quality\ngfpgan-infer --input photos/ --backend codeformer --bg_upsampler none\n</code></pre> <pre><code># Good balance of speed and quality\ngfpgan-infer --input photos/ --version 1.4 --metrics fast\n</code></pre>"},{"location":"guides/batch-processing/#batch-with-background-enhancement","title":"Batch with background enhancement","text":"Full enhancementFace-only (faster) <pre><code># Restore faces and enhance backgrounds\ngfpgan-infer --input photos/ --bg_upsampler realesrgan --upscale 2\n</code></pre> <pre><code># Skip background processing for speed\ngfpgan-infer --input photos/ --bg_upsampler none\n</code></pre>"},{"location":"guides/batch-processing/#device-and-memory-management","title":"Device and memory management","text":"Auto device selectionForce CPU (low memory)GPU with memory limits <pre><code>gfpgan-infer --input photos/ --device auto\n</code></pre> <pre><code>gfpgan-infer --input photos/ --device cpu\n</code></pre> <pre><code># Process smaller batches if GPU memory is limited\ngfpgan-infer --input photos/ --device cuda --bg_tile 200\n</code></pre>"},{"location":"guides/batch-processing/#quality-tracking-and-metrics","title":"Quality tracking and metrics","text":""},{"location":"guides/batch-processing/#enable-quality-metrics","title":"Enable quality metrics","text":"Fast metrics (LPIPS only)Detailed metrics (LPIPS, DISTS, ArcFace)No metrics (fastest) <pre><code>gfpgan-infer --input photos/ --metrics fast --report-path quick_report.json\n</code></pre> <pre><code>gfpgan-infer --input photos/ --metrics detailed --report-path full_report.json\n</code></pre> <pre><code>gfpgan-infer --input photos/ --metrics none\n</code></pre>"},{"location":"guides/batch-processing/#understanding-metric-reports","title":"Understanding metric reports","text":"<p>The generated JSON report includes:</p> <pre><code>{\n    \"summary\": {\n        \"total_images\": 150,\n        \"successful\": 147,\n        \"failed\": 3,\n        \"avg_lpips\": 0.234,\n        \"avg_processing_time\": 2.3\n    },\n    \"per_image\": {\n        \"photo001.jpg\": {\n            \"lpips\": 0.198,\n            \"dists\": 0.156,\n            \"arcface_similarity\": 0.892,\n            \"processing_time\": 2.1,\n            \"faces_detected\": 1\n        }\n    }\n}\n</code></pre> <p>Metric meanings: - LPIPS: Lower is better (perceptual similarity) - DISTS: Lower is better (structural similarity) - ArcFace: Higher is better (identity preservation)</p>"},{"location":"guides/batch-processing/#provenance-and-reproducibility","title":"Provenance and reproducibility","text":""},{"location":"guides/batch-processing/#deterministic-processing","title":"Deterministic processing","text":"<p>For reproducible results across runs:</p> <pre><code>gfpgan-infer --input photos/ --deterministic --random-seed 42\n</code></pre>"},{"location":"guides/batch-processing/#preserve-metadata","title":"Preserve metadata","text":"<p>Keep original EXIF data:</p> <pre><code>gfpgan-infer --input photos/ --preserve-metadata\n</code></pre>"},{"location":"guides/batch-processing/#provenance-tracking","title":"Provenance tracking","text":"<p>GFPGAN automatically saves processing information:</p> <pre><code>results/\n\u251c\u2500\u2500 restored_photos/         # Processed images\n\u251c\u2500\u2500 batch_report.json       # Quality metrics\n\u251c\u2500\u2500 processing_log.txt      # Detailed log\n\u2514\u2500\u2500 provenance.json         # Settings and environment info\n</code></pre>"},{"location":"guides/batch-processing/#monitoring-progress","title":"Monitoring progress","text":""},{"location":"guides/batch-processing/#progress-indicators","title":"Progress indicators","text":"Simple progress barDetailed loggingSilent mode <pre><code>gfpgan-infer --input photos/ --progress\n</code></pre> <pre><code>gfpgan-infer --input photos/ --verbose --log-file batch.log\n</code></pre> <pre><code>gfpgan-infer --input photos/ --quiet\n</code></pre>"},{"location":"guides/batch-processing/#handling-interruptions","title":"Handling interruptions","text":"<p>If processing is interrupted:</p> <pre><code># Resume from where it left off\ngfpgan-infer --input photos/ --resume --checkpoint-dir .checkpoints/\n</code></pre>"},{"location":"guides/batch-processing/#presets-and-automation","title":"Presets and automation","text":""},{"location":"guides/batch-processing/#create-processing-presets","title":"Create processing presets","text":"<p>Save commonly used settings:</p> High quality presetFast processing preset <pre><code># Create alias or script\nalias gfpgan-hq='gfpgan-infer --version 1.4 --bg_upsampler realesrgan --metrics detailed'\n\n# Use the preset\ngfpgan-hq --input photos/ --output results/\n</code></pre> <pre><code>alias gfpgan-fast='gfpgan-infer --backend codeformer --bg_upsampler none --metrics fast'\n\ngfpgan-fast --input photos/ --output results/\n</code></pre>"},{"location":"guides/batch-processing/#automation-scripts","title":"Automation scripts","text":"Bash script examplePython script example <pre><code>#!/bin/bash\n# batch_restore.sh\n\nINPUT_DIR=\"$1\"\nOUTPUT_DIR=\"$2\"\n\nif [ -z \"$INPUT_DIR\" ] || [ -z \"$OUTPUT_DIR\" ]; then\n    echo \"Usage: $0 &lt;input_dir&gt; &lt;output_dir&gt;\"\n    exit 1\nfi\n\ngfpgan-infer \\\n    --input \"$INPUT_DIR\" \\\n    --output \"$OUTPUT_DIR\" \\\n    --version 1.4 \\\n    --metrics detailed \\\n    --progress \\\n    --report-path \"$OUTPUT_DIR/quality_report.json\"\n</code></pre> <pre><code>#!/usr/bin/env python3\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef batch_restore(input_dir, output_dir, backend=\"gfpgan\", version=\"1.4\"):\n    cmd = [\n        \"gfpgan-infer\",\n        \"--input\", str(input_dir),\n        \"--output\", str(output_dir),\n        \"--backend\", backend,\n        \"--version\", version,\n        \"--metrics\", \"detailed\",\n        \"--progress\"\n    ]\n\n    subprocess.run(cmd, check=True)\n\nif __name__ == \"__main__\":\n    batch_restore(sys.argv[1], sys.argv[2])\n</code></pre>"},{"location":"guides/batch-processing/#troubleshooting-batch-jobs","title":"Troubleshooting batch jobs","text":""},{"location":"guides/batch-processing/#common-issues","title":"Common issues","text":"<p>Out of memory during batch processing</p> <p>Solutions: <pre><code># Use CPU mode\ngfpgan-infer --input photos/ --device cpu\n\n# Reduce background tile size\ngfpgan-infer --input photos/ --bg_tile 200\n\n# Disable background enhancement\ngfpgan-infer --input photos/ --bg_upsampler none\n</code></pre></p> <p>Some images failed to process</p> <p>Check the processing log: <pre><code>gfpgan-infer --input photos/ --log-file processing.log --verbose\n</code></pre> Common causes: - Corrupted image files - Unsupported format - No faces detected</p> <p>Processing is too slow</p> <p>Speed optimizations: <pre><code># Use faster backend\ngfpgan-infer --input photos/ --backend codeformer\n\n# Skip metrics\ngfpgan-infer --input photos/ --metrics none\n\n# Disable background processing\ngfpgan-infer --input photos/ --bg_upsampler none\n</code></pre></p>"},{"location":"guides/batch-processing/#performance-tips","title":"Performance tips","text":"<ol> <li>Sort by file size: Process smaller images first to get quick feedback</li> <li>Use SSD storage: Faster I/O improves batch processing speed</li> <li>Monitor GPU memory: Use <code>nvidia-smi</code> to watch memory usage</li> <li>Parallel processing: For very large batches, split into chunks</li> </ol> <p>Next steps: - Measure quality with metrics \u2192 - Choose the right backend \u2192 - Optimize hardware performance \u2192</p>"},{"location":"guides/choose-backend/","title":"Choose backend","text":"<p>Compare restoration backends and select the best one for your needs.</p>"},{"location":"guides/choose-backend/#backend-comparison-matrix","title":"Backend comparison matrix","text":"Backend Speed Quality Identity Memory Best for GFPGAN v1.4 Medium High Excellent Medium General photos GFPGAN v1.3 Medium High Good Medium Natural results GFPGAN v1.2 Medium Good Good Medium Sharp details CodeFormer Fast Medium Good Low Batch processing RestoreFormer++ Slow Highest Excellent High Professional work"},{"location":"guides/choose-backend/#detailed-comparisons","title":"Detailed comparisons","text":""},{"location":"guides/choose-backend/#gfpgan-models","title":"GFPGAN models","text":"GFPGAN v1.4 (Recommended)GFPGAN v1.3 (Natural)GFPGAN v1.2 (Sharp) <pre><code># New CLI\ngfpup run --input photo.jpg --backend gfpgan --output out/\n\n# Legacy shim\ngfpgan-infer --input photo.jpg --version 1.4\n</code></pre> <p>Strengths: - Excellent detail preservation - Good identity preservation - Works well on various photo types - Balanced speed/quality trade-off</p> <p>Best for: - General portrait restoration - Mixed photo collections - Production workflows</p> <p>Example use case: Family photo restoration, professional headshots</p> <pre><code># New CLI (select model via param if supported)\ngfpup run --input photo.jpg --backend gfpgan --output out/\n\n# Legacy shim\ngfpgan-infer --input photo.jpg --version 1.3\n</code></pre> <p>Strengths: - More natural-looking results - Less artificial sharpening - Good for low-quality inputs - Handles repeated restoration well</p> <p>Weaknesses: - Slightly less sharp than v1.4 - Some identity changes possible</p> <p>Best for: - Natural photo restoration - Social media photos - When subtlety is preferred</p> <p>Example use case: Old family photos, vintage portraits</p> <pre><code># Legacy shim example only\ngfpgan-infer --input photo.jpg --version 1.2\n</code></pre> <p>Strengths: - Very sharp output - Good detail enhancement - Suitable for beauty/makeup photos</p> <p>Weaknesses: - Can look unnatural - May over-enhance features</p> <p>Best for: - Beauty photography - When maximum sharpness is needed - Fashion/glamour photos</p>"},{"location":"guides/choose-backend/#alternative-backends","title":"Alternative backends","text":"CodeFormer (Fast)RestoreFormer++ (Premium) <pre><code># New CLI\ngfpup run --input photo.jpg --backend codeformer --output out/\n\n# Legacy shim\ngfpgan-infer --input photo.jpg --backend codeformer\n</code></pre> <p>Strengths: - Fastest processing - Lower memory usage - Good identity preservation - Controllable restoration strength</p> <p>Weaknesses: - Lower detail quality than GFPGAN - Less sophisticated texture handling</p> <p>Best for: - Large batch processing - Resource-constrained environments - Quick previews</p> <p>Example use case: Processing thousands of photos, real-time applications</p> <pre><code># New CLI uses canonical name restoreformerpp\ngfpup run --input photo.jpg --backend restoreformerpp --output out/\n\n# Legacy shim alias\ngfpgan-infer --input photo.jpg --backend restoreformer\n</code></pre> <p>Strengths: - Highest quality results - Excellent identity preservation - Superior texture reconstruction - Advanced face parsing</p> <p>Weaknesses: - Slowest processing - Highest memory usage - May require more powerful hardware</p> <p>Best for: - Professional photo restoration - High-value images - When quality is paramount</p> <p>Example use case: Archive restoration, professional photography, art restoration</p>"},{"location":"guides/choose-backend/#selection-guidelines","title":"Selection guidelines","text":""},{"location":"guides/choose-backend/#by-use-case","title":"By use case","text":"Personal photosProfessional workBatch processingArchive restoration <p>Recommended: GFPGAN v1.4 <pre><code>gfpgan-infer --input family_photo.jpg --version 1.4\n</code></pre> - Good balance of quality and speed - Preserves natural appearance - Handles various lighting conditions</p> <p>Recommended: RestoreFormer++ <pre><code>    gfpup run \\\n        --input portrait.jpg \\\n        --backend restoreformerpp \\\n        --metrics full \\\n        --output out/\n</code></pre> - Highest quality output - Excellent for client work - Detailed quality metrics</p> <p>Recommended: CodeFormer <pre><code>gfpup run --input photos/ --backend codeformer --output out/\n</code></pre> - Fastest processing - Lower resource usage - Still produces good results</p> <p>Recommended: GFPGAN v1.3 or RestoreFormer++ <pre><code># For natural results\ngfpgan-infer --input old_photo.jpg --version 1.3\n\n# For maximum quality\ngfpup run --input old_photo.jpg --backend restoreformerpp --output out/\n</code></pre></p>"},{"location":"guides/choose-backend/#by-hardware","title":"By hardware","text":"High-end GPU (8GB+ VRAM)Mid-range GPU (4-8GB VRAM)Low-end GPU (&lt;4GB VRAM)CPU only <ul> <li>\u2705 All backends supported</li> <li>Recommended: RestoreFormer++ for quality</li> <li>Alternative: GFPGAN v1.4 for speed</li> </ul> <pre><code>gfpup run --input photo.jpg --backend restoreformerpp --output out/ --compile\n</code></pre> <ul> <li>\u2705 GFPGAN (all versions)</li> <li>\u2705 CodeFormer</li> <li>\u26a0\ufe0f RestoreFormer++ (may need CPU fallback)</li> </ul> <pre><code>gfpgan-infer --input photo.jpg --version 1.4 --bg_tile 400\n</code></pre> <ul> <li>\u2705 CodeFormer</li> <li>\u2705 GFPGAN with reduced settings</li> <li>\u274c RestoreFormer++ (use CPU)</li> </ul> <pre><code>gfpup run --input photo.jpg --backend codeformer --output out/\n</code></pre> <ul> <li>\u2705 All backends (slower)</li> <li>Recommended: CodeFormer for speed</li> <li>Disable background enhancement</li> </ul> <pre><code>gfpup run --input photo.jpg --device cpu --backend codeformer --output out/\n</code></pre>"},{"location":"guides/choose-backend/#performance-benchmarks","title":"Performance benchmarks","text":""},{"location":"guides/choose-backend/#processing-speed-avg-per-image","title":"Processing speed (avg. per image)","text":"Backend GPU (RTX 3080) CPU (Intel i7) CodeFormer 0.8s 12s GFPGAN v1.4 1.2s 18s GFPGAN v1.3 1.1s 17s RestoreFormer++ 2.4s 45s"},{"location":"guides/choose-backend/#memory-usage","title":"Memory usage","text":"Backend GPU Memory System RAM CodeFormer 2GB 4GB GFPGAN v1.4 3GB 6GB GFPGAN v1.3 3GB 6GB RestoreFormer++ 5GB 8GB <p>Note: benchmarks based on 512x512 input images with background enhancement.</p>"},{"location":"guides/choose-backend/#quality-evaluation","title":"Quality evaluation","text":""},{"location":"guides/choose-backend/#objective-metrics","title":"Objective metrics","text":"<p>Use built-in metrics to compare backends:</p> <pre><code>    # Test multiple backends on the same image\n    gfpup run --input test_photo.jpg \\\n        --backend gfpgan --metrics full --output v14/\n    gfpup run --input test_photo.jpg \\\n        --backend codeformer --metrics full --output cf/\n    gfpup run \\\n        --input test_photo.jpg \\\n        --backend restoreformerpp \\\n        --metrics full \\\n        --output rf/\n</code></pre> <p>Typical metric ranges:</p> <ul> <li>LPIPS: 0.1-0.4 (lower is better)</li> <li>DISTS: 0.1-0.3 (lower is better)</li> <li>ArcFace: 0.7-0.95 (higher is better)</li> </ul>"},{"location":"guides/choose-backend/#visual-quality-checklist","title":"Visual quality checklist","text":"<p>When comparing results, look for:</p> <ul> <li>Sharpness: Are facial features well-defined?</li> <li>Naturalness: Does the person look realistic?</li> <li>Identity: Is the person still recognizable?</li> <li>Artifacts: Any unnatural textures or distortions?</li> <li>Consistency: Similar quality across multiple faces?</li> </ul>"},{"location":"guides/choose-backend/#advanced-configuration","title":"Advanced configuration","text":""},{"location":"guides/choose-backend/#optional-ensemble-experimental","title":"Optional ensemble (experimental)","text":"<p>You can blend outputs from multiple backends by selecting the ensemble backend. This is off by default and requires no extra installs for a simple blend.</p> <p>Example:</p> <pre><code>    gfpup run --input photo.jpg \\\n        --backend ensemble \\\n        --ensemble-backends gfpgan,codeformer \\\n        --ensemble-weights 0.5,0.5 \\\n        --output out/\n</code></pre> <p>Missing backends are skipped gracefully and the run proceeds.</p>"},{"location":"guides/choose-backend/#backend-specific-parameters","title":"Backend-specific parameters","text":"GFPGAN tuningCodeFormer tuningRestoreFormer++ tuning <pre><code># Adjust upsampling\ngfpgan-infer --input photo.jpg --version 1.4 --upscale 1  # No upsampling\ngfpgan-infer --input photo.jpg --version 1.4 --upscale 4  # 4x upsampling\n\n# Background tile size (affects memory)\ngfpgan-infer --input photo.jpg --version 1.4 --bg_tile 200  # Smaller tiles\n</code></pre> <pre><code># Restoration strength (if supported)\ngfpgan-infer --input photo.jpg --backend codeformer --fidelity 0.8\n\n# Face detection threshold\ngfpgan-infer --input photo.jpg --backend codeformer --detection_threshold 0.5\n</code></pre> <pre><code># High quality mode\ngfpgan-infer --input photo.jpg --backend restoreformer --high_quality\n\n# Memory optimization\ngfpgan-infer --input photo.jpg --backend restoreformer --memory_efficient\n</code></pre>"},{"location":"guides/choose-backend/#decision-flowchart","title":"Decision flowchart","text":"<pre><code>    Start\n        \u2193\n    Quality most important? \u2192 Yes \u2192 Use RestoreFormer++\n        \u2193 No\n    Speed most important? \u2192 Yes \u2192 Use CodeFormer\n        \u2193 No\n    Natural results preferred? \u2192 Yes \u2192 Use GFPGAN v1.3\n        \u2193 No\n    General use \u2192 Use GFPGAN v1.4\n</code></pre> <p>Next steps:</p> <ul> <li>Measure quality with metrics \u2192</li> <li>Process multiple photos \u2192</li> <li>Optimize hardware performance \u2192</li> </ul>"},{"location":"guides/face-enhancement/","title":"Face Enhancement","text":"<p>This guide walks you through enhancing faces using Restoria.</p> <ul> <li>What you\u2019ll learn:</li> <li>Choosing a backend (GFPGAN, CodeFormer, RestoreFormer++)</li> <li>Tuning options (upscale, strength)</li> <li>Batch vs single-image workflows</li> </ul> <p>Quick start:</p> <pre><code>restoria run --input inputs/whole_imgs --output out/ --backend gfpgan\n</code></pre> <p>Notes:</p> <ul> <li>For device selection, keep <code>--device auto</code> (default) and Restoria will choose.</li> <li>Identity metrics can be enabled with <code>--metrics fast</code> or   <code>--metrics full</code> when available.</li> </ul> <p>TODO:</p> <ul> <li>Add screenshots and recommended settings per backend.</li> </ul>"},{"location":"guides/guided/","title":"Guided backend (experimental, opt-in)","text":"<p>The guided backend lets you provide a reference image to bias restoration toward preserving identity.</p> <ul> <li>Enable:     <code>gfpup run --backend guided --reference path/to/ref.jpg --input &lt;img&gt; --output out/</code></li> <li>Defaults remain unchanged unless you opt in.</li> </ul> <p>What it does:</p> <ul> <li>Computes an identity similarity (ArcFace) between the input and the     reference, when available.</li> <li>If similarity is low, it slightly lowers the restoration weight to retain     more identity.</li> <li>Internally runs GFPGAN with the adjusted weight.</li> </ul> <p>Notes:</p> <ul> <li>If ArcFace or the reference image is unavailable, it gracefully falls back     to normal GFPGAN settings.</li> <li>The plan and metrics record <code>guided.reference</code>, <code>guided.arcface_cosine</code>, and     <code>guided.adjusted_weight</code> when applicable.</li> <li>This is a lightweight heuristic, not face swapping. No model weights are     changed.</li> </ul> <p>Troubleshooting:</p> <ul> <li>Install metrics extras for ArcFace support if you want identity similarity:     <code>pip install -e \".[metrics,arcface]\"</code></li> <li>You can continue using <code>--metrics off</code> and the backend will still run, just     without the guidance tweak.</li> </ul>"},{"location":"guides/hardware/","title":"Performance on your hardware","text":"<p>This project keeps the default backend and outputs unchanged, while providing optional fast paths.</p> <p>Fast paths (opt-in):</p> <ul> <li>torch.compile: <code>--compile default|max</code> to JIT optimize models. Falls back safely on error.</li> <li>ONNX Runtime: CPU is default; if installed, we auto-select the best available EP (CUDA \u2192 TensorRT \u2192 DirectML \u2192 CoreML \u2192 CPU).</li> </ul> <p>Tips:</p> <ul> <li>Use <code>gfpup doctor</code> to see Torch, CUDA and ORT providers on your machine.</li> <li>For laptops with both iGPU/dGPU, ensure the discrete GPU is selected.</li> <li>For CPU-only runs, set <code>--device cpu</code> and prefer <code>--quality quick</code>.</li> </ul> <p>TODO</p> <ul> <li>Add per-backend export helpers and quantization notes.</li> </ul>"},{"location":"guides/metrics/","title":"Quality metrics","text":"<p>Measure restoration quality with optional metrics and learn how to interpret the results.</p> <p>This guide covers both the new Restoria CLI and the transitional gfpup CLI. All metrics are optional and degrade gracefully when dependencies are missing.</p>"},{"location":"guides/metrics/#quick-start","title":"Quick start","text":"<p>Compute metrics during a run and write a metrics file next to outputs:</p> <pre><code># Restoria (recommended)\nrestoria run --input photos/ --output out/ --metrics full\n\n# Legacy helper (maps to the same metrics under the hood)\n</code></pre> <p>Outputs include:</p> <ul> <li>out/metrics.json \u2014 per-image metrics (and a planning block when using   Restoria)</li> <li>out/manifest.json \u2014 run manifest with runtime metadata</li> <li>Optional: CSV/HTML reports (gfpup only): add <code>--csv-out metrics.csv</code>   and/or <code>--html-report report.html</code></li> </ul>"},{"location":"guides/metrics/#metric-presets","title":"Metric presets","text":"<ul> <li>off \u2014 no extra metrics (fastest)</li> <li>fast \u2014 ArcFace identity (if available) only</li> <li>full \u2014 ArcFace + LPIPS-Alex + DISTS, plus no-reference IQA when available</li> </ul> <pre><code>restoria run --input photo.jpg --output out/ --metrics fast\nrestoria run --input photo.jpg --output out/ --metrics full\n</code></pre> <p>Tip: Install extras for metrics: <code>pip install -e .[metrics]</code>. ArcFace is optional; if weights/deps are missing, identity will be skipped.</p>"},{"location":"guides/metrics/#available-metrics","title":"Available metrics","text":""},{"location":"guides/metrics/#arcface-identity-cosine","title":"ArcFace identity cosine","text":"<ul> <li>What it measures: Facial identity preservation between input and restored image</li> <li>Range: 0.0 \u2013 1.0 (higher is better)</li> <li>Typical good range: 0.8 \u2013 0.95</li> </ul>"},{"location":"guides/metrics/#lpips-alexnet","title":"LPIPS (AlexNet)","text":"<ul> <li>What it measures: Perceptual similarity</li> <li>Range: ~0.0 \u2013 1.0 (lower is better)</li> <li>Typical good range: 0.1 \u2013 0.3</li> </ul>"},{"location":"guides/metrics/#dists","title":"DISTS","text":"<ul> <li>What it measures: Structure + texture similarity</li> <li>Range: ~0.0 \u2013 1.0 (lower is better)</li> <li>Typical good range: 0.1 \u2013 0.25</li> </ul>"},{"location":"guides/metrics/#noreference-iqa-besteffort","title":"No\u2011reference IQA (best\u2011effort)","text":"<ul> <li>NIQE / BRISQUE or other proxies, if the optional packages are present</li> <li>Recorded when available; not required for any workflow</li> </ul>"},{"location":"guides/metrics/#metricsjson-structure","title":"metrics.json structure","text":"<p>Restoria writes a lightweight metrics file:</p> <pre><code>{\n  \"metrics\": [\n    {\n      \"input\": \"photos/a.jpg\",\n      \"restored_img\": \"out/a.png\",\n      \"metrics\": {\n        \"arcface_cosine\": 0.91,\n        \"lpips_alex\": 0.18,\n        \"dists\": 0.14,\n        \"niqe\": 3.2,\n        \"runtime_sec\": 1.87,\n        \"vram_mb\": 2950\n      }\n    }\n  ],\n  \"plan\": {\n    \"backend\": \"gfpgan\",\n    \"reason\": \"default_backend\",\n    \"params\": {\"version\": \"1.4\"}\n  }\n}\n</code></pre> <p>Notes:</p> <ul> <li>The top-level <code>plan</code> block is present when using Restoria. gfpup writes   only <code>{\"metrics\": [...]}</code>.</li> <li>Missing metrics appear as absent or <code>null</code> values; we never hard-fail a   run if optional metrics can\u2019t be computed.</li> </ul>"},{"location":"guides/metrics/#comparing-backends","title":"Comparing backends","text":"<p>Run multiple backends and compare their metrics:</p> <pre><code>restoria run --input test/ --output out_gfpgan/ --backend gfpgan --metrics full\nrestoria run --input test/ --output out_codeformer/ \\\n  --backend codeformer --metrics full\nrestoria run --input test/ --output out_rfpp/ \\\n  --backend restoreformerpp --metrics full\n</code></pre> <p>Then aggregate the numbers in your own notebook/script. Example skeleton:</p> <pre><code>import json\nfrom pathlib import Path\n\ndef avg(vals):\n    vs = [v for v in vals if isinstance(v, (int, float))]\n    return sum(vs) / len(vs) if vs else None\n\ndef summarize(metrics_path):\n    data = json.loads(Path(metrics_path).read_text())\n    arc = []\n    lp = []\n    ds = []\n    t = []\n    for rec in data.get(\"metrics\", []):\n        m = rec.get(\"metrics\", {})\n        arc.append(m.get(\"arcface_cosine\"))\n        lp.append(m.get(\"lpips_alex\"))\n        ds.append(m.get(\"dists\"))\n        t.append(m.get(\"runtime_sec\"))\n    return {\n        \"avg_arcface\": avg(arc),\n        \"avg_lpips\": avg(lp),\n        \"avg_dists\": avg(ds),\n        \"avg_time\": avg(t),\n    }\n\nprint(\"GFPGAN:\", summarize(\"out_gfpgan/metrics.json\"))\nprint(\"CodeFormer:\", summarize(\"out_codeformer/metrics.json\"))\nprint(\"RFPP:\", summarize(\"out_rfpp/metrics.json\"))\n</code></pre>"},{"location":"guides/metrics/#quality-thresholds-examples","title":"Quality thresholds (examples)","text":"<ul> <li>Professional: LPIPS &lt; 0.2, DISTS &lt; 0.15, ArcFace &gt; 0.85</li> <li>Social/Personal: LPIPS &lt; 0.3, DISTS &lt; 0.25, ArcFace &gt; 0.75</li> <li>Archive/Historical: LPIPS &lt; 0.4, DISTS &lt; 0.35, ArcFace &gt; 0.65</li> </ul> <p>Interpret thresholds as guidelines; always inspect visuals.</p>"},{"location":"guides/metrics/#troubleshooting","title":"Troubleshooting","text":"<p>Metrics computation failed</p> <ul> <li>Ensure extras are installed: <code>pip install -e .[metrics]</code></li> <li>Some metrics require a GPU; otherwise they may be slow or skipped</li> <li>Try <code>--metrics fast</code> to compute identity only</li> </ul> <p>Performance</p> <ul> <li>Metrics add overhead: use <code>--metrics off</code> for production speed</li> <li>Consider computing metrics on a subset for QA</li> </ul> <p>Next:</p> <ul> <li>Choose the right backend \u2192 (choose-backend.md)</li> <li>Optimize hardware performance \u2192 (../HARDWARE_GUIDE.md)</li> <li>Batch processing \u2192 (batch-processing.md)</li> </ul>"},{"location":"guides/migration/","title":"Migration to Restoria","text":"<p>This project is being rebranded as Restoria \u2014 intelligent image revival. The new CLI and package are designed to be modular and local\u2011first while staying familiar.</p>"},{"location":"guides/migration/#what-changes-now","title":"What changes now","text":"<ul> <li>New CLI: <code>restoria</code></li> <li>New package import: <code>import restoria</code></li> <li>Legacy helper: <code>gfpup</code> remains available for a transition period.</li> </ul>"},{"location":"guides/migration/#11-command-mapping","title":"1:1 command mapping","text":"<ul> <li><code>gfpup run --input X --output Y</code> \u2192 <code>restoria run --input X --output Y</code></li> <li><code>gfpup doctor</code> \u2192 <code>restoria doctor</code></li> </ul> <p>Backends and flags remain the same for common workflows. Advanced options will continue to evolve behind optional extras and will degrade gracefully if not installed.</p>"},{"location":"guides/migration/#installing","title":"Installing","text":"<ul> <li>Base (editable): <code>pip install -e .</code></li> <li>With metrics: <code>pip install -e \".[metrics]\"</code></li> <li>With ONNX Runtime: <code>pip install -e \".[ort]\"</code></li> </ul>"},{"location":"guides/migration/#notes-on-compatibility","title":"Notes on compatibility","text":"<ul> <li>The primary distribution metadata still uses the existing project name   temporarily. The <code>restoria</code> console script and package are provided now, and   the distribution rename will follow a SemVer\u2011tracked release.</li> <li>No hard redirects are required. Shims will be removed in the next major once   the migration window closes.</li> </ul>"},{"location":"guides/repo-rename/","title":"Repository rename &amp; migration checklist","text":"<p>This page lists safe, incremental steps to rename the repository and PyPI package while keeping users productive.</p> <p>Goal: migrate branding and distribution names with zero breaking changes for existing workflows until users opt in.</p>"},{"location":"guides/repo-rename/#principles","title":"Principles","text":"<ul> <li>Backward compatibility by default: existing CLIs and flags continue to work.</li> <li>Shims only: legacy entry points remain but display a one-time deprecation notice.</li> <li>Deterministic behavior: same inputs produce same plan/output under the same version.</li> <li>Clear docs and changelog: announce deprecation windows and timelines.</li> </ul>"},{"location":"guides/repo-rename/#step-by-step","title":"Step-by-step","text":"<ol> <li> <p>Repository rename</p> </li> <li> <p>Rename GitHub repo to the new name in Settings.</p> </li> <li>Verify redirects are auto-created (GitHub handles this). Keep old     remote URLs in docs for one minor release.</li> <li> <p>Update badges and links in README and docs.</p> </li> <li> <p>Package metadata</p> </li> <li> <p>Update <code>pyproject.toml</code> project name and URLs.</p> </li> <li>Keep console scripts for both new and legacy entry points during the     transition.</li> <li> <p>Bump minor version and add a changelog entry noting the rename and     deprecation window.</p> </li> <li> <p>CI and docs</p> </li> <li> <p>Update release workflows to publish to the new package name (if     changing on PyPI).</p> </li> <li>Keep a compatibility wheel that provides legacy entry points if     feasible.</li> <li> <p>Use mike to version the docs. Add a banner on latest docs highlighting     the new name.</p> </li> <li> <p>Deprecation policy</p> </li> <li> <p>Show a one-time deprecation notice in legacy CLI recommending the new     CLI name and commands.</p> </li> <li>Provide a migration guide (see Migration to Restoria) with     side-by-side examples.</li> <li> <p>After two minor versions, consider removing the legacy shim, following     SemVer (major) if behavior changes.</p> </li> <li> <p>Weight and model registries</p> </li> <li> <p>Keep weight resolution modules stable; do not rename on disk abruptly.</p> </li> <li>Provide redirects or lookup fallbacks for old registry names to new     names.</li> </ol>"},{"location":"guides/repo-rename/#validation","title":"Validation","text":"<ul> <li>Run light tests (<code>tests_light</code>) locally and in CI. Avoid heavy model downloads.</li> <li>Smoke-test CLI paths: plan-only, dry-run, and normal run for a small sample image.</li> <li>Confirm manifests and metrics are still written with stable key names.</li> </ul>"},{"location":"guides/repo-rename/#rollback","title":"Rollback","text":"<ul> <li>If users report blockers, you can temporarily restore older wheel     versions and mark the latest as yanked on PyPI while issuing a patch     release.</li> </ul>"},{"location":"guides/repo-rename/#see-also","title":"See also","text":"<ul> <li>Migration to Restoria (User Guides)</li> <li>Governance \u2192 Versioning policy</li> </ul>"},{"location":"guides/restore-a-photo/","title":"Restore a photo","text":"<p>Learn how to restore a single image using GFPGAN's CLI and web interface.</p>"},{"location":"guides/restore-a-photo/#quick-start","title":"Quick start","text":"<p>For a damaged or blurry photo, restoration takes just one command:</p> <pre><code>gfpgan-infer --input damaged_photo.jpg --version 1.4\n</code></pre> <p>Results are saved to <code>results/</code> with before/after comparison images.</p>"},{"location":"guides/restore-a-photo/#cli-workflow","title":"CLI workflow","text":""},{"location":"guides/restore-a-photo/#basic-restoration","title":"Basic restoration","text":"Single imageCustom output locationUpscale while restoring <pre><code>gfpgan-infer --input photo.jpg --version 1.4\n</code></pre> <pre><code>gfpgan-infer --input photo.jpg --output restored/ --version 1.4\n</code></pre> <pre><code>gfpgan-infer --input photo.jpg --version 1.4 --upscale 2\n</code></pre>"},{"location":"guides/restore-a-photo/#choose-your-backend","title":"Choose your backend","text":"<p>Different backends offer different trade-offs:</p> GFPGAN v1.4 (recommended)GFPGAN v1.3 (natural)CodeFormer (fast) <p><pre><code>gfpgan-infer --input photo.jpg --version 1.4\n</code></pre> - Best for: General photos and portraits - Quality: High detail preservation - Speed: Medium</p> <p><pre><code>gfpgan-infer --input photo.jpg --version 1.3\n</code></pre> - Best for: Natural-looking results - Quality: Good, less sharp than v1.4 - Speed: Medium</p> <p><pre><code>gfpgan-infer --input photo.jpg --backend codeformer\n</code></pre> - Best for: Batch processing - Quality: Good - Speed: Fast</p>"},{"location":"guides/restore-a-photo/#advanced-options","title":"Advanced options","text":"Background enhancementFace detection optionsDevice selection <pre><code># Enhance background with Real-ESRGAN\ngfpgan-infer --input photo.jpg --bg_upsampler realesrgan\n\n# Disable background enhancement for speed\ngfpgan-infer --input photo.jpg --bg_upsampler none\n</code></pre> <pre><code># Only restore the center face\ngfpgan-infer --input photo.jpg --only_center_face\n\n# Input is already an aligned face crop\ngfpgan-infer --input face_crop.jpg --aligned\n</code></pre> <pre><code># Force CPU (if GPU has issues)\ngfpgan-infer --input photo.jpg --device cpu\n\n# Auto-detect best device\ngfpgan-infer --input photo.jpg --device auto\n</code></pre>"},{"location":"guides/restore-a-photo/#web-interface","title":"Web interface","text":"<p>Launch the interactive web UI for drag-and-drop restoration:</p> <pre><code>python -m gfpgan.gradio_app\n</code></pre> <p>Then open http://localhost:7860 in your browser.</p>"},{"location":"guides/restore-a-photo/#web-interface-features","title":"Web interface features","text":"<ul> <li>Drag and drop: Upload images directly</li> <li>Real-time preview: See results before saving</li> <li>Parameter adjustment: Change models and settings interactively</li> <li>Download options: Get individual results or batch ZIP</li> </ul>"},{"location":"guides/restore-a-photo/#custom-web-server","title":"Custom web server","text":"<p>For more control, use the FastAPI server:</p> <pre><code>uvicorn services.api.main:app --reload --host 0.0.0.0 --port 8000\n</code></pre> <p>API documentation available at: - Interactive docs: http://localhost:8000/docs - ReDoc format: http://localhost:8000/redoc</p>"},{"location":"guides/restore-a-photo/#understanding-results","title":"Understanding results","text":""},{"location":"guides/restore-a-photo/#output-structure","title":"Output structure","text":"<p>After running restoration, you'll find:</p> <pre><code>results/\n\u251c\u2500\u2500 restored_faces/           # Individual face crops (restored)\n\u251c\u2500\u2500 cropped_faces/           # Original face crops (input)\n\u251c\u2500\u2500 cmp_photo.jpg           # Before/after comparison\n\u2514\u2500\u2500 photo.jpg               # Full restored image\n</code></pre>"},{"location":"guides/restore-a-photo/#quality-assessment","title":"Quality assessment","text":"<p>Visual indicators to check: - Sharpness: Are facial features well-defined? - Identity preservation: Does the person still look like themselves? - Artifacts: Any unnatural textures or distortions? - Background: Is the non-face area properly enhanced?</p>"},{"location":"guides/restore-a-photo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/restore-a-photo/#common-issues","title":"Common issues","text":"<p>No faces detected</p> <p><pre><code>Warning: No faces detected in the input image.\n</code></pre> Solutions: - Ensure the image contains visible faces - Try a different face detection threshold - Check if the image is too small or low quality</p> <p>CUDA out of memory</p> <p><pre><code>RuntimeError: CUDA out of memory\n</code></pre> Solutions: <pre><code># Use CPU instead\ngfpgan-infer --input photo.jpg --device cpu\n\n# Reduce image size first\ngfpgan-infer --input photo.jpg --upscale 1\n</code></pre></p> <p>Poor restoration quality</p> <p>Try different backends: <pre><code># More natural results\ngfpgan-infer --input photo.jpg --version 1.3\n\n# Better identity preservation\ngfpgan-infer --input photo.jpg --backend restoreformer\n</code></pre></p>"},{"location":"guides/restore-a-photo/#getting-help","title":"Getting help","text":"<p>If restoration quality isn't satisfactory:</p> <ol> <li>Check our backend comparison guide</li> <li>Review quality metrics for objective evaluation</li> <li>See troubleshooting for technical issues</li> </ol> <p>Next steps: - Process multiple photos \u2192 - Measure restoration quality \u2192 - Choose the right backend \u2192</p>"},{"location":"product/brand/","title":"Brand decision \u2014 Phase 0","text":""},{"location":"product/brand/#candidate-names-shortlist","title":"Candidate names (shortlist)","text":"<ul> <li>Restoria \u2014 intelligent image revival</li> <li>Revalia \u2014 restorative vision</li> <li>Vervid \u2014 vivid restoration engine</li> <li>Auvion \u2014 authentic vision restore</li> <li>Lumenia \u2014 bring images to light</li> <li>Ristoro \u2014 restore, refined</li> <li>Revique \u2014 restoration boutique</li> <li>Finestra \u2014 clarity restored</li> </ul>"},{"location":"product/brand/#chosen-name-tagline","title":"Chosen name &amp; tagline","text":"<ul> <li>Name: Restoria</li> <li>Tagline: intelligent image revival</li> </ul>"},{"location":"product/brand/#scope-statement","title":"Scope statement","text":"<p>Restoria is a future\u2011proof, modular, and evaluation\u2011driven image restoration toolkit for faces and general photos. It\u2019s privacy\u2011first and local\u2011first by default, with optional accelerators (torch.compile, ONNX Runtime) when available. The architecture is multi\u2011backend and extensible \u2014 plug in restorers, background upscalers, probes, and metrics without churn \u2014 and results are measured with identity and perceptual metrics to keep improvements grounded. Sensible defaults degrade gracefully if extras aren\u2019t installed, and the CLI is task\u2011first for single images and batches.</p>"},{"location":"product/data-card/","title":"Data Card: GFPGAN Training Datasets","text":""},{"location":"product/data-card/#dataset-overview","title":"Dataset Overview","text":"<p>This data card documents the datasets used for training and evaluating GFPGAN models, following best practices for dataset transparency and responsible AI development.</p> <p>\u26a0\ufe0f Important: This data card is under development. Many sections require additional research and documentation from the original model training process.</p>"},{"location":"product/data-card/#dataset-summary","title":"Dataset Summary","text":""},{"location":"product/data-card/#primary-training-data","title":"Primary Training Data","text":"<p>\u26a0\ufe0f TODO: Comprehensive training data documentation needed</p> Component Description Status Source datasets [NEEDS DOCUMENTATION] TODO Total images [NEEDS DOCUMENTATION] TODO Face count [NEEDS DOCUMENTATION] TODO Resolution range [NEEDS DOCUMENTATION] TODO Collection period [NEEDS DOCUMENTATION] TODO"},{"location":"product/data-card/#evaluation-datasets","title":"Evaluation Datasets","text":"Dataset Size Purpose License Availability CelebA-HQ 30k imgs HQ eval Custom Research-only FFHQ 70k imgs Face-gen bench BY-NC-SA 4.0 Public Helen 2,330 imgs Landmark eval Academic Research-only LFW 13k imgs ID verification Public domain Public"},{"location":"product/data-card/#data-collection-and-processing","title":"Data Collection and Processing","text":""},{"location":"product/data-card/#collection-methodology","title":"Collection Methodology","text":"<p>\u26a0\ufe0f TODO: Original data collection process needs documentation</p>"},{"location":"product/data-card/#data-sources","title":"Data Sources","text":"<ul> <li>Web scraping: [DETAILS NEEDED]</li> <li>Public datasets: [LIST NEEDED]</li> <li>Synthetic generation: [METHODS NEEDED]</li> <li>User contributions: [POLICIES NEEDED]</li> </ul>"},{"location":"product/data-card/#collection-criteria","title":"Collection Criteria","text":"<ul> <li>Face visibility: [CRITERIA NEEDED]</li> <li>Image quality: [STANDARDS NEEDED]</li> <li>Resolution requirements: [MINIMUMS NEEDED]</li> <li>Demographic diversity: [TARGETS NEEDED]</li> </ul>"},{"location":"product/data-card/#data-processing-pipeline","title":"Data Processing Pipeline","text":""},{"location":"product/data-card/#preprocessing-steps","title":"Preprocessing Steps","text":"<ol> <li>Face detection: MTCNN or RetinaFace detection</li> <li>Alignment: Facial landmark-based alignment</li> <li>Cropping: Center crop to facial region</li> <li>Resizing: Standardize to 512x512 resolution</li> <li>Quality filtering: Remove low-quality samples</li> </ol>"},{"location":"product/data-card/#data-augmentation","title":"Data Augmentation","text":"<ul> <li>Geometric transforms: Rotation, scaling, flipping</li> <li>Color adjustments: Brightness, contrast, saturation</li> <li>Degradation simulation: Blur, noise, compression</li> <li>Occlusion: Partial face masking</li> </ul>"},{"location":"product/data-card/#quality-control","title":"Quality Control","text":"<ul> <li>Manual review: [PROCESS NEEDED]</li> <li>Automated filtering: [CRITERIA NEEDED]</li> <li>Duplicate detection: [METHODS NEEDED]</li> <li>Privacy screening: [PROTOCOLS NEEDED]</li> </ul>"},{"location":"product/data-card/#dataset-composition","title":"Dataset Composition","text":""},{"location":"product/data-card/#demographic-distribution","title":"Demographic Distribution","text":"<p>\u26a0\ufe0f TODO: Comprehensive demographic analysis needed</p>"},{"location":"product/data-card/#age-groups","title":"Age Groups","text":"<ul> <li>Children (0-17): [PERCENTAGE NEEDED]</li> <li>Young adults (18-35): [PERCENTAGE NEEDED]</li> <li>Middle-aged (36-55): [PERCENTAGE NEEDED]</li> <li>Older adults (55+): [PERCENTAGE NEEDED]</li> </ul>"},{"location":"product/data-card/#gender-representation","title":"Gender Representation","text":"<ul> <li>Male: [PERCENTAGE NEEDED]</li> <li>Female: [PERCENTAGE NEEDED]</li> <li>Non-binary/Other: [PERCENTAGE NEEDED]</li> <li>Not specified: [PERCENTAGE NEEDED]</li> </ul>"},{"location":"product/data-card/#ethnic-and-racial-diversity","title":"Ethnic and Racial Diversity","text":"<ul> <li>White/Caucasian: [PERCENTAGE NEEDED]</li> <li>Black/African American: [PERCENTAGE NEEDED]</li> <li>Asian: [PERCENTAGE NEEDED]</li> <li>Hispanic/Latino: [PERCENTAGE NEEDED]</li> <li>Middle Eastern: [PERCENTAGE NEEDED]</li> <li>Mixed/Other: [PERCENTAGE NEEDED]</li> </ul>"},{"location":"product/data-card/#geographic-distribution","title":"Geographic Distribution","text":"<ul> <li>North America: [PERCENTAGE NEEDED]</li> <li>Europe: [PERCENTAGE NEEDED]</li> <li>Asia: [PERCENTAGE NEEDED]</li> <li>Other regions: [PERCENTAGE NEEDED]</li> </ul>"},{"location":"product/data-card/#technical-characteristics","title":"Technical Characteristics","text":""},{"location":"product/data-card/#image-properties","title":"Image Properties","text":"<ul> <li>Resolution distribution: 128x128 to 1024x1024 (standardized to 512x512)</li> <li>Color space: RGB, sRGB color profile</li> <li>File formats: JPEG, PNG (converted to PNG for training)</li> <li>Compression levels: Various (original quality preserved)</li> </ul>"},{"location":"product/data-card/#face-characteristics","title":"Face Characteristics","text":"<ul> <li>Face size range: 64x64 to 512x512 pixels</li> <li>Pose variation: Primarily frontal (\u00b130 degrees)</li> <li>Expression variety: Neutral to moderate expressions</li> <li>Lighting conditions: Various natural and artificial lighting</li> </ul>"},{"location":"product/data-card/#privacy-ethics","title":"Privacy &amp; Ethics","text":"<p>For information about privacy practices and ethical considerations:</p> <ul> <li>Data handling practices: See our security policy</li> <li>Responsible AI principles: See our contributing guidelines</li> </ul>"},{"location":"product/data-card/#known-limitations-and-biases","title":"Known Limitations and Biases","text":""},{"location":"product/data-card/#identified-biases","title":"Identified Biases","text":"<p>\u26a0\ufe0f TODO: Comprehensive bias analysis needed</p>"},{"location":"product/data-card/#demographic-biases","title":"Demographic Biases","text":"<ul> <li>Age bias: [ANALYSIS NEEDED]</li> <li>Gender bias: [ANALYSIS NEEDED]</li> <li>Racial bias: [ANALYSIS NEEDED]</li> <li>Geographic bias: [ANALYSIS NEEDED]</li> </ul>"},{"location":"product/data-card/#quality-biases","title":"Quality Biases","text":"<ul> <li>High-quality overrepresentation: Professional photos vs. amateur</li> <li>Pose bias: Frontal faces overrepresented</li> <li>Expression bias: Neutral expressions dominant</li> <li>Lighting bias: Well-lit faces overrepresented</li> </ul>"},{"location":"product/data-card/#impact-assessment","title":"Impact Assessment","text":""},{"location":"product/data-card/#model-performance-impact","title":"Model Performance Impact","text":"<ul> <li>Demographic performance gaps: [MEASUREMENT NEEDED]</li> <li>Quality variations: [ASSESSMENT NEEDED]</li> <li>Use case limitations: [DOCUMENTATION NEEDED]</li> <li>Fairness implications: [ANALYSIS NEEDED]</li> </ul>"},{"location":"product/data-card/#mitigation-strategies","title":"Mitigation Strategies","text":"<ul> <li>Diverse evaluation: Multi-demographic test sets</li> <li>Bias monitoring: Regular fairness assessments</li> <li>Data augmentation: Synthetic diversity enhancement</li> <li>Community feedback: Bias reporting mechanisms</li> </ul>"},{"location":"product/data-card/#data-governance","title":"Data Governance","text":""},{"location":"product/data-card/#data-management","title":"Data Management","text":""},{"location":"product/data-card/#storage-and-security","title":"Storage and Security","text":"<ul> <li>Data encryption: At rest and in transit</li> <li>Access controls: Role-based permissions</li> <li>Audit logging: Access and modification tracking</li> <li>Backup procedures: Secure, versioned backups</li> </ul>"},{"location":"product/data-card/#version-control","title":"Version Control","text":"<ul> <li>Dataset versioning: Semantic versioning system</li> <li>Change tracking: Detailed modification logs</li> <li>Reproducibility: Exact dataset recreation capability</li> <li>Documentation: Comprehensive change documentation</li> </ul>"},{"location":"product/data-card/#update-and-maintenance","title":"Update and Maintenance","text":""},{"location":"product/data-card/#regular-updates","title":"Regular Updates","text":"<ul> <li>Bias assessment: Quarterly fairness reviews</li> <li>Quality improvement: Ongoing data curation</li> <li>Coverage expansion: Demographic gap filling</li> <li>Privacy compliance: Regular policy alignment</li> </ul>"},{"location":"product/data-card/#community-involvement","title":"Community Involvement","text":"<ul> <li>Feedback collection: User experience reports</li> <li>Bias reporting: Community bias identification</li> <li>Data contributions: Voluntary diverse data sharing</li> <li>Advisory input: External expert consultation</li> </ul>"},{"location":"product/data-card/#usage-guidelines","title":"Usage Guidelines","text":""},{"location":"product/data-card/#appropriate-uses","title":"Appropriate Uses","text":"<p>\u2705 Recommended applications:</p> <ul> <li>Research: Academic computer vision research</li> <li>Development: Face restoration algorithm improvement</li> <li>Evaluation: Model performance benchmarking</li> <li>Education: Learning about face processing techniques</li> </ul>"},{"location":"product/data-card/#restricted-uses","title":"Restricted Uses","text":"<p>\u274c Inappropriate applications:</p> <ul> <li>Surveillance: Identification or tracking individuals</li> <li>Discrimination: Biased decision-making systems</li> <li>Commercial exploitation: Unauthorized commercial use</li> <li>Privacy violation: Processing without consent</li> </ul>"},{"location":"product/data-card/#best-practices","title":"Best Practices","text":""},{"location":"product/data-card/#responsible-usage","title":"Responsible Usage","text":"<ol> <li>Bias awareness: Understand and account for dataset limitations</li> <li>Evaluation diversity: Test on diverse populations</li> <li>Transparency: Document data usage in publications</li> <li>Privacy respect: Honor original consent and restrictions</li> </ol>"},{"location":"product/data-card/#technical-recommendations","title":"Technical Recommendations","text":"<ol> <li>Subset selection: Use representative subsets for evaluation</li> <li>Augmentation: Apply appropriate data augmentation</li> <li>Validation: Cross-validate on multiple datasets</li> <li>Documentation: Maintain detailed usage records</li> </ol>"},{"location":"product/data-card/#contact-and-reporting","title":"Contact and Reporting","text":""},{"location":"product/data-card/#data-issues","title":"Data Issues","text":"<ul> <li>Dataset errors: data-issues@gfpgan.ai</li> <li>Privacy concerns: privacy@gfpgan.ai</li> <li>Bias reports: bias@gfpgan.ai</li> <li>General questions: GitHub Discussions</li> </ul>"},{"location":"product/data-card/#contributing","title":"Contributing","text":""},{"location":"product/data-card/#data-contributions","title":"Data Contributions","text":"<ul> <li>Diverse datasets: Help improve demographic coverage</li> <li>Quality assessment: Manual data quality evaluation</li> <li>Bias analysis: Demographic bias identification</li> <li>Documentation: Improve data card completeness</li> </ul>"},{"location":"product/data-card/#review-process","title":"Review Process","text":"<ol> <li>Community review: Public feedback on data practices</li> <li>Expert consultation: External bias and ethics review</li> <li>Regular updates: Quarterly data card revisions</li> <li>Transparency reports: Annual data governance summaries</li> </ol>"},{"location":"product/data-card/#references-and-standards","title":"References and Standards","text":""},{"location":"product/data-card/#standards-compliance","title":"Standards Compliance","text":"<ul> <li>ISO/IEC 23053: Framework for AI risk management</li> <li>NIST AI RMF: AI Risk Management Framework</li> <li>IEEE 2857: Privacy engineering for AI</li> <li>Model Cards: Google's model documentation standard</li> </ul>"},{"location":"product/data-card/#related-documentation","title":"Related Documentation","text":"<ul> <li>Model Card: GFPGAN Model Card</li> <li>Security Policy: Security guidelines</li> <li>Privacy Policy: Data privacy practices</li> <li>Ethical Guidelines: AI ethics framework</li> </ul> <p>Last Updated: September 2024 Next Review: December 2024 Status: Under development - comprehensive data analysis in progress</p> <p>\u26a0\ufe0f Important Notice: This data card is incomplete and under active development. Many critical details about the training data require investigation and documentation. We are committed to improving transparency and welcome community input to make this documentation more complete and accurate.</p>"},{"location":"product/model-card/","title":"Model Card: GFPGAN","text":""},{"location":"product/model-card/#model-overview","title":"Model Overview","text":"<p>Model Name: GFPGAN (Generative Facial Prior GAN) Version: 1.4 Model Type: Generative Adversarial Network for face restoration License: Apache 2.0</p>"},{"location":"product/model-card/#intended-use","title":"Intended Use","text":""},{"location":"product/model-card/#primary-use-cases","title":"Primary Use Cases","text":"<ul> <li>Photo restoration: Enhance damaged, blurred, or low-quality face photos</li> <li>Image enhancement: Improve facial details in compressed or degraded images</li> <li>Historical photo recovery: Restore old or deteriorated photographs</li> <li>Content creation: Enhance facial quality in digital media</li> </ul>"},{"location":"product/model-card/#intended-users","title":"Intended Users","text":"<ul> <li>Photographers: Professional and amateur photo enhancement</li> <li>Archivists: Digital preservation of historical photographs</li> <li>Content creators: Video and image post-production</li> <li>Researchers: Computer vision and image processing studies</li> </ul>"},{"location":"product/model-card/#out-of-scope-uses","title":"Out-of-Scope Uses","text":"<p>\u274c Not intended for:</p> <ul> <li>Real-time video processing (performance limitations)</li> <li>Non-facial image enhancement (specialized for faces)</li> <li>Identity modification or deepfake creation</li> <li>Medical diagnosis or analysis</li> <li>Surveillance or law enforcement identification</li> </ul>"},{"location":"product/model-card/#model-details","title":"Model Details","text":""},{"location":"product/model-card/#architecture","title":"Architecture","text":"<ul> <li>Base Model: StyleGAN2 generator with facial priors</li> <li>Training Framework: PyTorch with custom loss functions</li> <li>Input Resolution: 512x512 pixels (faces automatically detected and cropped)</li> <li>Output Resolution: 512x512 to 2048x2048 (depending on upscale factor)</li> </ul>"},{"location":"product/model-card/#model-versions","title":"Model Versions","text":"Version Release Date Key Features Model Size v1.4 2024-Q4 Best identity preservation ~348MB v1.3 2024-Q2 Improved texture quality ~348MB v1.2 2024-Q1 Enhanced stability ~348MB"},{"location":"product/model-card/#training-data","title":"Training Data","text":"<p>\u26a0\ufe0f TODO: Detailed training data information needed</p> <ul> <li>Dataset composition: [NEEDS DOCUMENTATION]</li> <li>Data sources: [NEEDS DOCUMENTATION]</li> <li>Number of images: [NEEDS DOCUMENTATION]</li> <li>Demographics: [NEEDS ANALYSIS]</li> <li>Geographic coverage: [NEEDS ANALYSIS]</li> </ul>"},{"location":"product/model-card/#performance-and-limitations","title":"Performance and Limitations","text":""},{"location":"product/model-card/#model-performance","title":"Model Performance","text":""},{"location":"product/model-card/#quantitative-metrics","title":"Quantitative Metrics","text":"<p>\u26a0\ufe0f TODO: Comprehensive evaluation needed</p> Metric GFPGAN v1.4 GFPGAN v1.3 Baseline LPIPS \u2193 [TODO] [TODO] [TODO] DISTS \u2193 [TODO] [TODO] [TODO] ArcFace Similarity \u2191 [TODO] [TODO] [TODO] Processing Time (GPU) ~2-3s ~1-2s -"},{"location":"product/model-card/#qualitative-assessment","title":"Qualitative Assessment","text":"<p>\u2705 Strengths:</p> <ul> <li>Excellent identity preservation</li> <li>Natural-looking texture generation</li> <li>Robust to various degradation types</li> <li>Maintains facial structure and features</li> </ul> <p>\u26a0\ufe0f Limitations:</p> <ul> <li>Performance degrades with extreme degradation</li> <li>May struggle with very small faces (&lt;64px)</li> <li>Requires GPU for reasonable performance</li> <li>Limited to frontal and near-frontal faces</li> </ul>"},{"location":"product/model-card/#known-biases-and-fairness","title":"Known Biases and Fairness","text":"<p>\u26a0\ufe0f TODO: Comprehensive bias analysis needed</p>"},{"location":"product/model-card/#demographic-performance","title":"Demographic Performance","text":"<ul> <li>Age groups: [NEEDS ANALYSIS]</li> <li>Gender representation: [NEEDS ANALYSIS]</li> <li>Ethnic diversity: [NEEDS ANALYSIS]</li> <li>Skin tone coverage: [NEEDS ANALYSIS]</li> </ul>"},{"location":"product/model-card/#mitigation-strategies","title":"Mitigation Strategies","text":"<ul> <li>Regular bias auditing planned</li> <li>Diverse evaluation datasets in development</li> <li>Community feedback collection for bias reporting</li> </ul>"},{"location":"product/model-card/#technical-limitations","title":"Technical Limitations","text":""},{"location":"product/model-card/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>Minimum GPU: 4GB VRAM for basic operation</li> <li>Recommended GPU: 8GB+ VRAM for optimal performance</li> <li>CPU fallback: Available but significantly slower (10-50x)</li> </ul>"},{"location":"product/model-card/#input-constraints","title":"Input Constraints","text":"<ul> <li>Face size: Minimum 32x32 pixels for detection</li> <li>Image formats: JPG, PNG, WebP, BMP</li> <li>Maximum resolution: 4K (auto-resized if larger)</li> <li>Face orientation: Works best with frontal faces</li> </ul>"},{"location":"product/model-card/#ethical-considerations","title":"Ethical Considerations","text":""},{"location":"product/model-card/#responsible-ai-principles","title":"Responsible AI Principles","text":""},{"location":"product/model-card/#transparency","title":"Transparency","text":"<ul> <li>Open-source implementation and weights</li> <li>Documented limitations and failure cases</li> <li>Clear usage guidelines and best practices</li> </ul>"},{"location":"product/model-card/#fairness","title":"Fairness","text":"<ul> <li>\u26a0\ufe0f TODO: Bias evaluation across demographic groups</li> <li>Commitment to addressing identified biases</li> <li>Inclusive evaluation methodologies</li> </ul>"},{"location":"product/model-card/#privacy","title":"Privacy","text":"<ul> <li>Local processing (no cloud upload required)</li> <li>No data retention or telemetry by default</li> <li>User control over all processed images</li> </ul>"},{"location":"product/model-card/#accountability","title":"Accountability","text":"<ul> <li>Clear documentation of intended uses</li> <li>Guidance on inappropriate applications</li> <li>Community reporting mechanisms</li> </ul>"},{"location":"product/model-card/#potential-risks","title":"Potential Risks","text":""},{"location":"product/model-card/#misuse-scenarios","title":"Misuse Scenarios","text":"<p>\u26a0\ufe0f High Risk:</p> <ul> <li>Identity manipulation: Creating misleading enhanced photos</li> <li>Deepfake preparation: Using enhanced faces for synthetic media</li> <li>Non-consensual enhancement: Processing photos without permission</li> </ul> <p>\u26a0\ufe0f Medium Risk:</p> <ul> <li>Historical revisionism: Inappropriately \"correcting\" historical photos</li> <li>Surveillance enhancement: Improving low-quality surveillance footage</li> <li>Bias amplification: Reinforcing beauty standards or demographic preferences</li> </ul>"},{"location":"product/model-card/#risk-mitigation","title":"Risk Mitigation","text":"<ul> <li>Clear documentation of appropriate uses</li> <li>Community guidelines and reporting</li> <li>Technical limitations to prevent real-time abuse</li> <li>Education about ethical implications</li> </ul>"},{"location":"product/model-card/#evaluation-and-validation","title":"Evaluation and Validation","text":""},{"location":"product/model-card/#test-datasets","title":"Test Datasets","text":"<p>\u26a0\ufe0f TODO: Standardized evaluation datasets needed</p> <ul> <li>CelebA-HQ: [RESULTS NEEDED]</li> <li>FFHQ: [RESULTS NEEDED]</li> <li>Helen: [RESULTS NEEDED]</li> <li>Custom benchmark: [UNDER DEVELOPMENT]</li> </ul>"},{"location":"product/model-card/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"product/model-card/#technical-quality","title":"Technical Quality","text":"<ul> <li>LPIPS: Perceptual similarity measurement</li> <li>DISTS: Image structural similarity</li> <li>SSIM: Structural similarity index</li> <li>PSNR: Peak signal-to-noise ratio</li> </ul>"},{"location":"product/model-card/#identity-preservation","title":"Identity Preservation","text":"<ul> <li>ArcFace: Identity similarity scores</li> <li>FaceNet: Feature distance measurement</li> <li>Human evaluation: Perceptual identity studies</li> </ul>"},{"location":"product/model-card/#fairness-evaluation","title":"Fairness Evaluation","text":"<ul> <li>\u26a0\ufe0f TODO: Demographic parity analysis</li> <li>\u26a0\ufe0f TODO: Equal opportunity assessment</li> <li>\u26a0\ufe0f TODO: Individual fairness evaluation</li> </ul>"},{"location":"product/model-card/#human-evaluation","title":"Human Evaluation","text":"<p>\u26a0\ufe0f TODO: User study needed</p> <ul> <li>Quality assessment: Professional photographer evaluation</li> <li>Identity preservation: Human rater studies</li> <li>Bias detection: Diverse evaluator panels</li> <li>Use case validation: Target user feedback</li> </ul>"},{"location":"product/model-card/#environmental-impact","title":"Environmental Impact","text":""},{"location":"product/model-card/#carbon-footprint","title":"Carbon Footprint","text":"<p>\u26a0\ufe0f TODO: Environmental impact assessment needed</p> <ul> <li>Training emissions: [NEEDS CALCULATION]</li> <li>Inference efficiency: ~2-3 seconds per image on modern GPUs</li> <li>Hardware efficiency: Optimized for consumer GPUs</li> </ul>"},{"location":"product/model-card/#sustainability-measures","title":"Sustainability Measures","text":"<ul> <li>Model optimization for efficiency</li> <li>Support for various hardware configurations</li> <li>Local processing to reduce data transfer</li> <li>Open-source to prevent duplicate training</li> </ul>"},{"location":"product/model-card/#model-governance","title":"Model Governance","text":""},{"location":"product/model-card/#version-control","title":"Version Control","text":"<ul> <li>Semantic versioning: Clear versioning for compatibility</li> <li>Model registry: Centralized model distribution</li> <li>Reproducibility: Locked dependency versions</li> <li>Rollback capability: Previous versions maintained</li> </ul>"},{"location":"product/model-card/#monitoring-and-updates","title":"Monitoring and Updates","text":"<ul> <li>Performance monitoring: Automated quality checks</li> <li>Bias monitoring: Regular fairness evaluations</li> <li>Security updates: Vulnerability patching</li> <li>Community feedback: Issue tracking and resolution</li> </ul>"},{"location":"product/model-card/#access-and-distribution","title":"Access and Distribution","text":"<ul> <li>Open access: Free download for research and development</li> <li>Commercial use: Permitted under Apache 2.0 license</li> <li>Distribution channels: GitHub releases and model hubs</li> <li>Version compatibility: Backward compatibility guarantees</li> </ul>"},{"location":"product/model-card/#contact-and-feedback","title":"Contact and Feedback","text":""},{"location":"product/model-card/#reporting-issues","title":"Reporting Issues","text":"<ul> <li>Technical issues: GitHub Issues</li> <li>Ethical concerns: mailto:ethics@gfpgan.ai</li> <li>Security vulnerabilities: Security Policy</li> <li>Bias reports: mailto:bias@gfpgan.ai</li> </ul>"},{"location":"product/model-card/#contributing","title":"Contributing","text":"<ul> <li>Model improvements: Community contributions welcome</li> <li>Evaluation data: Diverse evaluation datasets needed</li> <li>Bias testing: Fairness evaluation contributions</li> <li>Documentation: Help improve this model card</li> </ul>"},{"location":"product/model-card/#references-and-citations","title":"References and Citations","text":""},{"location":"product/model-card/#academic-citations","title":"Academic Citations","text":"<pre><code>@InProceedings{wang2021gfpgan,\n    author = {Xintao Wang and Yu Li and Honglun Zhang and Ying Shan},\n    title = {Towards Real-World Blind Face Restoration with Generative Facial Prior},\n    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n    year = {2021}\n}\n</code></pre>"},{"location":"product/model-card/#related-work","title":"Related Work","text":"<ul> <li>StyleGAN2: Foundation architecture for face generation</li> <li>Real-ESRGAN: Background enhancement technology</li> <li>ArcFace: Identity preservation evaluation</li> <li>Face detection: MTCNN and RetinaFace integration</li> </ul> <p>Last Updated: September 2024 Next Review: December 2024 Status: Active development - some evaluations pending</p> <p>\u26a0\ufe0f Note: This model card is under active development. Sections marked with \"TODO\" require additional research and evaluation. We welcome community contributions to improve the completeness and accuracy of this documentation.</p>"},{"location":"product/performance/","title":"Performance Guide","text":"<p>Tips to get the best performance on your hardware.</p> <ul> <li>Torch 2 compile path: <code>--compile</code> (falls back safely)</li> <li>ONNX Runtime providers: <code>--ort-providers</code> (if using ORT backend)</li> <li>Batch processing strategies</li> <li>CPU vs CUDA vs MPS notes</li> </ul> <p>TODO: Add benchmarks and provider matrices.</p>"},{"location":"reference/determinism/","title":"Determinism &amp; Seeding","text":"<p>This page explains how to make runs reproducible across Restoria and the legacy GFPP CLI, what \"deterministic\" means per component, and the practical limits when using CUDA.</p>"},{"location":"reference/determinism/#quick-basics","title":"Quick basics","text":"<ul> <li>Seed controls random, NumPy, and Torch RNGs.</li> <li>Deterministic CUDA constrains CuDNN to deterministic algorithms and disables   benchmarking. This can slow down runs and reduce kernel choices.</li> <li>Planning and heuristics are designed to be deterministic for the same inputs   and options.</li> </ul>"},{"location":"reference/determinism/#restoria-cli","title":"Restoria CLI","text":"<p>Restoria's <code>restoria run</code> provides explicit seeding and deterministic options:</p> <ul> <li><code>--seed &lt;int&gt;</code>: seeds Python <code>random</code>, NumPy, and Torch (if available).</li> <li><code>--deterministic</code>: enables CuDNN deterministic mode and disables benchmarking   when Torch is present and CUDA is used.</li> </ul> <p>Example:</p> <pre><code>restoria run \\\n  --input samples/portrait.jpg \\\n  --output out/ \\\n  --backend gfpgan \\\n  --device cuda \\\n  --seed 123 \\\n  --deterministic\n</code></pre> <p>Planning is deterministic: given the same image and flags (backend, experimental, compile, and ORT providers), the same plan is produced. The seed and deterministic choices are recorded in the run manifest.</p> <p>Tips for reproducibility with Restoria:</p> <ul> <li>Prefer CPU for stricter determinism when comparing outputs across machines.</li> <li>Keep <code>--metrics</code> mode the same between runs; metrics affect what is computed   but not the restoration itself.</li> </ul>"},{"location":"reference/determinism/#gfpp-cli-gfpup","title":"GFPP CLI (gfpup)","text":"<p>GFPP exposes seeding and deterministic controls directly.</p> <ul> <li><code>--seed &lt;int&gt;</code>: sets seeds for Python <code>random</code>, NumPy, and Torch.</li> <li><code>--deterministic</code>: enables deterministic CuDNN and disables benchmarking when   using CUDA.</li> </ul> <p>Example:</p> <pre><code>gfpup run \\\n  --input samples/portrait.jpg \\\n  --output out/ \\\n  --backend gfpgan \\\n  --device cuda \\\n  --seed 123 \\\n  --deterministic\n</code></pre> <p>Edge cases and notes:</p> <ul> <li>CPU runs are typically more reproducible across hardware and drivers.</li> <li>CUDA determinism may still vary across different GPU architectures, drivers,   and Torch versions.</li> <li>Some backends may rely on operations that are not strictly deterministic   under all conditions; in those cases, GFPP CLIs aim to constrain the most   common sources of nondeterminism.</li> </ul>"},{"location":"reference/determinism/#legacy-gfpgan-cli","title":"Legacy GFPGAN CLI","text":"<p>For completeness, the legacy <code>gfpgan-infer</code> supports:</p> <ul> <li><code>--seed &lt;int&gt;</code></li> <li><code>--deterministic-cuda</code> (enables deterministic CuDNN; slower)</li> </ul> <p>Example:</p> <pre><code>gfpgan-infer -i samples/portrait.jpg -o results/ \\\n  --device cuda --seed 123 --deterministic-cuda\n</code></pre>"},{"location":"reference/determinism/#planner-determinism","title":"Planner determinism","text":"<p>The orchestrator (planner) computes a stable Plan for a given image and options. Heuristics use fixed thresholds and normalized scores to avoid minor numeric jitter, e.g.:</p> <ul> <li>Quality scores rounded/standardized to canonical values (like defaulting   weight to 0.6 for moderate degradation)</li> <li>Face detection and routing rules that do not depend on global randomness</li> </ul> <p>This means repeated runs with the same inputs and flags will yield the same <code>plan</code> object in the outputs (<code>metrics.json</code> includes a <code>plan</code> block in Restoria, and manifests record args and runtime environment).</p>"},{"location":"reference/determinism/#recommended-workflow","title":"Recommended workflow","text":"<ul> <li>For rigorous A/B comparisons: use <code>gfpup run</code> with <code>--seed</code> and   <code>--deterministic</code>, keep versions and hardware fixed, and compare outputs.</li> <li>For everyday use where exact bitwise determinism is not required: use   <code>restoria run</code> and rely on deterministic planning and stable defaults.</li> </ul>"},{"location":"reference/manifests-and-metrics/","title":"Manifests &amp; Metrics","text":"<p>This page documents the JSON files written by Restoria and GFPP: <code>manifest.json</code> (run summary) and <code>metrics.json</code> (per-image records and optional plan block).</p> <ul> <li><code>manifest.json</code> captures args, device, results summary, an optional   <code>metrics_file</code> pointer, and environment info.</li> <li><code>metrics.json</code> contains per-image records under <code>metrics</code>, plus (in Restoria)   a <code>plan</code> block explaining backend selection and parameters.</li> </ul>"},{"location":"reference/manifests-and-metrics/#restoria-files","title":"Restoria files","text":"<p>Written by <code>restoria run</code>.</p>"},{"location":"reference/manifests-and-metrics/#restoria-manifestjson","title":"Restoria manifest.json","text":"<p>Shape (fields may be absent if unavailable):</p> <pre><code>{\n  \"args\": {\"backend\": \"gfpgan\", \"metrics\": \"fast\", \"device\": \"auto\"},\n  \"device\": \"cpu\",\n  \"results\": [\n    {\n      \"input\": \"samples/portrait.jpg\",\n      \"backend\": \"gfpgan\",\n      \"restored_img\": \"out/portrait.png\",\n      \"metrics\": {\"arcface_cosine\": 0.87}\n    }\n  ],\n  \"metrics_file\": \"metrics.json\",\n  \"env\": {\n    \"runtime\": {\n      \"compile\": false,\n      \"ort_providers\": []\n    }\n  }\n}\n</code></pre> <p>Notes:</p> <ul> <li><code>device</code> may be <code>auto</code>, <code>cpu</code>, <code>cuda</code>, or <code>null</code> if detection failed.</li> <li><code>env.runtime</code> contains runtime hints; expanders may add keys in future, but   existing keys will remain stable.</li> </ul>"},{"location":"reference/manifests-and-metrics/#restoria-metricsjson","title":"Restoria metrics.json","text":"<pre><code>{\n  \"metrics\": [\n    {\n      \"input\": \"samples/portrait.jpg\",\n      \"backend\": \"gfpgan\",\n      \"restored_img\": \"out/portrait.png\",\n      \"metrics\": {\n        \"arcface_cosine\": 0.91,\n        \"lpips_alex\": 0.18,\n        \"dists\": 0.14\n      }\n    }\n  ],\n  \"plan\": {\n    \"backend\": \"gfpgan\",\n    \"reason\": \"requested backend\",\n    \"params\": {\"weight\": 0.6}\n  }\n}\n</code></pre> <p>Notes:</p> <ul> <li>When <code>--metrics off</code>, values may be empty/omitted; <code>metrics.json</code> still   exists with <code>{\"metrics\": [...]}</code> to keep automation stable.</li> <li><code>plan</code> is present in Restoria to make routing decisions auditable.</li> </ul>"},{"location":"reference/manifests-and-metrics/#gfpp-files","title":"GFPP files","text":"<p>Written by <code>gfpup run</code>.</p>"},{"location":"reference/manifests-and-metrics/#gfpp-manifestjson","title":"GFPP manifest.json","text":"<p>Built via <code>src/gfpp/io/manifest.py</code>:</p> <pre><code>{\n  \"args\": {\"backend\": \"gfpgan\", \"metrics\": \"fast\", \"device\": \"auto\"},\n  \"device\": \"cpu\",\n  \"started_at\": 1699999999.12,\n  \"ended_at\": 1699999999.85,\n  \"results\": [\n    {\"input\": \"...\", \"restored_img\": \"...\", \"metrics\": {\"runtime_sec\": 0.52}}\n  ],\n  \"models\": [],\n  \"metrics_file\": \"metrics.json\",\n  \"env\": {\"torch\": \"2.x\", \"cuda\": false, \"git\": \"abc123\"}\n}\n</code></pre>"},{"location":"reference/manifests-and-metrics/#gfpp-metricsjson","title":"GFPP metrics.json","text":"<p>GFPP writes a simple wrapper:</p> <pre><code>{\"metrics\": [ {\"input\": \"...\", \"restored_img\": \"...\", \"metrics\": {}} ]}\n</code></pre> <p>Fields in <code>metrics</code> may include:</p> <ul> <li><code>arcface_cosine</code> (higher is better)</li> <li><code>lpips_alex</code> (lower is better)</li> <li><code>dists</code> (lower is better)</li> <li><code>runtime_sec</code>, <code>vram_mb</code>, and backend-specific extras</li> </ul>"},{"location":"reference/manifests-and-metrics/#stability-guarantees","title":"Stability guarantees","text":"<ul> <li>Key names are stable once released. New keys may be added, but existing keys   will not be renamed.</li> <li>Missing optional metrics must be represented as <code>null</code> or omitted; consumers   should not rely on their presence.</li> <li>File locations: both CLIs default to writing into the specified <code>--output</code>   directory. Restoria uses <code>manifest.json</code> and <code>metrics.json</code> at the root of   that directory.</li> </ul>"},{"location":"usage/api/","title":"API Usage (FastAPI)","text":"<ul> <li>Install extras: <code>pip install -e .[api]</code></li> <li>Run locally with uvicorn:</li> </ul> <pre><code>uvicorn services.api.main:app --reload --port 8000\n</code></pre> <ul> <li>Health: <code>GET /healthz</code></li> <li>Restore: <code>POST /restore</code> (multipart/form-data)</li> <li>form field <code>files</code>: one or more images</li> <li>query params: <code>version=1.4</code>, <code>upscale=2</code>, <code>backend=gfpgan</code>, <code>device=auto</code>,     <code>dry_run=true</code>, <code>metrics=fast|full|off</code></li> </ul> <p>Notes</p> <ul> <li>Default is <code>dry_run=true</code> for smoke-safety; set <code>dry_run=false</code> for actual   restoration.</li> <li>In non-dry-run, the server resolves weights via centralized weight logic   (same as CLI) and runs the selected backend.</li> <li>CI runs an API smoke check as part of the workflow.</li> </ul>"},{"location":"usage/cli/","title":"CLI Usage","text":"<p>Two CLIs are available:</p> <ul> <li><code>gfpup</code> \u2014 new, modular CLI powered by <code>gfpp</code> (recommended)</li> <li><code>gfpgan-infer</code> \u2014 legacy shim, kept for backward compatibility</li> </ul>"},{"location":"usage/cli/#gfpup-run","title":"gfpup run","text":"<p>Key flags:</p> <ul> <li><code>--input</code> file or folder of images</li> <li><code>--backend</code> one of: gfpgan, codeformer, restoreformerpp, ensemble, ...</li> <li><code>--auto</code> enable planner to choose a backend and normalized params</li> <li><code>--metrics</code> off|fast|full (fast = ArcFace; full = ArcFace + LPIPS + DISTS)</li> <li><code>--device</code> auto|cpu|cuda|mps</li> <li><code>--dry-run</code> parse/plan and write manifest without executing</li> <li><code>--plan-only</code> print the plan and exit (no IO)</li> <li><code>--compile</code> try torch.compile (safe fallback)</li> <li><code>--ort-providers</code> providers for ORT when applicable</li> </ul> <p>Examples:</p> <pre><code># Basic run with GFPGAN\ngfpup run --input inputs/whole_imgs --backend gfpgan --output out/\n\n# Let the planner pick a backend and parameters\ngfpup run --input samples/portrait.jpg --auto --metrics fast --output out/\n\n# Plan-only / dry-run\ngfpup run --input samples/portrait.jpg --plan-only\ngfpup run --input samples/portrait.jpg --dry-run --output out/\n\n# Deterministic with seed\ngfpup run --input samples/portrait.jpg --seed 123 --deterministic\n</code></pre> <p>Outputs:</p> <ul> <li><code>manifest.json</code> \u2014 args, runtime, results, models used</li> <li><code>metrics.json</code> \u2014 per-image metrics (values may be None if unavailable)</li> </ul> <p>Notes:</p> <ul> <li>Optional features degrade gracefully; missing metrics result in <code>null</code>   values rather than errors.</li> <li>Heavy libraries (torch, cv2) are imported lazily to keep startup fast.</li> </ul>"},{"location":"usage/cli/#legacy-shim-gfpgan-infer","title":"Legacy shim: gfpgan-infer","text":"<pre><code>usage: gfpgan-infer [-i INPUT] [-o OUTPUT] [-v VERSION] [-s UPSCALE]\n                    [--bg_upsampler BG_UPSAMPLER] [--bg_tile BG_TILE]\n                    [--suffix SUFFIX] [--only_center_face] [--aligned]\n                    [--ext EXT] [-w WEIGHT]\n                    [--device {auto,cpu,cuda}] [--dry-run] [--no-download]\n                    [--model-path MODEL_PATH] [--seed SEED] [--no-cmp]\n</code></pre> <p>Common examples:</p> <ul> <li><code>gfpgan-infer -i inputs/whole_imgs -o results -v 1.4 -s 2 --device auto</code></li> <li><code>gfpgan-infer -i my.jpg -v 1.3 --no-download --model-path ./weights/GFPGANv1.3.pth</code></li> <li><code>gfpgan-infer --dry-run -v 1.4</code> (validate and exit)</li> </ul> <p>Notes:</p> <ul> <li>On CPU, Real-ESRGAN background upsampling may be disabled for speed.</li> <li><code>--seed</code> sets seeds for random, numpy, and torch.</li> <li>The shim prints a deprecation warning to stderr but maintains flag behavior   and output layout for backward compatibility.</li> </ul>"},{"location":"usage/colab/","title":"Colab Guide","text":"<p>Open the notebook:</p> <ul> <li>GFPGAN Colab Notebook</li> </ul> <p>Features</p> <ul> <li>Install cell sets up Torch + Basicsr master (for torchvision compatibility)</li> <li>Interactive UI for:</li> <li>Uploading images</li> <li>Fetching images from URLs</li> <li>Optional Drive mount</li> <li>Selecting version, upscale, weight, and options</li> <li>Running inference and previewing results</li> <li>ZIP download of the results directory</li> <li>First-image before/after slider when original is available</li> </ul> <p>Tips</p> <ul> <li>Use a GPU runtime to enable background upsampling (Real-ESRGAN) and for speed.</li> <li>The notebook prints versions of Torch, Torchvision, and Basicsr to help debugging.</li> </ul>"},{"location":"usage/gradio/","title":"Local Gradio App","text":"<p>Run a lightweight local UI for GFPGAN.</p> <ul> <li>Install (editable): <code>pip install -e .[dev]</code></li> <li>Launch: <code>gfpgan-gradio --server-port 7860 --share</code></li> <li>Options: <code>--server-name 0.0.0.0</code> (default), <code>--share</code> for a public link.</li> </ul> <p>Features - Upload multiple images, pick version, device (auto/cpu/cuda), upscale, weight. - Choose detector, enable/disable face parsing. - Optional background upsampler with precision and tile controls (GPU). - Displays restored images and device info.</p> <p>Notes - The app lazily loads models and uses the same inference logic as the CLI. - For better results, download weights first: <code>gfpgan-download-weights -v 1.4</code>.</p>"},{"location":"usage/gradio/#docker-local-app","title":"Docker (local app)","text":"<p>Build and run the Gradio app via Docker (CPU):</p> <ul> <li>Build: <code>docker build -t gfpgan-app .</code></li> <li>Run: <code>docker run --rm -p 7860:7860 gfpgan-app</code></li> </ul> <p>Then open http://localhost:7860.</p> <p>Notes - The image installs Torch 2.x CPU wheels and BasicSR master for compatibility. - For GPU, consider a CUDA base image and matching Torch wheels (not included here).</p>"},{"location":"usage/recipes/","title":"CLI Recipes","text":"<ul> <li>CPU batch of images, disable background, cap to 20 images</li> </ul> <pre><code>restoria run --input inputs/whole_imgs \\\n  --output results --backend gfpgan \\\n  --device cpu --metrics off\n\ngfpup run --input inputs/whole_imgs \\\n  --backend gfpgan --output results --device cpu\n\n# Legacy\ngfpgan-infer -i inputs/whole_imgs -o results -v 1.4 \\\n  --device cpu --bg_upsampler none --max-images 20\n</code></pre> <ul> <li>GPU quality run with weight sweep and manifest</li> </ul> <pre><code>gfpup run --input my.jpg --backend gfpgan \\\n  --output results --metrics full\n\n# Legacy\ngfpgan-infer -i my.jpg -o results -v 1.4 --device cuda \\\n  --sweep-weight 0.3,0.5,0.7 --manifest results/manifest.json\n</code></pre> <ul> <li>Deterministic CUDA with seed (potentially slower)</li> </ul> <pre><code>restoria run --input my.jpg --backend gfpgan \\\n  --seed 123 --deterministic --output results\ngfpup run --input my.jpg --backend gfpgan \\\n  --seed 123 --deterministic --output results\n\n# Legacy\ngfpgan-infer -i my.jpg -v 1.4 --device cuda --deterministic-cuda --seed 123\n</code></pre> <ul> <li>Change detector and parsing behavior</li> </ul> <pre><code>gfpup run --input img.jpg --backend gfpgan --output results\n\n# Legacy\ngfpgan-infer -i img.jpg -v 1.3 --detector scrfd --no-parse\n</code></pre> <ul> <li>Print environment versions and run</li> </ul> <pre><code>gfpup doctor &amp;&amp; gfpup run --input img.jpg \\\n  --backend gfpgan --output results --verbose\n\n# Legacy\ngfpgan-infer -i img.jpg -v 1.4 --print-env --verbose\n</code></pre>"},{"location":"usage/recipes/#performance-autopilot-tips","title":"Performance &amp; Autopilot Tips","text":"<ul> <li>Autopilot (<code>--auto</code>):</li> <li>For common photos, try: <code>--auto --select-by sharpness</code>.</li> <li>For portraits where identity matters: <code>--auto --select-by identity</code>     (falls back to sharpness if identity backend is unavailable).</li> <li> <p>Autopilot tries a small set of model/weight combos (e.g., 1.2/1.3 with     0.3/0.5) and picks the best by the metric on the restored image.</p> </li> <li> <p>Hardware-aware defaults (<code>--auto-hw</code>):</p> </li> <li>On CUDA: sets <code>--bg_precision fp16</code> and tiles based on VRAM (0/600/400 for     high/med/low VRAM).</li> <li> <p>On CPU: sets a sensible <code>--workers</code> value up to 4.</p> </li> <li> <p>CPU concurrency (<code>--workers N</code>):</p> </li> <li>Parallelizes images across processes; each worker maintains its own restorer.</li> <li> <p>Start with 2\u20134 workers depending on core count and memory; avoid high values.</p> </li> <li> <p>Quality/perf trade-offs:</p> </li> <li>Increase <code>--bg_tile</code> or disable background (<code>--bg_upsampler none</code>) if you     encounter OOM.</li> <li>Lower <code>--upscale</code> for faster throughput on large batches.</li> </ul>"}]}