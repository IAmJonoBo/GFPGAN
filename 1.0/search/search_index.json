{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GFPGAN","text":""},{"location":"#professional-face-restoration-powered-by-generative-ai","title":"Professional face restoration powered by generative AI","text":"<p>Restore---</p> <p>Ready to get started? Install GFPGAN \u2192h state-of-the-art generative AI. GFPGAN combines deep learning and generative adversarial networks to intelligently reconstruct facial details from low-quality, damaged, or blurred images.</p>"},{"location":"#three-ways-to-get-started","title":"Three ways to get started","text":""},{"location":"#material-image-restore-a-photo","title":":material-image: Restore a photo","text":"<p>Single image restoration with CLI or web interface</p> CLIWeb UI <pre><code>pip install gfpgan\ngfpgan-infer --input photo.jpg --version 1.4\n</code></pre> <pre><code>python -m gfpgan.gradio_app\n# Open http://localhost:7860\n</code></pre> <p>\u2192 Restore a photo guide</p>"},{"location":"#material-folder-multiple-batch-process","title":":material-folder-multiple: Batch process","text":"<p>Process entire folders with consistent quality</p> <pre><code>gfpgan-infer --input photos/ --backend gfpgan --metrics fast --output results/\n</code></pre> <p>\u2192 Batch processing guide</p>"},{"location":"#material-chart-line-measure-quality","title":":material-chart-line: Measure quality","text":"<p>Objective evaluation with LPIPS, DISTS, and ArcFace metrics</p> <pre><code>gfpgan-infer --input photos/ --metrics detailed --report-path quality_report.json\n</code></pre> <p>\u2192 Quality metrics guide</p>"},{"location":"#core-capabilities","title":"Core capabilities","text":"Feature Description Multiple backends GFPGAN, CodeFormer, RestoreFormer++ - choose speed vs quality Cross-platform Windows, macOS, Linux with GPU acceleration (CUDA, MPS, DirectML) Privacy-first Process images locally\u2014no cloud uploads required Production-ready Clean API, robust CLI, batch processing with provenance tracking Measurable quality Built-in metrics for objective evaluation"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>API documentation \u2014 FastAPI auto-docs and examples</li> <li>Choose the right backend \u2014 Speed vs quality comparison</li> <li>Hardware setup \u2014 GPU optimization and troubleshooting</li> <li>FAQ \u2014 Common questions and solutions</li> </ul>"},{"location":"#platform-support","title":"Platform support","text":"Platform GPU Acceleration Status Linux CUDA 11.8+ / ROCm \u2705 Full support Windows CUDA / DirectML \u2705 Full support macOS Metal Performance Shaders \u2705 Full support CPU-only All platforms \u2705 Slower, but works <p>Ready to get started? Install GFPGAN \u2192</p> <p>Ready to get started? Install GFPGAN \u2192Documentation</p> <p>Welcome to the fork documentation. This fork focuses on modern developer ergonomics, a smoother Colab experience, and practical inference.</p> <ul> <li>What\u2019s new vs upstream:</li> <li>Modern CI and linting</li> <li>Colab with interactive UI and compatibility fixes (BasicSR master + modern torchvision)</li> <li>CLI quality-of-life flags and console entrypoint</li> <li>Safer repository defaults and optional light tests</li> </ul> <p>Quick links - Quickstart: ./quickstart.md - CLI Usage: ./usage/cli.md - Colab Guide: ./usage/colab.md - Compatibility Matrix: ./COMPATIBILITY.md - Contributing: ./contributing.md</p> <p>Upstream reference: https://github.com/TencentARC/GFPGAN</p>"},{"location":"ACCESSIBILITY/","title":"Accessibility (WCAG 2.2 AA)","text":"<ul> <li>Visible focus outlines with adequate contrast</li> <li>Keyboard-first navigation and shortcuts</li> <li>Before/after slider must be usable without a mouse</li> <li>Motion kept subtle (ease-out 160\u2013220ms)</li> <li>Dark/light themes with system preference sync</li> </ul>"},{"location":"BACKEND_MATRIX/","title":"Backend Matrix","text":"Engine Torch 2.x ONNX Runtime TensorRT MPS Notes GFPGAN \u2713 planned planned \u2713 Baseline CodeFormer \u2713 (extra) planned planned \u2713 Optional RestoreFormer++ \u2713 (extra) planned planned \u2713 Optional DiffBIR EXPERIMENTAL \u2013 \u2013 \u2013 Heavy HYPIR EXPERIMENTAL \u2013 \u2013 \u2013 Heavy <p>Background upsamplers:</p> Upsampler Torch ONNX Notes RealESRGAN \u2713 \u2013 Default on CUDA SwinIR planned \u2013 Optional"},{"location":"COMPATIBILITY/","title":"Compatibility Matrix","text":"<p>This fork supports two primary tracks to balance stability and modern stacks.</p> <ul> <li>Torch 1.x (stable, Python 3.10): use <code>extras = [dev, torch1]</code> or <code>-c constraints-3.10-compat.txt</code>.</li> <li>Torch 2.x (modern, Python 3.11+): use <code>extras = [dev, torch2]</code>. Install BasicSR from master if needed.</li> </ul> <p>Known-good combinations</p> <ul> <li>Python 3.10 + Torch 1.13.1 + torchvision 0.14.1 + basicsr 1.4.2</li> <li>Python 3.11 + Torch 2.4.1 + torchvision 0.19.1 + basicsr (master)</li> </ul> <p>Matrix (guide) - CPU (3.11): Torch 2.4.1, Torchvision 0.19.1, Basicsr master, facexlib &gt;=0.3.0 - CUDA 12.1 (3.11): install torch/torchvision/torchaudio from PyTorch CUDA 12.1 index; Basicsr master - OpenCV: use prebuilt wheels (&gt;=4.8) for 3.11; older tracks pin to &lt;4.8</p> <p>Notes - Basicsr 1.4.2 uses <code>torchvision.transforms.functional_tensor</code> which was removed in torchvision 0.15+. For Torch 2.x stacks, install BasicSR from GitHub master. - CPU runs disable Real-ESRGAN background upsampling by default for speed. - Apple Silicon: prefer Python 3.10 + constraints for prebuilt wheels; Torch 2.x on arm64 is improving but may pull heavier builds.</p> <p>Quick recipes - 3.10 stable (Torch 1.x): <code>scripts/setup_uv.sh --python 3.10 --track torch1</code> - 3.11 modern (Torch 2.x): <code>scripts/setup_uv.sh --python 3.11 --track torch2</code> - Pip + constraints (3.10): <code>pip install -e .[dev] -c constraints-3.10-compat.txt</code></p>"},{"location":"DECISION_TABLE/","title":"Engine Decision Table","text":"<p>This table summarizes when to prefer each engine and the trade\u2011offs involved.</p> <ul> <li>GFPGAN (default)</li> <li>Strengths: Natural outputs, good identity retention on most inputs, fast with Torch 2.x and <code>torch.compile</code>.</li> <li>Use when: Mixed quality inputs, portraits, multi\u2011face scenes.</li> <li> <p>Notes: Background upsampling via RealESRGAN improves whole\u2011image perception.</p> </li> <li> <p>CodeFormer</p> </li> <li>Strengths: Robust on severely degraded faces (blur, compression); fidelity control from 0..1.</li> <li>Use when: Very low\u2011quality inputs, surveillance\u2011like frames.</li> <li> <p>Notes: Higher fidelity values can preserve more original structure at the cost of fine detail.</p> </li> <li> <p>RestoreFormer / RestoreFormer++</p> </li> <li>Strengths: Identity\u2011faithful restoration; stable on higher\u2011quality inputs.</li> <li>Use when: Faces are mostly sharp; you want minimal identity drift.</li> <li> <p>Notes: \"++\" variant uses updated weights when available.</p> </li> <li> <p>GFPGAN (ONNX Runtime)</p> </li> <li>Strengths: Portable runtime; can leverage CUDA/TensorRT/DirectML/CoreML providers.</li> <li>Use when: You need deployment flexibility or lower latency with hardware\u2011accelerated EPs.</li> <li> <p>Notes: Provide <code>--model-path-onnx</code> or JobSpec <code>model_path_onnx</code>.</p> </li> <li> <p>DiffBIR / HYPIR (experimental)</p> </li> <li>Strengths: Diffusion\u2011prior and heavy models for challenging scenes.</li> <li>Use when: Research or advanced users; expect longer runtimes.</li> <li>Notes: Behind feature flags; not included in CI.</li> </ul> <p>Trade\u2011offs - Identity vs Detail: CodeFormer (with fidelity) and RestoreFormer++ bias toward identity; GFPGAN often yields more natural detail. - Speed vs Quality: Use the Quality preset to map tiling/precision; ORT providers can improve latency. The optimizer can evaluate several weights and pick the best under a time budget.</p> <p>Metrics - ArcFace cosine (identity): Higher is better; used for identity lock and optimizer. - LPIPS/DISTS (perceptual): Lower is better; optional in fast/full metrics modes. - NIQE/BRISQUE (no\u2011ref): Optional for auto backend heuristics.</p>"},{"location":"DEV_ENV/","title":"DEV ENV","text":"<p>Development Environment (uv)</p> <p>Overview - Uses uv for fast, reproducible Python environments and locking. - Default target: Python 3.10 with a CPU-only stack compatible with BasicSR 1.4.2.</p> <p>Prereqs - Install uv: macOS <code>brew install uv</code>, Linux <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code>. - Python 3.10 recommended. 3.11+ is experimental (see below).</p> <p>Quick Start (3.11, Torch 2.x default) 1) Sync deps (with dev tools):    - <code>scripts/setup_uv.sh --python 3.11 --track torch2</code> 2) Run tests:    - <code>scripts/test.sh</code> 3) Lint / format:    - <code>scripts/lint.sh</code>    - <code>scripts/fmt.sh</code></p> <p>Notes - The runtime depends on <code>torch</code> and <code>torchvision</code>. For compatibility with <code>basicsr==1.4.2</code>, we supply a Torch 1.x extra (<code>torch1</code>) and version markers to keep NumPy/Skimage/OpenCV in a compatible range for Python 3.10. - If you install via pip directly, you can use the constraints file: <code>pip install -r requirements.txt -c constraints-3.10-compat.txt</code>.</p> <p>Python 3.10 (Torch 1.x track; stable) - For maximum compatibility with <code>basicsr==1.4.2</code> and CPU-only usage:   - <code>scripts/setup_uv.sh --python 3.10 --track torch1</code>   - Or with pip + constraints: <code>pip install -r requirements.txt -c constraints-3.10-compat.txt</code></p> <p>Torch 2.x notes - Torch 2.x (3.11+) may require Basicsr master:   - <code>uv pip install --no-cache-dir --upgrade \"git+https://github.com/xinntao/BasicSR@master\"</code>   - Alternatively use <code>constraints-3.11-experimental.txt</code> with pip.</p> <p>Apple Silicon - All constraints are chosen to have prebuilt wheels on macOS arm64 where possible. If you hit build issues, ensure you\u2019re on Python 3.10 and use the 3.10 constraints.</p>"},{"location":"GFPP_QUICKSTART/","title":"GFPP Quickstart","text":"<p>Install (editable) with optional extras:</p> <pre><code>pip install -e \".[dev,metrics,arcface,codeformer,restoreformerpp,ort,web]\"\n</code></pre> <p>Run the new CLI (GFPGAN baseline):</p> <pre><code>gfpup run --input inputs/whole_imgs --backend gfpgan --metrics fast --output out/\n# Optional flags:\n#   --dry-run (copy inputs \u2192 outputs, no model load)\n#   --quality {quick|balanced|best} (maps BG SR tile/precision)\n#   --auto-backend (choose engine per-image)\n#   --identity-lock --identity-threshold 0.25\n#   --optimize --weights-cand \"0.3,0.5,0.7\"\n</code></pre> <p>API dev server:</p> <pre><code>uvicorn services.api.main:app --reload\n# REST endpoints:\n#   POST /jobs (JobSpec)\n#   GET  /jobs, /jobs/:id, /results/:id\n#   WS   /jobs/:id/stream (status/image/manifest/eof events)\n#   POST /jobs/:id/rerun (overrides)\n</code></pre> <p>Web (dev):</p> <pre><code>cd apps/web\npnpm i\npnpm dev\n</code></pre>"},{"location":"HARDWARE_GUIDE/","title":"Hardware &amp; Performance Guide","text":"<ul> <li>CUDA GPUs (recommended): enables RealESRGAN background upsampling and torch.compile.</li> <li>Apple Silicon (MPS): GFPGAN works; ONNX CoreML EP planned.</li> <li>CPU-only: Works for small images; disable background upsampling for speed.</li> </ul> <p>Tips: - Use <code>--compile default</code> to JIT-compile models on Torch 2.x. - Adjust tile size and precision (<code>fp16</code>) based on VRAM. - Set <code>--seed</code> and <code>--deterministic</code> for reproducible runs.</p>"},{"location":"ORT_GUIDE/","title":"ONNX Runtime Guide (GFPP)","text":"<p>This guide explains how to run GFPGAN with ONNX Runtime.</p> <ul> <li>Install ORT:</li> <li>CPU: <code>pip install onnxruntime</code></li> <li> <p>CUDA/TensorRT: <code>pip install onnxruntime-gpu</code> (and ensure CUDA/TensorRT installed)</p> </li> <li> <p>Export ONNX graph (outline):</p> </li> <li>The CLI has a stub: <code>gfpup export-onnx --version 1.4 --model-path /path/GFPGANv1.4.pth --out gfpgan.onnx</code></li> <li> <p>Implement export by initializing <code>restorer.gfpgan</code> and calling <code>torch.onnx.export</code> with a 512x512 BCHW normalized dummy input. Validate with ORT.</p> </li> <li> <p>Run ORT backend:</p> </li> <li>CLI: <code>gfpup run --input inputs/whole_imgs --backend gfpgan-ort --model-path-onnx /path/gfpgan.onnx --output out/</code></li> <li> <p>API: pass <code>{\\\"backend\\\":\\\"gfpgan-ort\\\",\\\"model_path_onnx\\\":\\\"/path/gfpgan.onnx\\\"}</code> in the JobSpec.</p> </li> <li> <p>Providers:</p> </li> <li> <p>The runtime logs available providers and the selected provider; metrics include <code>ort_provider</code> and <code>ort_init_sec</code>.</p> </li> <li> <p>Fallback:</p> </li> <li>If ORT initialization or inference is not available, the system falls back to the Torch backend transparently and records <code>backend: onnxruntime+torch-fallback</code>.</li> </ul>"},{"location":"TORCH2/","title":"TORCH2","text":"<p>Torch 2.x Migration Path (Experimental)</p> <p>Goals - Support Python 3.11+ and Torch/Torchvision 2.x while keeping the stable 3.10 + Torch 1.x path intact.</p> <p>Status - GFPGAN code imports from <code>torchvision.ops</code> and <code>torchvision.transforms.functional</code>, which are compatible with 0.17\u20130.19. - Known blocker was in <code>basicsr==1.4.2</code> using <code>torchvision.transforms.functional_tensor</code> removed in torchvision 0.15+. Upstream master addresses this.</p> <p>How to Try 1) Create a uv env for Python 3.11 with Torch 2.x extra:    - <code>uv sync -p 3.11 -E dev -E torch2</code> 2) If you hit Basicsr import errors, install master:    - <code>uv pip install --no-cache-dir --upgrade \"git+https://github.com/xinntao/BasicSR@master\"</code> 3) Run tests:    - <code>uv run pytest -q</code></p> <p>Notes - Some torchvision ops (e.g., <code>roi_align</code>) rely on compiled C++/CUDA ops. CPU wheels include C++ kernels; CUDA requires matching CUDA wheels. - If you need CUDA, install the appropriate Torch/Torchvision CUDA wheels and re-sync the environment. - If downstream regressions appear (numerics, tolerance), we can add conditional branches or loosen test tolerances for the Torch 2.x track.</p> <p>Next Steps (if adopting Torch 2.x by default) - Change the default scripts to <code>--track torch2</code> and set CI matrix to Python 3.11. - Remove Torch 1.x constraints when Basicsr master (or a tagged release) is required and stable.</p>"},{"location":"UI_GUIDE/","title":"UI Guide (GFPP Web)","text":"<p>The web app (Next.js 15 + React 19) provides a one-screen flow:</p> <ul> <li>Controls: Backend (Torch/ORT/CodeFormer/RF++), ONNX model path (for ORT), Preset, Metrics, Background, Quality, Auto Backend, Identity Lock.</li> <li>Submit: \"Submit Dry-Run Job\" sends a job to the local API and opens a WebSocket stream for progress.</li> <li>Queue: shows job list with progress bars, a ZIP download link when finished, and a \"Re-run (dry)\" button.</li> <li>Results: a gallery of before/after comparisons and per-image metrics cards. Identity lock retries display a green badge.</li> <li>Per-image Re-run: \"Re-run this (dry)\" submits a rerun for that specific input using the current controls.</li> </ul> <p>Dev setup:</p> <pre><code>uvicorn services.api.main:app --reload --port 3000\ncd apps/web &amp;&amp; pnpm i &amp;&amp; pnpm dev\n</code></pre> <p>Images are proxied via <code>/file?path=</code> for convenience in dev; in production, serve static files via a proper file server.</p>"},{"location":"contributing/","title":"Contributing (Fork)","text":"<p>Thanks for considering a contribution! This fork aims to remain close to upstream while improving practical usage.</p> <ul> <li>Dev setup</li> <li><code>pip install -e .[dev]</code></li> <li>Enable pre-commit hooks: <code>pre-commit install</code></li> <li>Lint locally: <code>ruff check . &amp;&amp; black .</code></li> <li> <p>Run light tests: <code>pytest -q tests_light</code></p> </li> <li> <p>PR guidelines</p> </li> <li>Keep diffs focused and well-described</li> <li>Prefer small, reviewable changes</li> <li> <p>Add or update tests when changing behavior</p> </li> <li> <p>Syncing with upstream</p> </li> <li>We periodically pull from upstream <code>TencentARC/GFPGAN</code> and resolve conflicts.</li> </ul> <p>Code of Conduct: see <code>CODE_OF_CONDUCT.md</code>.</p>"},{"location":"faq/","title":"FAQ","text":"<ul> <li>Which Python/Torch should I use?</li> <li> <p>Python 3.11 with Torch 2.x is recommended. See docs/COMPATIBILITY.md for details.</p> </li> <li> <p>Why is background upsampling disabled on CPU?</p> </li> <li> <p>Real-ESRGAN is heavy on CPU and often slower than desired; this fork disables it by default on CPU for a smoother UX. Use GPU for best results.</p> </li> <li> <p>How can I run inference without downloads?</p> </li> <li> <p>Use <code>--no-download</code> and pass <code>--model-path /path/to/GFPGANv1.4.pth</code>.</p> </li> <li> <p>How do I quickly validate the CLI works locally?</p> </li> <li> <p><code>gfpgan-infer --dry-run -v 1.4</code> parses arguments and exits.</p> </li> <li> <p>Where is the docs site?</p> </li> <li>Once enabled via GitHub Pages on gh-pages, the site will be available at the repo Pages URL.</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":"<ul> <li>Install (editable):</li> <li><code>pip install -e .[dev]</code></li> <li>Download weights (optional, local cache):</li> <li>List: <code>gfpgan-download-weights --list</code></li> <li>Fetch v1.4: <code>gfpgan-download-weights -v 1.4</code></li> <li>Inference on CPU or GPU:</li> <li><code>gfpgan-infer --input inputs/whole_imgs --version 1.4 --upscale 2 --device auto</code></li> <li>Optional: <code>--compile default</code> (Torch 2.x) for speedups</li> <li>Helpful flags:</li> <li><code>--dry-run</code> (validate args), <code>--no-download</code> (require local weights), <code>--model-path</code> (override weights)</li> <li><code>--bg_upsampler none</code> (disable background upsampling), <code>--bg_precision fp32|fp16|auto</code></li> <li><code>--detector scrfd|retinaface_*</code> (switch detector), <code>--no-parse</code> (disable parsing)</li> <li><code>--manifest outputs.json</code> to save a simple results manifest</li> <li><code>--sweep-weight 0.3,0.5,0.7</code> to run multiple weights</li> <li><code>--print-env</code> to display versions and CUDA availability</li> <li>Quality: <code>--jpg-quality 95</code>, <code>--png-compress 3</code>, <code>--webp-quality 90</code></li> <li>Colab notebook (UI):</li> <li>GFPGAN Colab Demo</li> </ul> <p>Weights &amp; cache - Default weights dir: <code>gfpgan/weights</code> (override with <code>GFPGAN_WEIGHTS_DIR</code>) - Hugging Face Hub: set <code>GFPGAN_HF_REPO</code> to enable hub resolution; set <code>HF_HUB_OFFLINE=1</code> for cache-only</p> <p>See: CLI Usage (./usage/cli.md) and Colab Guide (./usage/colab.md).</p> <p>Known good versions - Python 3.11 with Torch 2.x track (e.g., torch 2.4.1, torchvision 0.19.1) - Basicsr from master when using modern torchvision (CI/Colab default)</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<ul> <li>Basicsr import error with <code>functional_tensor</code></li> <li>Symptom: <code>ModuleNotFoundError: No module named 'torchvision.transforms.functional_tensor'</code></li> <li> <p>Fix: Install Basicsr from master (works with modern torchvision)</p> <ul> <li>Pip: <code>pip uninstall -y basicsr &amp;&amp; pip install --no-cache-dir --force-reinstall \"git+https://github.com/xinntao/BasicSR@master\"</code></li> <li>Colab: already handled in the install cell.</li> </ul> </li> <li> <p>Colab is slow or runs out of memory</p> </li> <li>Use a GPU runtime (Runtime \u2192 Change runtime type \u2192 GPU)</li> <li>Keep background upsampling disabled on CPU (default in this fork)</li> <li> <p>Reduce <code>--upscale</code> or batch size (fewer/lighter images)</p> </li> <li> <p>Heavy installs on CI/Colab</p> </li> <li>Prefer pinned Python (3.11) with Torch 2.x track</li> <li> <p>The tests (light) job avoids heavyweight tests and validates CLI and imports</p> </li> <li> <p>Deterministic results</p> </li> <li>Use <code>--seed</code> for reproducibility (random, numpy, torch)</li> <li> <p>Consider CPU for stricter determinism (or set CUDA deterministic options)</p> </li> <li> <p>Missing model weights</p> </li> <li>Use <code>--model-path</code> to point to local weights</li> <li>Or remove <code>--no-download</code> to allow fetching from the release URLs</li> </ul>"},{"location":"about/acknowledgements/","title":"Acknowledgements","text":"<p>We express our gratitude to the researchers, developers, and communities whose foundational work and contributions made GFPGAN possible.</p>"},{"location":"about/acknowledgements/#research-foundations","title":"Research Foundations","text":""},{"location":"about/acknowledgements/#original-research","title":"Original Research","text":"<p>This project builds upon the foundational research and implementations from:</p> <p>GFPGAN: Towards Real-World Blind Face Restoration with Generative Facial Prior Xintao Wang, Yu Li, Honglun Zhang, Ying Shan IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021 Paper | Original Implementation</p> <p>We acknowledge and thank the original authors for their groundbreaking research in generative facial priors and their open-source contribution that enabled this project.</p>"},{"location":"about/acknowledgements/#core-dependencies","title":"Core Dependencies","text":""},{"location":"about/acknowledgements/#essential-libraries","title":"Essential Libraries","text":"<p>BasicSR - Super-Resolution Framework Xintao Wang and contributors GitHub Provides the fundamental training and inference framework for super-resolution models.</p> <p>FaceXLib - Face Detection and Analysis Xintao Wang and contributors GitHub Essential for face detection, alignment, and facial feature analysis.</p>"},{"location":"about/acknowledgements/#model-architectures","title":"Model Architectures","text":"<p>StyleGAN2 - Generative Adversarial Networks Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila Provides the core generative architecture for high-quality face synthesis.</p> <p>Real-ESRGAN - Real-World Image Super-Resolution Xintao Wang and contributors Background enhancement and general image upscaling capabilities.</p>"},{"location":"about/acknowledgements/#technical-infrastructure","title":"Technical Infrastructure","text":""},{"location":"about/acknowledgements/#deep-learning-frameworks","title":"Deep Learning Frameworks","text":"<p>PyTorch - Machine Learning Framework Facebook AI Research and contributors The foundation for all model training and inference operations.</p> <p>OpenCV - Computer Vision Library Intel Corporation and contributors Essential for image processing, manipulation, and I/O operations.</p>"},{"location":"about/acknowledgements/#web-and-api-framework","title":"Web and API Framework","text":"<p>Gradio - Machine Learning Web Interfaces Abubakar Abid and the Gradio team Enables the intuitive web interface for interactive face restoration.</p> <p>FastAPI - Modern Web Framework Sebasti\u00e1n Ram\u00edrez and contributors Powers the REST API for programmatic access to restoration capabilities.</p>"},{"location":"about/acknowledgements/#community-and-development","title":"Community and Development","text":""},{"location":"about/acknowledgements/#development-tools","title":"Development Tools","text":"<p>MkDocs Material - Documentation Framework Martin Donath and contributors Provides the beautiful and functional documentation site.</p> <p>Ruff - Python Linting and Formatting Charlie Marsh and contributors Ensures code quality and consistency throughout the project.</p> <p>Pre-commit - Git Hook Framework Anthony Sottile and contributors Maintains code quality standards and automated checks.</p>"},{"location":"about/acknowledgements/#testing-and-quality-assurance","title":"Testing and Quality Assurance","text":"<p>pytest - Testing Framework Holger Krekel and contributors Comprehensive testing infrastructure for reliability and quality.</p> <p>GitHub Actions - CI/CD Platform GitHub and contributors Automated testing, building, and deployment workflows.</p>"},{"location":"about/acknowledgements/#data-and-evaluation","title":"Data and Evaluation","text":""},{"location":"about/acknowledgements/#evaluation-datasets","title":"Evaluation Datasets","text":"<p>CelebA-HQ - High-Quality Celebrity Faces Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen High-resolution celebrity face dataset for evaluation and benchmarking.</p> <p>FFHQ - Flickr-Faces-HQ Dataset Tero Karras, Samuli Laine, Timo Aila Diverse, high-quality face dataset for comprehensive evaluation.</p>"},{"location":"about/acknowledgements/#metrics-and-evaluation","title":"Metrics and Evaluation","text":"<p>LPIPS - Learned Perceptual Image Patch Similarity Richard Zhang, Phillip Isola, Alexei A. Efros Perceptual similarity measurement for image quality assessment.</p> <p>ArcFace - Additive Angular Margin Loss Jiankang Deng, Jia Guo, Stefanos Zafeiriou Identity preservation evaluation through facial recognition embeddings.</p>"},{"location":"about/acknowledgements/#inspiration-and-related-work","title":"Inspiration and Related Work","text":""},{"location":"about/acknowledgements/#face-restoration-research","title":"Face Restoration Research","text":"<p>DFDNet - Deep Face Dictionary Network Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, Lei Zhang Pioneering work in dictionary-based face restoration.</p> <p>CodeFormer - Learning to Restore Face Images Shangchen Zhou, Kelvin C.K. Chan, Chongyi Li, Chen Change Loy Advanced transformer-based approach to face restoration.</p> <p>RestoreFormer - High-Quality Blind Face Restoration Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, Ping Luo State-of-the-art transformer architecture for face enhancement.</p>"},{"location":"about/acknowledgements/#community-contributions","title":"Community Contributions","text":""},{"location":"about/acknowledgements/#contributors","title":"Contributors","text":"<p>We thank all community members who have contributed to this project through:</p> <ul> <li>Code contributions and bug fixes</li> <li>Documentation improvements</li> <li>Issue reporting and testing</li> <li>Feature suggestions and feedback</li> <li>Community support and discussions</li> </ul>"},{"location":"about/acknowledgements/#special-recognition","title":"Special Recognition","text":"<ul> <li>Beta testers: Early adopters who provided valuable feedback</li> <li>Documentation reviewers: Contributors who improved clarity and accuracy</li> <li>Accessibility advocates: Those who helped improve usability for all users</li> <li>Security researchers: Responsible disclosure of security considerations</li> </ul>"},{"location":"about/acknowledgements/#institutional-support","title":"Institutional Support","text":""},{"location":"about/acknowledgements/#research-community","title":"Research Community","text":"<ul> <li>Computer Vision research community: For advancing the field of face restoration</li> <li>Open-source community: For fostering collaboration and knowledge sharing</li> <li>AI ethics researchers: For guidance on responsible AI development</li> </ul>"},{"location":"about/acknowledgements/#standards-and-guidelines","title":"Standards and Guidelines","text":"<p>Model Cards for Model Reporting Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru Framework for transparent model documentation.</p> <p>Datasheets for Datasets Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III, Kate Crawford Guidelines for comprehensive dataset documentation.</p>"},{"location":"about/acknowledgements/#license-and-legal","title":"License and Legal","text":""},{"location":"about/acknowledgements/#open-source-licenses","title":"Open Source Licenses","text":"<p>This project is made possible by the generous open-source licenses of our dependencies:</p> <ul> <li>Apache License 2.0: Core project license ensuring freedom to use, modify, and distribute</li> <li>MIT License: Many utility libraries and tools</li> <li>BSD Licenses: Scientific computing and computer vision libraries</li> <li>Creative Commons: Documentation and educational content</li> </ul>"},{"location":"about/acknowledgements/#patent-considerations","title":"Patent Considerations","text":"<p>We acknowledge that some techniques used in this project may be covered by patents. Users should ensure compliance with applicable patent laws in their jurisdiction.</p>"},{"location":"about/acknowledgements/#disclaimer","title":"Disclaimer","text":"<p>While we strive to acknowledge all contributors and influences, this list may not be exhaustive. If you believe your work should be acknowledged here, please contact us at acknowledgements@gfpgan.ai.</p> <p>The inclusion of any work, dataset, or contribution in these acknowledgements does not imply endorsement of this project by the original authors or institutions.</p> <p>Contributing to acknowledgements: If you've contributed to this project or believe your work should be acknowledged, please open an issue or contact us directly.</p> <p>Citation: When citing this project, please also consider citing the foundational research and key dependencies that make this work possible.</p>"},{"location":"api/","title":"API Documentation","text":"<p>GFPGAN provides both REST API and Python API for integrating face restoration into your applications.</p>"},{"location":"api/#rest-api","title":"REST API","text":""},{"location":"api/#auto-generated-documentation","title":"Auto-generated Documentation","text":"<p>GFPGAN includes a FastAPI-based REST API with automatic documentation:</p> <ul> <li>Interactive docs: http://localhost:8000/docs (Swagger UI)</li> <li>Alternative docs: http://localhost:8000/redoc (ReDoc)</li> <li>OpenAPI spec: http://localhost:8000/openapi.json</li> </ul>"},{"location":"api/#starting-the-api-server","title":"Starting the API Server","text":"<pre><code># Production server\nuvicorn services.api.main:app --host 0.0.0.0 --port 8000\n\n# Development server with auto-reload\nuvicorn services.api.main:app --reload --port 8000\n</code></pre>"},{"location":"api/#authentication","title":"Authentication","text":"<p>Currently, the API runs without authentication for simplicity. For production deployments, implement authentication:</p> <pre><code># Example: API key authentication\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer\n\nsecurity = HTTPBearer()\n\ndef verify_api_key(token: str = Depends(security)):\n    if token.credentials != \"your-api-key\":\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid API key\"\n        )\n    return token\n</code></pre>"},{"location":"api/#example-api-usage","title":"Example API Usage","text":""},{"location":"api/#single-image-restoration","title":"Single Image Restoration","text":"<p>Request: <pre><code>curl -X POST \"http://localhost:8000/restore\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F \"file=@damaged_photo.jpg\" \\\n  -F \"version=1.4\" \\\n  -F \"upscale=2\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"status\": \"success\",\n  \"restoration_id\": \"12345-abcde\",\n  \"original_size\": [800, 600],\n  \"restored_size\": [1600, 1200],\n  \"processing_time\": 2.3,\n  \"download_url\": \"/download/12345-abcde\"\n}\n</code></pre></p>"},{"location":"api/#batch-processing","title":"Batch Processing","text":"<p>Request: <pre><code>curl -X POST \"http://localhost:8000/batch\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"image_urls\": [\n      \"https://example.com/photo1.jpg\",\n      \"https://example.com/photo2.jpg\"\n    ],\n    \"settings\": {\n      \"version\": \"1.4\",\n      \"upscale\": 2,\n      \"background_enhance\": true\n    }\n  }'\n</code></pre></p> <p>Response: <pre><code>{\n  \"status\": \"accepted\",\n  \"batch_id\": \"batch-67890\",\n  \"estimated_completion\": \"2024-01-15T10:30:00Z\",\n  \"status_url\": \"/batch/batch-67890/status\"\n}\n</code></pre></p>"},{"location":"api/#quality-metrics","title":"Quality Metrics","text":"<p>Request: <pre><code>curl -X POST \"http://localhost:8000/metrics\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F \"original=@original.jpg\" \\\n  -F \"restored=@restored.jpg\" \\\n  -F \"metrics[]=lpips\" \\\n  -F \"metrics[]=dists\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"metrics\": {\n    \"lpips\": 0.234,\n    \"dists\": 0.156,\n    \"arcface_similarity\": 0.892\n  },\n  \"analysis\": {\n    \"quality_score\": 8.5,\n    \"identity_preservation\": \"excellent\",\n    \"texture_realism\": \"high\"\n  }\n}\n</code></pre></p>"},{"location":"api/#python-api","title":"Python API","text":""},{"location":"api/#core-classes","title":"Core Classes","text":""},{"location":"api/#gfpganer","title":"GFPGANer","text":"<p>Main restoration class for processing images:</p> <pre><code>from gfpgan import GFPGANer\nimport cv2\n\n# Initialize restorer\nrestorer = GFPGANer(\n    model_path='experiments/pretrained_models/GFPGANv1.4.pth',\n    upscale=2,\n    arch='clean',\n    channel_multiplier=2,\n    bg_upsampler=None  # or 'realesrgan'\n)\n\n# Restore image\ninput_img = cv2.imread('input.jpg')\ncropped_faces, restored_imgs, restored_faces = restorer.enhance(\n    input_img,\n    has_aligned=False,\n    only_center_face=False,\n    paste_back=True\n)\n\n# Save result\ncv2.imwrite('output.jpg', restored_imgs[0])\n</code></pre>"},{"location":"api/#key-parameters","title":"Key Parameters","text":"<ul> <li>model_path: Path to GFPGAN model file</li> <li>upscale: Upscaling factor (1, 2, or 4)</li> <li>arch: Model architecture ('clean', 'original')</li> <li>channel_multiplier: Channel multiplier for StyleGAN decoder</li> <li>bg_upsampler: Background upsampler ('realesrgan', 'esrgan', None)</li> </ul>"},{"location":"api/#utility-functions","title":"Utility Functions","text":""},{"location":"api/#image-processing","title":"Image Processing","text":"<pre><code>from gfpgan.utils import restore_image\n\n# Simple restoration function\nrestored_img = restore_image(\n    image_path='input.jpg',\n    output_path='output.jpg',\n    version='1.4',\n    upscale=2\n)\n</code></pre>"},{"location":"api/#model-management","title":"Model Management","text":"<pre><code>from gfpgan.utils import download_model, list_models\n\n# Download model if not present\nmodel_path = download_model('GFPGANv1.4')\n\n# List available models\nmodels = list_models()\nprint(models)  # ['GFPGANv1.3', 'GFPGANv1.4', 'RestoreFormer++']\n</code></pre>"},{"location":"api/#quality-metrics_1","title":"Quality Metrics","text":"<pre><code>from gfpgan.metrics import calculate_metrics\n\n# Calculate quality metrics\nmetrics = calculate_metrics(\n    original_img='original.jpg',\n    restored_img='restored.jpg',\n    metrics=['lpips', 'dists', 'arcface']\n)\n\nprint(f\"LPIPS: {metrics['lpips']:.3f}\")\nprint(f\"DISTS: {metrics['dists']:.3f}\")\nprint(f\"ArcFace Similarity: {metrics['arcface']:.3f}\")\n</code></pre>"},{"location":"api/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/#custom-model-configuration","title":"Custom Model Configuration","text":"<pre><code>from gfpgan import GFPGANer\n\n# Custom model configuration\nrestorer = GFPGANer(\n    model_path='path/to/custom_model.pth',\n    upscale=2,\n    arch='clean',\n    channel_multiplier=2,\n    bg_upsampler='realesrgan',\n    device='cuda',  # or 'cpu', 'mps'\n    model_root_path='experiments/pretrained_models'\n)\n\n# Process with custom settings\noutput = restorer.enhance(\n    img=input_image,\n    has_aligned=False,\n    only_center_face=False,\n    paste_back=True,\n    weight=0.5  # Blend weight with original\n)\n</code></pre>"},{"location":"api/#batch-processing_1","title":"Batch Processing","text":"<pre><code>import os\nfrom pathlib import Path\nfrom gfpgan import GFPGANer\n\nrestorer = GFPGANer(model_path='GFPGANv1.4.pth', upscale=2)\n\n# Process directory\ninput_dir = Path('input_images')\noutput_dir = Path('restored_images')\noutput_dir.mkdir(exist_ok=True)\n\nfor img_path in input_dir.glob('*.jpg'):\n    img = cv2.imread(str(img_path))\n    _, restored_imgs, _ = restorer.enhance(img)\n\n    output_path = output_dir / f\"restored_{img_path.name}\"\n    cv2.imwrite(str(output_path), restored_imgs[0])\n    print(f\"Processed: {img_path.name}\")\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<pre><code>from gfpgan import GFPGANer\nfrom gfpgan.exceptions import GFPGANError, ModelNotFoundError\n\ntry:\n    restorer = GFPGANer(model_path='invalid_model.pth')\nexcept ModelNotFoundError:\n    print(\"Model not found, downloading...\")\n    # Handle model download\n\ntry:\n    result = restorer.enhance(damaged_image)\nexcept GFPGANError as e:\n    print(f\"Restoration failed: {e}\")\n    # Handle restoration failure\n</code></pre>"},{"location":"api/#integration-examples","title":"Integration Examples","text":""},{"location":"api/#flask-application","title":"Flask Application","text":"<pre><code>from flask import Flask, request, send_file\nfrom gfpgan import GFPGANer\nimport cv2\nimport tempfile\n\napp = Flask(__name__)\nrestorer = GFPGANer(model_path='GFPGANv1.4.pth', upscale=2)\n\n@app.route('/restore', methods=['POST'])\ndef restore_image():\n    file = request.files['image']\n\n    # Save uploaded file\n    with tempfile.NamedTemporaryFile(suffix='.jpg') as tmp_input:\n        file.save(tmp_input.name)\n        img = cv2.imread(tmp_input.name)\n\n        # Restore image\n        _, restored_imgs, _ = restorer.enhance(img)\n\n        # Save result\n        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp_output:\n            cv2.imwrite(tmp_output.name, restored_imgs[0])\n            return send_file(tmp_output.name, as_attachment=True)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"api/#streamlit-application","title":"Streamlit Application","text":"<pre><code>import streamlit as st\nfrom gfpgan import GFPGANer\nimport cv2\nimport numpy as np\n\n@st.cache_resource\ndef load_restorer():\n    return GFPGANer(model_path='GFPGANv1.4.pth', upscale=2)\n\nst.title(\"GFPGAN Face Restoration\")\n\nuploaded_file = st.file_uploader(\"Choose an image\", type=['jpg', 'jpeg', 'png'])\n\nif uploaded_file is not None:\n    # Convert uploaded file to OpenCV format\n    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n    image = cv2.imdecode(file_bytes, 1)\n\n    col1, col2 = st.columns(2)\n\n    with col1:\n        st.header(\"Original\")\n        st.image(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    with col2:\n        st.header(\"Restored\")\n        restorer = load_restorer()\n        _, restored_imgs, _ = restorer.enhance(image)\n        st.image(cv2.cvtColor(restored_imgs[0], cv2.COLOR_BGR2RGB))\n</code></pre>"},{"location":"api/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code># Check GPU availability\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"MPS available: {torch.backends.mps.is_available()}\")\n\n# Use specific device\nrestorer = GFPGANer(\n    model_path='GFPGANv1.4.pth',\n    device='cuda:0'  # or 'cpu', 'mps'\n)\n</code></pre>"},{"location":"api/#memory-management","title":"Memory Management","text":"<pre><code># For processing large images or batches\nrestorer = GFPGANer(\n    model_path='GFPGANv1.4.pth',\n    upscale=1,  # Reduce upscaling to save memory\n    bg_upsampler=None  # Disable background upsampling\n)\n\n# Clear GPU cache after processing\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"api/#error-codes","title":"Error Codes","text":"Code Description Resolution 400 Invalid input format Check image format and size 404 Model not found Verify model path or download 413 Image too large Reduce image size or increase limits 500 Processing failed Check GPU memory and input validity 503 Service overloaded Implement rate limiting or retry <p>Need help? Check our guides or create an issue.</p>"},{"location":"getting-started/install/","title":"Installation","text":""},{"location":"getting-started/install/#quick-install","title":"Quick install","text":"<p>The fastest way to get started:</p> <pre><code>pip install gfpgan\n</code></pre> <p>This installs the core GFPGAN package with basic dependencies.</p>"},{"location":"getting-started/install/#platform-specific-setup","title":"Platform-specific setup","text":""},{"location":"getting-started/install/#windows","title":"Windows","text":"NVIDIA GPUDirectML (AMD/Intel)CPU only <pre><code># Install PyTorch with CUDA support first\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n\n# Install GFPGAN\npip install gfpgan\n</code></pre> <pre><code># Install PyTorch with DirectML support\npip install torch-directml\n\n# Install GFPGAN\npip install gfpgan\n</code></pre> <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\npip install gfpgan\n</code></pre>"},{"location":"getting-started/install/#macos","title":"macOS","text":"Apple Silicon (M1/M2/M3)Intel Mac <pre><code># Metal Performance Shaders (MPS) support included\npip install gfpgan\n</code></pre> <pre><code># CPU-only installation\npip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\npip install gfpgan\n</code></pre>"},{"location":"getting-started/install/#linux","title":"Linux","text":"NVIDIA GPUAMD GPU (ROCm)CPU only <pre><code># Install PyTorch with CUDA support\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n\n# Install GFPGAN\npip install gfpgan\n</code></pre> <pre><code># Install PyTorch with ROCm support\npip install torch torchvision --index-url https://download.pytorch.org/whl/rocm5.6\n\n# Install GFPGAN\npip install gfpgan\n</code></pre> <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\npip install gfpgan\n</code></pre>"},{"location":"getting-started/install/#development-installation","title":"Development installation","text":"<p>For contributing or using the latest features:</p> <pre><code>git clone https://github.com/IAmJonoBo/GFPGAN.git\ncd GFPGAN\npip install -e \".[dev,metrics,web]\"\n</code></pre>"},{"location":"getting-started/install/#optional-extras","title":"Optional extras","text":"<p>Install additional features as needed:</p> <pre><code># Web interface and API\npip install -e \".[web]\"\n\n# Quality metrics\npip install -e \".[metrics]\"\n\n# Development tools\npip install -e \".[dev]\"\n\n# All extras\npip install -e \".[dev,metrics,web]\"\n</code></pre>"},{"location":"getting-started/install/#verify-installation","title":"Verify installation","text":"<p>Test your installation:</p> <pre><code># Check if GFPGAN is installed\ngfpgan-infer --help\n\n# Test with a simple command (dry run)\ngfpgan-infer --input test.jpg --dry-run\n</code></pre>"},{"location":"getting-started/install/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/install/#common-issues","title":"Common issues","text":"<p>CUDA out of memory</p> <p>Try CPU mode or reduce batch size: <pre><code>gfpgan-infer --input photo.jpg --device cpu\n</code></pre></p> <p>ModuleNotFoundError: No module named 'cv2'</p> <p>Install OpenCV: <pre><code>pip install opencv-python\n</code></pre></p> <p>No module named 'basicsr'</p> <p>Install BasicSR: <pre><code>pip install basicsr\n</code></pre></p>"},{"location":"getting-started/install/#getting-help","title":"Getting help","text":"<p>If you encounter issues:</p> <ol> <li>Check our troubleshooting guide</li> <li>Search existing issues</li> <li>Create a new issue with:</li> <li>Your platform (OS, GPU model)</li> <li>Python version (<code>python --version</code>)</li> <li>Error message and full traceback</li> </ol>"},{"location":"getting-started/install/#system-requirements","title":"System requirements","text":"Component Minimum Recommended Python 3.8+ 3.10+ RAM 4GB 8GB+ GPU Memory N/A (CPU) 4GB+ Storage 2GB 5GB+ (for models) <p>Next: Restore your first photo \u2192</p>"},{"location":"governance/contributing/","title":"Contributing to GFPGAN","text":"<p>We welcome contributions to GFPGAN! This guide will help you get started with developing and contributing to the project.</p>"},{"location":"governance/contributing/#getting-started","title":"Getting started","text":""},{"location":"governance/contributing/#development-environment-setup","title":"Development environment setup","text":"<ol> <li>Fork and clone the repository</li> </ol> <pre><code>git clone https://github.com/YOUR_USERNAME/GFPGAN.git\ncd GFPGAN\n</code></pre> <ol> <li>Create a virtual environment</li> </ol> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre> <ol> <li>Install in development mode</li> </ol> <pre><code>pip install -e \".[dev,metrics,web]\"\n</code></pre> <p>This installs:    - dev: Linting, formatting, and testing tools    - metrics: Quality evaluation dependencies    - web: Web interface dependencies</p> <ol> <li>Set up pre-commit hooks</li> </ol> <pre><code>pre-commit install\n</code></pre>"},{"location":"governance/contributing/#development-workflow","title":"Development workflow","text":"<ol> <li>Create a feature branch</li> </ol> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <ol> <li> <p>Make your changes</p> </li> <li> <p>Write clear, well-documented code</p> </li> <li>Follow existing code style and patterns</li> <li> <p>Add tests for new functionality</p> </li> <li> <p>Test your changes</p> </li> </ol> <pre><code># Run linting and formatting\nruff check .\nblack .\n\n# Run tests\npytest tests_light/  # Quick tests\npytest tests/        # Full test suite\n</code></pre> <ol> <li>Commit and push</li> </ol> <pre><code>git add .\ngit commit -m \"feat: add your feature description\"\ngit push origin feature/your-feature-name\n</code></pre> <ol> <li> <p>Create a pull request</p> </li> <li> <p>Use a clear, descriptive title</p> </li> <li>Explain what your changes do and why</li> <li>Reference any related issues</li> <li>Ensure all checks pass</li> </ol>"},{"location":"governance/contributing/#code-style-and-standards","title":"Code style and standards","text":""},{"location":"governance/contributing/#python-code-style","title":"Python code style","text":"<p>We use automated formatting and linting:</p> <ul> <li>Black: Code formatting</li> <li>Ruff: Fast linting and import sorting</li> <li>Type hints: Required for public APIs</li> </ul> <pre><code># Format code\nblack .\n\n# Check linting\nruff check .\n\n# Fix auto-fixable issues\nruff check --fix .\n</code></pre>"},{"location":"governance/contributing/#commit-message-format","title":"Commit message format","text":"<p>We follow Conventional Commits:</p> <pre><code>type(scope): description\n\n[optional body]\n\n[optional footer(s)]\n</code></pre> <p>Types: - <code>feat</code>: New feature - <code>fix</code>: Bug fix - <code>docs</code>: Documentation changes - <code>refactor</code>: Code refactoring - <code>test</code>: Adding or updating tests - <code>chore</code>: Maintenance tasks</p> <p>Examples: <pre><code>feat(cli): add dry-run mode for batch processing\nfix(web): handle missing face detection gracefully\ndocs(guides): update hardware requirements\n</code></pre></p>"},{"location":"governance/contributing/#documentation","title":"Documentation","text":"<p>All user-facing features should include documentation:</p> <ul> <li>API functions: Docstrings with examples</li> <li>CLI commands: Help text and guide updates</li> <li>New features: Usage guides and examples</li> </ul>"},{"location":"governance/contributing/#testing","title":"Testing","text":""},{"location":"governance/contributing/#test-structure","title":"Test structure","text":"<pre><code>tests/                 # Full test suite\n\u251c\u2500\u2500 test_gfpgan_arch.py\n\u251c\u2500\u2500 test_models.py\n\u2514\u2500\u2500 ...\n\ntests_light/           # Quick tests for CI\n\u251c\u2500\u2500 test_basic.py\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"governance/contributing/#running-tests","title":"Running tests","text":"<pre><code># Quick tests (recommended for development)\npytest tests_light/ -v\n\n# Full test suite (for comprehensive validation)\npytest tests/ -v\n\n# Test specific functionality\npytest tests/test_gfpgan_model.py -v\n\n# Test with coverage\npytest tests_light/ --cov=gfpgan --cov-report=html\n</code></pre>"},{"location":"governance/contributing/#writing-tests","title":"Writing tests","text":"<ul> <li>Test both success and failure cases</li> <li>Use clear, descriptive test names</li> <li>Include edge cases and boundary conditions</li> <li>Mock external dependencies when needed</li> </ul> <p>Example: <pre><code>def test_gfpgan_infer_with_valid_image():\n    \"\"\"Test that GFPGAN inference works with valid input image.\"\"\"\n    # Test implementation\n    pass\n\ndef test_gfpgan_infer_with_invalid_format():\n    \"\"\"Test that GFPGAN handles invalid image formats gracefully.\"\"\"\n    # Test implementation\n    pass\n</code></pre></p>"},{"location":"governance/contributing/#pull-request-checklist","title":"Pull request checklist","text":"<p>Before submitting a pull request, ensure:</p> <ul> <li>[ ] Code quality</li> <li>[ ] Code follows style guidelines (Black + Ruff)</li> <li>[ ] All tests pass (<code>pytest tests_light/</code>)</li> <li> <p>[ ] No linting errors (<code>ruff check .</code>)</p> </li> <li> <p>[ ] Documentation</p> </li> <li>[ ] Docstrings added for new functions/classes</li> <li>[ ] User guides updated if needed</li> <li> <p>[ ] CHANGELOG.md updated for user-facing changes</p> </li> <li> <p>[ ] Testing</p> </li> <li>[ ] New tests added for new functionality</li> <li>[ ] Existing tests still pass</li> <li> <p>[ ] Edge cases considered</p> </li> <li> <p>[ ] Compatibility</p> </li> <li>[ ] Changes don't break existing API</li> <li>[ ] Backward compatibility maintained</li> <li>[ ] Cross-platform compatibility verified</li> </ul>"},{"location":"governance/contributing/#development-tasks","title":"Development tasks","text":""},{"location":"governance/contributing/#running-the-documentation-site-locally","title":"Running the documentation site locally","text":"<pre><code># Install docs dependencies\npip install -e \".[docs]\"\n\n# Serve docs locally\nmkdocs serve\n\n# Build static site\nmkdocs build\n</code></pre>"},{"location":"governance/contributing/#building-and-testing-packages","title":"Building and testing packages","text":"<pre><code># Build wheel\npython -m build\n\n# Test installation\npip install dist/gfpgan-*.whl\n</code></pre>"},{"location":"governance/contributing/#benchmarking-changes","title":"Benchmarking changes","text":"<pre><code># Run benchmark suite\npython bench/run_bench.py --input inputs/samples/ --output bench_results/\n\n# Compare with baseline\npython bench/compare_results.py --baseline bench_baseline.json --current bench_results.json\n</code></pre>"},{"location":"governance/contributing/#release-process","title":"Release process","text":"<ol> <li>Update version numbers</li> <li><code>VERSION</code> file</li> <li><code>pyproject.toml</code></li> <li> <p><code>gfpgan/version.py</code></p> </li> <li> <p>Update CHANGELOG.md</p> </li> <li>Move items from \"Unreleased\" to new version section</li> <li> <p>Follow Keep a Changelog format</p> </li> <li> <p>Create release PR</p> </li> <li>Title: \"Release v1.2.3\"</li> <li>Include changelog highlights</li> <li> <p>Ensure all tests pass</p> </li> <li> <p>Tag and release <pre><code>git tag v1.2.3\ngit push origin v1.2.3\n</code></pre></p> </li> <li> <p>Deploy documentation <pre><code>mike deploy --push --update-aliases 1.2.3 latest\n</code></pre></p> </li> </ol>"},{"location":"governance/contributing/#getting-help","title":"Getting help","text":"<ul> <li>Questions: Open a discussion</li> <li>Bugs: Create an issue</li> <li>Features: Start with a discussion, then create an issue</li> <li>Security: See our security policy</li> </ul>"},{"location":"governance/contributing/#code-of-conduct","title":"Code of conduct","text":"<p>Please read and follow our Code of Conduct. We're committed to providing a welcoming and inclusive environment for all contributors.</p> <p>Thank you for contributing to GFPGAN! \ud83c\udf89</p>"},{"location":"governance/maintainers/","title":"Maintainers","text":"<p>This document outlines the maintainer structure, responsibilities, and processes for GFPGAN.</p>"},{"location":"governance/maintainers/#current-maintainers","title":"Current Maintainers","text":""},{"location":"governance/maintainers/#lead-maintainer","title":"Lead Maintainer","text":"<ul> <li>@IAmJonoBo - Project lead, releases, infrastructure</li> </ul>"},{"location":"governance/maintainers/#core-maintainers","title":"Core Maintainers","text":"<ul> <li>@IAmJonoBo - General maintenance, code review, documentation</li> </ul>"},{"location":"governance/maintainers/#area-maintainers","title":"Area Maintainers","text":"Area Maintainer Responsibilities Core Engine @IAmJonoBo Model architectures, inference pipeline Documentation @IAmJonoBo Docs site, guides, API documentation CI/CD @IAmJonoBo Build, test, and deployment automation Web Interface @IAmJonoBo Gradio app, API server"},{"location":"governance/maintainers/#responsibilities","title":"Responsibilities","text":""},{"location":"governance/maintainers/#lead-maintainer_1","title":"Lead Maintainer","text":"<ul> <li>Release management: Planning, coordination, and execution</li> <li>Project direction: Technical roadmap and architecture decisions</li> <li>Community management: Issue triage, contributor onboarding</li> <li>Security: Vulnerability response and security policy enforcement</li> </ul>"},{"location":"governance/maintainers/#core-maintainers_1","title":"Core Maintainers","text":"<ul> <li>Code review: Review and approve pull requests</li> <li>Issue triage: Label, prioritize, and route issues</li> <li>Documentation: Maintain and improve project documentation</li> <li>Testing: Ensure comprehensive test coverage</li> </ul>"},{"location":"governance/maintainers/#area-maintainers_1","title":"Area Maintainers","text":"<ul> <li>Domain expertise: Deep knowledge of specific components</li> <li>Feature development: Lead development in their area</li> <li>Code review: Review PRs affecting their domain</li> <li>Documentation: Maintain area-specific documentation</li> </ul>"},{"location":"governance/maintainers/#processes","title":"Processes","text":""},{"location":"governance/maintainers/#issue-triage","title":"Issue Triage","text":"<p>Issues are triaged using these labels:</p>"},{"location":"governance/maintainers/#priority-labels","title":"Priority Labels","text":"<ul> <li><code>priority/critical</code> - Security issues, data loss, crashes</li> <li><code>priority/high</code> - Significant functionality problems</li> <li><code>priority/medium</code> - General bugs and improvements</li> <li><code>priority/low</code> - Minor enhancements, cleanup</li> </ul>"},{"location":"governance/maintainers/#type-labels","title":"Type Labels","text":"<ul> <li><code>type/bug</code> - Confirmed bugs</li> <li><code>type/feature</code> - New feature requests</li> <li><code>type/enhancement</code> - Improvements to existing features</li> <li><code>type/docs</code> - Documentation improvements</li> <li><code>type/question</code> - Support questions</li> </ul>"},{"location":"governance/maintainers/#area-labels","title":"Area Labels","text":"<ul> <li><code>area/core</code> - Core inference engine</li> <li><code>area/models</code> - Model architectures and weights</li> <li><code>area/cli</code> - Command-line interface</li> <li><code>area/web</code> - Web interface and API</li> <li><code>area/docs</code> - Documentation</li> <li><code>area/ci</code> - Continuous integration</li> </ul>"},{"location":"governance/maintainers/#status-labels","title":"Status Labels","text":"<ul> <li><code>status/needs-info</code> - Waiting for more information</li> <li><code>status/needs-reproduction</code> - Waiting for reproduction steps</li> <li><code>status/blocked</code> - Blocked by external dependencies</li> <li><code>status/wontfix</code> - Will not be fixed (with explanation)</li> </ul>"},{"location":"governance/maintainers/#pull-request-review","title":"Pull Request Review","text":"<p>All PRs require:</p> <ol> <li>Automated checks: All CI checks must pass</li> <li>Code review: At least one maintainer approval</li> <li>Area review: Area maintainer approval for specialized changes</li> <li>Documentation: Updated docs for user-facing changes</li> </ol>"},{"location":"governance/maintainers/#review-guidelines","title":"Review Guidelines","text":"<p>Reviewers should check for:</p> <ul> <li>Functionality: Does the change work as intended?</li> <li>Code quality: Is the code readable and maintainable?</li> <li>Testing: Are there appropriate tests?</li> <li>Documentation: Are user-facing changes documented?</li> <li>Compatibility: Does it maintain backward compatibility?</li> <li>Performance: Does it impact performance significantly?</li> </ul>"},{"location":"governance/maintainers/#release-process","title":"Release Process","text":"<p>Releases follow this process:</p> <ol> <li>Version planning: Determine scope and version number</li> <li>Feature freeze: No new features for patch releases</li> <li>Testing: Run full test suite and manual testing</li> <li>Documentation: Update changelog and version docs</li> <li>Release creation: Tag and create GitHub release</li> <li>Distribution: Deploy to PyPI and update docs site</li> <li>Announcement: Communicate release to community</li> </ol>"},{"location":"governance/maintainers/#becoming-a-maintainer","title":"Becoming a Maintainer","text":"<p>Path to maintainership:</p> <ol> <li>Contribution history: Consistent, quality contributions</li> <li>Community involvement: Help with issues and reviews</li> <li>Domain expertise: Deep knowledge of a specific area</li> <li>Nomination: Current maintainer nomination</li> <li>Consensus: Agreement from lead maintainer</li> </ol>"},{"location":"governance/maintainers/#maintainer-emeritus","title":"Maintainer Emeritus","text":"<p>Former maintainers who have stepped back but contributed significantly:</p> <ul> <li>Recognition in project documentation</li> <li>Optional advisory role for major decisions</li> <li>Credit in release notes and acknowledgments</li> </ul>"},{"location":"governance/maintainers/#decision-making","title":"Decision Making","text":""},{"location":"governance/maintainers/#consensus-model","title":"Consensus Model","text":"<ul> <li>Minor changes: Any maintainer can approve</li> <li>Major changes: Require lead maintainer approval</li> <li>Breaking changes: Require consensus from core maintainers</li> <li>Architecture changes: Require community discussion</li> </ul>"},{"location":"governance/maintainers/#conflict-resolution","title":"Conflict Resolution","text":"<ol> <li>Discussion: Open dialogue in issues or discussions</li> <li>Technical review: Evaluate technical merits</li> <li>Community input: Gather broader community feedback</li> <li>Final decision: Lead maintainer makes final call if needed</li> </ol>"},{"location":"governance/maintainers/#communication","title":"Communication","text":""},{"location":"governance/maintainers/#channels","title":"Channels","text":"<ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>GitHub Discussions: General questions and ideas</li> <li>Pull Requests: Code changes and reviews</li> <li>Email: Security issues and sensitive matters</li> </ul>"},{"location":"governance/maintainers/#response-times","title":"Response Times","text":"<p>Target response times for maintainers:</p> <ul> <li>Security issues: 48 hours</li> <li>Critical bugs: 72 hours</li> <li>General issues: 1 week</li> <li>Pull requests: 1 week</li> <li>Feature requests: 2 weeks</li> </ul>"},{"location":"governance/maintainers/#meetings","title":"Meetings","text":"<ul> <li>Monthly sync: Core maintainer coordination (as needed)</li> <li>Release planning: Before major releases</li> <li>Ad-hoc: For urgent issues or major decisions</li> </ul>"},{"location":"governance/maintainers/#support","title":"Support","text":""},{"location":"governance/maintainers/#community-support","title":"Community Support","text":"<ul> <li>Documentation: Comprehensive guides and API docs</li> <li>Examples: Working code examples and tutorials</li> <li>FAQ: Common questions and solutions</li> <li>Discussions: Community Q&amp;A forum</li> </ul>"},{"location":"governance/maintainers/#professional-support","title":"Professional Support","text":"<p>For organizations requiring professional support:</p> <ul> <li>Consulting: Custom integration and optimization</li> <li>Training: Team training and best practices</li> <li>SLA: Service level agreements for critical applications</li> </ul> <p>Contact: support@gfpgan.ai</p> <p>Want to become a maintainer? Start by contributing and engaging with the community. See our contributing guide for details.</p>"},{"location":"governance/security/","title":"Security Policy","text":""},{"location":"governance/security/#reporting-security-vulnerabilities","title":"Reporting Security Vulnerabilities","text":"<p>We take the security of GFPGAN seriously. If you discover a security vulnerability, please follow these guidelines for responsible disclosure.</p>"},{"location":"governance/security/#private-reporting","title":"Private Reporting","text":"<p>For security issues, please do not create public GitHub issues. Instead, report vulnerabilities privately:</p> <ol> <li>Email: Send details to security@gfpgan.ai</li> <li>GitHub: Use private vulnerability reporting</li> </ol>"},{"location":"governance/security/#what-to-include","title":"What to Include","text":"<p>When reporting a vulnerability, please provide:</p> <ul> <li>Description: Clear explanation of the issue</li> <li>Impact: What an attacker could accomplish</li> <li>Reproduction: Step-by-step instructions to reproduce</li> <li>Environment: Operating system, Python version, GFPGAN version</li> <li>Proof of concept: Code or screenshots (if applicable)</li> </ul>"},{"location":"governance/security/#our-response-process","title":"Our Response Process","text":"<ol> <li>Acknowledgment: We'll confirm receipt within 48 hours</li> <li>Assessment: Initial assessment within 5 business days</li> <li>Updates: Regular status updates every 5 business days</li> <li>Resolution: Coordinated disclosure once fixed</li> </ol>"},{"location":"governance/security/#supported-versions","title":"Supported Versions","text":"<p>We provide security updates for:</p> Version Supported 1.4.x \u2705 Full support 1.3.x \u2705 Security fixes &lt; 1.3 \u274c No support"},{"location":"governance/security/#security-best-practices","title":"Security Best Practices","text":"<p>When using GFPGAN:</p>"},{"location":"governance/security/#input-validation","title":"Input Validation","text":"<ul> <li>Validate file types: Only process trusted image formats</li> <li>Size limits: Implement reasonable file size restrictions</li> <li>Sanitize paths: Validate input/output file paths</li> <li>Network isolation: Run processing in isolated environments</li> </ul> <pre><code># Example: Safe file handling\ndef safe_process_image(file_path):\n    # Validate file extension\n    allowed_extensions = {'.jpg', '.jpeg', '.png', '.webp'}\n    if not any(file_path.lower().endswith(ext) for ext in allowed_extensions):\n        raise ValueError(\"Unsupported file format\")\n\n    # Validate file size (e.g., max 50MB)\n    if os.path.getsize(file_path) &gt; 50 * 1024 * 1024:\n        raise ValueError(\"File too large\")\n\n    # Process with GFPGAN\n    return gfpgan_infer(file_path)\n</code></pre>"},{"location":"governance/security/#api-security","title":"API Security","text":"<ul> <li>Rate limiting: Implement request rate limits</li> <li>Authentication: Secure API endpoints appropriately</li> <li>Input sanitization: Validate all API inputs</li> <li>Output filtering: Don't expose internal paths or errors</li> </ul>"},{"location":"governance/security/#model-security","title":"Model Security","text":"<ul> <li>Verify checksums: Validate model file integrity</li> <li>Trusted sources: Only download models from official sources</li> <li>Isolation: Run inference in sandboxed environments</li> <li>Resource limits: Set memory and computation limits</li> </ul>"},{"location":"governance/security/#known-security-considerations","title":"Known Security Considerations","text":""},{"location":"governance/security/#model-files","title":"Model Files","text":"<ul> <li>GFPGAN uses PyTorch <code>.pth</code> model files</li> <li>These files can contain arbitrary Python code</li> <li>Only use models from trusted sources</li> <li>Verify file checksums when available</li> </ul>"},{"location":"governance/security/#dependencies","title":"Dependencies","text":"<ul> <li>Regular dependency updates via Dependabot</li> <li>Security scanning with CodeQL</li> <li>Vulnerability monitoring for third-party packages</li> </ul>"},{"location":"governance/security/#processing-risks","title":"Processing Risks","text":"<ul> <li>Memory exhaustion: Large images can cause OOM</li> <li>Path traversal: Malicious file paths could access system files</li> <li>Model poisoning: Malicious models could execute arbitrary code</li> </ul>"},{"location":"governance/security/#security-features","title":"Security Features","text":""},{"location":"governance/security/#built-in-protections","title":"Built-in Protections","text":"<ul> <li>Input validation: File format and size checking</li> <li>Error handling: Graceful failure without information leakage</li> <li>Resource management: Memory and GPU memory cleanup</li> <li>Dependency pinning: Locked dependency versions for reproducibility</li> </ul>"},{"location":"governance/security/#optional-security-enhancements","title":"Optional Security Enhancements","text":"<pre><code># Run in restricted container\ndocker run --rm -it --security-opt=no-new-privileges \\\n  --cap-drop=ALL --user=1000:1000 gfpgan:latest\n\n# Process with resource limits\ntimeout 300 gfpgan-infer --input photo.jpg --device cpu\n</code></pre>"},{"location":"governance/security/#disclosure-timeline","title":"Disclosure Timeline","text":"<ul> <li>Day 0: Vulnerability reported privately</li> <li>Day 2: Acknowledgment sent to reporter</li> <li>Day 7: Initial assessment and triage</li> <li>Day 14-30: Fix development and testing</li> <li>Day 30-60: Coordinated disclosure and release</li> <li>Day 60+: Public disclosure (if fix is available)</li> </ul>"},{"location":"governance/security/#security-updates","title":"Security Updates","text":"<p>Security updates are released as:</p> <ol> <li>Patch versions: Critical security fixes (e.g., 1.4.1 \u2192 1.4.2)</li> <li>GitHub Security Advisories: Public disclosure with details</li> <li>CVE assignments: For significant vulnerabilities</li> <li>Release notes: Clear security fix descriptions</li> </ol>"},{"location":"governance/security/#contact-information","title":"Contact Information","text":"<ul> <li>Security Email: security@gfpgan.ai</li> <li>PGP Key: Available at https://gfpgan.ai/.well-known/pgp-key.txt</li> <li>GitHub Security: https://github.com/IAmJonoBo/GFPGAN/security</li> </ul>"},{"location":"governance/security/#acknowledgments","title":"Acknowledgments","text":"<p>We appreciate responsible security researchers and will acknowledge contributions in:</p> <ul> <li>Security advisories</li> <li>Release notes</li> <li>Hall of Fame (with permission)</li> </ul> <p>Questions about security? Contact us at security@gfpgan.ai or review our contributing guidelines.</p>"},{"location":"governance/versioning/","title":"Versioning Policy","text":"<p>GFPGAN follows Semantic Versioning (SemVer) for releases and maintains clear backward compatibility guarantees.</p>"},{"location":"governance/versioning/#version-format","title":"Version Format","text":"<p>Versions follow the format: <code>MAJOR.MINOR.PATCH</code></p> <ul> <li>MAJOR: Breaking changes that require user action</li> <li>MINOR: New features that are backward compatible</li> <li>PATCH: Bug fixes and security updates</li> </ul>"},{"location":"governance/versioning/#examples","title":"Examples","text":"<ul> <li><code>1.4.0</code> \u2192 <code>1.4.1</code>: Patch release (bug fixes)</li> <li><code>1.4.0</code> \u2192 <code>1.5.0</code>: Minor release (new features)</li> <li><code>1.4.0</code> \u2192 <code>2.0.0</code>: Major release (breaking changes)</li> </ul>"},{"location":"governance/versioning/#release-types","title":"Release Types","text":""},{"location":"governance/versioning/#patch-releases-xyz","title":"Patch Releases (x.y.Z)","text":"<p>What's included:</p> <ul> <li>Bug fixes</li> <li>Security updates</li> <li>Documentation improvements</li> <li>Performance optimizations (without API changes)</li> </ul> <p>Backward compatibility: \u2705 Fully compatible</p> <p>Example: <code>1.4.0</code> \u2192 <code>1.4.1</code></p> <pre><code># Safe to upgrade\npip install --upgrade gfpgan\n</code></pre>"},{"location":"governance/versioning/#minor-releases-xyz","title":"Minor Releases (x.Y.z)","text":"<p>What's included:</p> <ul> <li>New features</li> <li>New CLI options</li> <li>New API methods</li> <li>Model improvements</li> <li>Dependency updates (compatible)</li> </ul> <p>Backward compatibility: \u2705 Fully compatible</p> <p>Example: <code>1.4.0</code> \u2192 <code>1.5.0</code></p> <pre><code># Safe to upgrade, new features available\npip install --upgrade gfpgan\n</code></pre>"},{"location":"governance/versioning/#major-releases-xyz","title":"Major Releases (X.y.z)","text":"<p>What's included:</p> <ul> <li>Breaking API changes</li> <li>Removed deprecated features</li> <li>Incompatible dependency updates</li> <li>Major architecture changes</li> </ul> <p>Backward compatibility: \u274c May require code changes</p> <p>Example: <code>1.4.0</code> \u2192 <code>2.0.0</code></p> <pre><code># Review migration guide before upgrading\npip install --upgrade gfpgan\n</code></pre>"},{"location":"governance/versioning/#public-api-definition","title":"Public API Definition","text":"<p>Our public API includes:</p>"},{"location":"governance/versioning/#command-line-interface","title":"Command Line Interface","text":"<pre><code># Stable CLI commands\ngfpgan-infer --input photo.jpg --version 1.4\ngfpgan-doctor\n</code></pre>"},{"location":"governance/versioning/#python-api","title":"Python API","text":"<pre><code># Core classes and functions\nfrom gfpgan import GFPGANer\nfrom gfpgan.utils import restore_image\n\n# Public methods and parameters\nrestorer = GFPGANer(model_path='...', upscale=2)\nresult = restorer.enhance(image, has_aligned=False)\n</code></pre>"},{"location":"governance/versioning/#configuration-files","title":"Configuration Files","text":"<pre><code># Model registry format\nmodels:\n  gfpgan_v1.4:\n    url: \"...\"\n    sha256: \"...\"\n</code></pre>"},{"location":"governance/versioning/#compatibility-guarantees","title":"Compatibility Guarantees","text":""},{"location":"governance/versioning/#within-major-versions","title":"Within Major Versions","text":"<ul> <li>CLI commands: Existing commands continue to work</li> <li>API methods: Public methods maintain signatures</li> <li>Configuration: Existing configs remain valid</li> <li>Model files: Compatible model formats</li> </ul>"},{"location":"governance/versioning/#deprecation-policy","title":"Deprecation Policy","text":"<p>Before removing features:</p> <ol> <li>Deprecation warning: Added in minor release</li> <li>Documentation: Updated with alternatives</li> <li>Migration guide: Provided for complex changes</li> <li>Removal: In next major release (minimum 6 months)</li> </ol> <p>Example deprecation: <pre><code># v1.4.0 - deprecation warning\nwarnings.warn(\"restore_face() is deprecated, use enhance() instead\")\n\n# v2.0.0 - removal\n# restore_face() method removed\n</code></pre></p>"},{"location":"governance/versioning/#release-schedule","title":"Release Schedule","text":""},{"location":"governance/versioning/#regular-releases","title":"Regular Releases","text":"<ul> <li>Patch releases: As needed for critical fixes</li> <li>Minor releases: Every 2-3 months</li> <li>Major releases: Every 12-18 months</li> </ul>"},{"location":"governance/versioning/#security-releases","title":"Security Releases","text":"<ul> <li>Critical vulnerabilities: Within 48 hours</li> <li>High severity: Within 1 week</li> <li>Medium/Low severity: Next regular release</li> </ul>"},{"location":"governance/versioning/#support-windows","title":"Support Windows","text":""},{"location":"governance/versioning/#active-support","title":"Active Support","text":"Version Release Date End of Support Security Fixes 1.4.x 2024-Q4 2025-Q4 \u2705 Yes 1.3.x 2024-Q2 2024-Q4 \u2705 Yes"},{"location":"governance/versioning/#legacy-support","title":"Legacy Support","text":"<ul> <li>Bug fixes: Current major version only</li> <li>Security fixes: Current + previous major version</li> <li>New features: Current major version only</li> </ul>"},{"location":"governance/versioning/#pre-release-versions","title":"Pre-release Versions","text":""},{"location":"governance/versioning/#alpha-releases","title":"Alpha Releases","text":"<ul> <li>Format: <code>1.5.0a1</code>, <code>1.5.0a2</code></li> <li>Purpose: Early feature testing</li> <li>Stability: Unstable, may have breaking changes</li> <li>Availability: GitHub releases only</li> </ul>"},{"location":"governance/versioning/#beta-releases","title":"Beta Releases","text":"<ul> <li>Format: <code>1.5.0b1</code>, <code>1.5.0b2</code></li> <li>Purpose: Feature-complete testing</li> <li>Stability: More stable, API frozen</li> <li>Availability: GitHub releases and PyPI</li> </ul>"},{"location":"governance/versioning/#release-candidates","title":"Release Candidates","text":"<ul> <li>Format: <code>1.5.0rc1</code>, <code>1.5.0rc2</code></li> <li>Purpose: Final testing before release</li> <li>Stability: Production-ready candidate</li> <li>Availability: GitHub releases and PyPI</li> </ul>"},{"location":"governance/versioning/#migration-support","title":"Migration Support","text":""},{"location":"governance/versioning/#migration-guides","title":"Migration Guides","text":"<p>For major releases, we provide:</p> <ul> <li>Step-by-step instructions: How to update code</li> <li>Breaking change summaries: What changed and why</li> <li>Compatibility shims: Temporary compatibility layers</li> <li>Example migrations: Before/after code examples</li> </ul>"},{"location":"governance/versioning/#tools","title":"Tools","text":"<pre><code># Check compatibility with new version\ngfpgan-doctor --check-compatibility 2.0.0\n\n# Automated migration assistance\ngfpgan-migrate --from 1.4 --to 2.0 --scan ./my_project\n</code></pre>"},{"location":"governance/versioning/#version-information","title":"Version Information","text":""},{"location":"governance/versioning/#runtime-version-check","title":"Runtime Version Check","text":"<pre><code>import gfpgan\nprint(gfpgan.__version__)  # \"1.4.1\"\n\n# Programmatic version comparison\nfrom packaging import version\nif version.parse(gfpgan.__version__) &gt;= version.parse(\"1.4.0\"):\n    # Use new features\n    pass\n</code></pre>"},{"location":"governance/versioning/#cli-version-check","title":"CLI Version Check","text":"<pre><code>gfpgan-infer --version\n# GFPGAN 1.4.1\n</code></pre>"},{"location":"governance/versioning/#documentation-versioning","title":"Documentation Versioning","text":""},{"location":"governance/versioning/#docs-site-versioning","title":"Docs Site Versioning","text":"<ul> <li>Latest: Current stable release</li> <li>Development: Main branch (unreleased)</li> <li>Historical: Previous major versions</li> </ul> <p>Access via: <code>https://gfpgan.ai/docs/v1.4/</code></p>"},{"location":"governance/versioning/#api-documentation","title":"API Documentation","text":"<ul> <li>Stable: Generated from release tags</li> <li>Development: Generated from main branch</li> <li>Legacy: Maintained for supported versions</li> </ul>"},{"location":"governance/versioning/#change-communication","title":"Change Communication","text":""},{"location":"governance/versioning/#changelog","title":"Changelog","text":"<p>All releases include detailed changelogs:</p> <ul> <li>Added: New features</li> <li>Changed: Modifications to existing features</li> <li>Deprecated: Features marked for removal</li> <li>Removed: Deleted features</li> <li>Fixed: Bug fixes</li> <li>Security: Security improvements</li> </ul>"},{"location":"governance/versioning/#release-notes","title":"Release Notes","text":"<p>Major releases include:</p> <ul> <li>Upgrade guide: Step-by-step instructions</li> <li>Highlights: Key new features</li> <li>Breaking changes: What requires code changes</li> <li>Performance improvements: Benchmark comparisons</li> </ul>"},{"location":"governance/versioning/#notifications","title":"Notifications","text":"<ul> <li>GitHub releases: Automatic notifications</li> <li>PyPI: Package update notifications</li> <li>Documentation: Version-specific announcements</li> </ul> <p>Questions about versioning? See our FAQ or contributing guide.</p>"},{"location":"guides/batch-processing/","title":"Batch processing","text":"<p>Process entire folders of images with consistent settings and quality tracking.</p>"},{"location":"guides/batch-processing/#quick-start","title":"Quick start","text":"<p>Restore all images in a folder:</p> <pre><code>gfpgan-infer --input photos/ --output results/ --version 1.4\n</code></pre> <p>GFPGAN automatically processes all supported image formats (JPG, PNG, WEBP) in the input folder.</p>"},{"location":"guides/batch-processing/#basic-batch-operations","title":"Basic batch operations","text":""},{"location":"guides/batch-processing/#process-a-folder","title":"Process a folder","text":"Default settingsCustom output folderWith quality metrics <pre><code>gfpgan-infer --input photos/ --version 1.4\n</code></pre> <pre><code>gfpgan-infer --input photos/ --output restored_photos/ --version 1.4\n</code></pre> <pre><code>gfpgan-infer --input photos/ --metrics fast --report-path batch_report.json\n</code></pre>"},{"location":"guides/batch-processing/#supported-file-formats","title":"Supported file formats","text":"<p>GFPGAN processes these image formats: - JPEG/JPG - Most common format - PNG - Lossless format, good for high quality - WEBP - Modern format with good compression - BMP - Uncompressed bitmap - TIFF - High quality, often used professionally</p>"},{"location":"guides/batch-processing/#advanced-batch-processing","title":"Advanced batch processing","text":""},{"location":"guides/batch-processing/#choose-processing-backend","title":"Choose processing backend","text":"Quality-focusedSpeed-focusedBalanced <pre><code># Best quality, slower processing\ngfpgan-infer --input photos/ --backend restoreformer --metrics detailed\n</code></pre> <pre><code># Faster processing, good quality\ngfpgan-infer --input photos/ --backend codeformer --bg_upsampler none\n</code></pre> <pre><code># Good balance of speed and quality\ngfpgan-infer --input photos/ --version 1.4 --metrics fast\n</code></pre>"},{"location":"guides/batch-processing/#batch-with-background-enhancement","title":"Batch with background enhancement","text":"Full enhancementFace-only (faster) <pre><code># Restore faces and enhance backgrounds\ngfpgan-infer --input photos/ --bg_upsampler realesrgan --upscale 2\n</code></pre> <pre><code># Skip background processing for speed\ngfpgan-infer --input photos/ --bg_upsampler none\n</code></pre>"},{"location":"guides/batch-processing/#device-and-memory-management","title":"Device and memory management","text":"Auto device selectionForce CPU (low memory)GPU with memory limits <pre><code>gfpgan-infer --input photos/ --device auto\n</code></pre> <pre><code>gfpgan-infer --input photos/ --device cpu\n</code></pre> <pre><code># Process smaller batches if GPU memory is limited\ngfpgan-infer --input photos/ --device cuda --bg_tile 200\n</code></pre>"},{"location":"guides/batch-processing/#quality-tracking-and-metrics","title":"Quality tracking and metrics","text":""},{"location":"guides/batch-processing/#enable-quality-metrics","title":"Enable quality metrics","text":"Fast metrics (LPIPS only)Detailed metrics (LPIPS, DISTS, ArcFace)No metrics (fastest) <pre><code>gfpgan-infer --input photos/ --metrics fast --report-path quick_report.json\n</code></pre> <pre><code>gfpgan-infer --input photos/ --metrics detailed --report-path full_report.json\n</code></pre> <pre><code>gfpgan-infer --input photos/ --metrics none\n</code></pre>"},{"location":"guides/batch-processing/#understanding-metric-reports","title":"Understanding metric reports","text":"<p>The generated JSON report includes:</p> <pre><code>{\n    \"summary\": {\n        \"total_images\": 150,\n        \"successful\": 147,\n        \"failed\": 3,\n        \"avg_lpips\": 0.234,\n        \"avg_processing_time\": 2.3\n    },\n    \"per_image\": {\n        \"photo001.jpg\": {\n            \"lpips\": 0.198,\n            \"dists\": 0.156,\n            \"arcface_similarity\": 0.892,\n            \"processing_time\": 2.1,\n            \"faces_detected\": 1\n        }\n    }\n}\n</code></pre> <p>Metric meanings: - LPIPS: Lower is better (perceptual similarity) - DISTS: Lower is better (structural similarity) - ArcFace: Higher is better (identity preservation)</p>"},{"location":"guides/batch-processing/#provenance-and-reproducibility","title":"Provenance and reproducibility","text":""},{"location":"guides/batch-processing/#deterministic-processing","title":"Deterministic processing","text":"<p>For reproducible results across runs:</p> <pre><code>gfpgan-infer --input photos/ --deterministic --random-seed 42\n</code></pre>"},{"location":"guides/batch-processing/#preserve-metadata","title":"Preserve metadata","text":"<p>Keep original EXIF data:</p> <pre><code>gfpgan-infer --input photos/ --preserve-metadata\n</code></pre>"},{"location":"guides/batch-processing/#provenance-tracking","title":"Provenance tracking","text":"<p>GFPGAN automatically saves processing information:</p> <pre><code>results/\n\u251c\u2500\u2500 restored_photos/         # Processed images\n\u251c\u2500\u2500 batch_report.json       # Quality metrics\n\u251c\u2500\u2500 processing_log.txt      # Detailed log\n\u2514\u2500\u2500 provenance.json         # Settings and environment info\n</code></pre>"},{"location":"guides/batch-processing/#monitoring-progress","title":"Monitoring progress","text":""},{"location":"guides/batch-processing/#progress-indicators","title":"Progress indicators","text":"Simple progress barDetailed loggingSilent mode <pre><code>gfpgan-infer --input photos/ --progress\n</code></pre> <pre><code>gfpgan-infer --input photos/ --verbose --log-file batch.log\n</code></pre> <pre><code>gfpgan-infer --input photos/ --quiet\n</code></pre>"},{"location":"guides/batch-processing/#handling-interruptions","title":"Handling interruptions","text":"<p>If processing is interrupted:</p> <pre><code># Resume from where it left off\ngfpgan-infer --input photos/ --resume --checkpoint-dir .checkpoints/\n</code></pre>"},{"location":"guides/batch-processing/#presets-and-automation","title":"Presets and automation","text":""},{"location":"guides/batch-processing/#create-processing-presets","title":"Create processing presets","text":"<p>Save commonly used settings:</p> High quality presetFast processing preset <pre><code># Create alias or script\nalias gfpgan-hq='gfpgan-infer --version 1.4 --bg_upsampler realesrgan --metrics detailed'\n\n# Use the preset\ngfpgan-hq --input photos/ --output results/\n</code></pre> <pre><code>alias gfpgan-fast='gfpgan-infer --backend codeformer --bg_upsampler none --metrics fast'\n\ngfpgan-fast --input photos/ --output results/\n</code></pre>"},{"location":"guides/batch-processing/#automation-scripts","title":"Automation scripts","text":"Bash script examplePython script example <pre><code>#!/bin/bash\n# batch_restore.sh\n\nINPUT_DIR=\"$1\"\nOUTPUT_DIR=\"$2\"\n\nif [ -z \"$INPUT_DIR\" ] || [ -z \"$OUTPUT_DIR\" ]; then\n    echo \"Usage: $0 &lt;input_dir&gt; &lt;output_dir&gt;\"\n    exit 1\nfi\n\ngfpgan-infer \\\n    --input \"$INPUT_DIR\" \\\n    --output \"$OUTPUT_DIR\" \\\n    --version 1.4 \\\n    --metrics detailed \\\n    --progress \\\n    --report-path \"$OUTPUT_DIR/quality_report.json\"\n</code></pre> <pre><code>#!/usr/bin/env python3\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef batch_restore(input_dir, output_dir, backend=\"gfpgan\", version=\"1.4\"):\n    cmd = [\n        \"gfpgan-infer\",\n        \"--input\", str(input_dir),\n        \"--output\", str(output_dir),\n        \"--backend\", backend,\n        \"--version\", version,\n        \"--metrics\", \"detailed\",\n        \"--progress\"\n    ]\n\n    subprocess.run(cmd, check=True)\n\nif __name__ == \"__main__\":\n    batch_restore(sys.argv[1], sys.argv[2])\n</code></pre>"},{"location":"guides/batch-processing/#troubleshooting-batch-jobs","title":"Troubleshooting batch jobs","text":""},{"location":"guides/batch-processing/#common-issues","title":"Common issues","text":"<p>Out of memory during batch processing</p> <p>Solutions: <pre><code># Use CPU mode\ngfpgan-infer --input photos/ --device cpu\n\n# Reduce background tile size\ngfpgan-infer --input photos/ --bg_tile 200\n\n# Disable background enhancement\ngfpgan-infer --input photos/ --bg_upsampler none\n</code></pre></p> <p>Some images failed to process</p> <p>Check the processing log: <pre><code>gfpgan-infer --input photos/ --log-file processing.log --verbose\n</code></pre> Common causes: - Corrupted image files - Unsupported format - No faces detected</p> <p>Processing is too slow</p> <p>Speed optimizations: <pre><code># Use faster backend\ngfpgan-infer --input photos/ --backend codeformer\n\n# Skip metrics\ngfpgan-infer --input photos/ --metrics none\n\n# Disable background processing\ngfpgan-infer --input photos/ --bg_upsampler none\n</code></pre></p>"},{"location":"guides/batch-processing/#performance-tips","title":"Performance tips","text":"<ol> <li>Sort by file size: Process smaller images first to get quick feedback</li> <li>Use SSD storage: Faster I/O improves batch processing speed</li> <li>Monitor GPU memory: Use <code>nvidia-smi</code> to watch memory usage</li> <li>Parallel processing: For very large batches, split into chunks</li> </ol> <p>Next steps: - Measure quality with metrics \u2192 - Choose the right backend \u2192 - Optimize hardware performance \u2192</p>"},{"location":"guides/choose-backend/","title":"Choose backend","text":"<p>Compare restoration backends and select the best one for your needs.</p>"},{"location":"guides/choose-backend/#backend-comparison-matrix","title":"Backend comparison matrix","text":"Backend Speed Quality Identity Memory Best for GFPGAN v1.4 Medium High Excellent Medium General photos GFPGAN v1.3 Medium High Good Medium Natural results GFPGAN v1.2 Medium Good Good Medium Sharp details CodeFormer Fast Medium Good Low Batch processing RestoreFormer++ Slow Highest Excellent High Professional work"},{"location":"guides/choose-backend/#detailed-comparisons","title":"Detailed comparisons","text":""},{"location":"guides/choose-backend/#gfpgan-models","title":"GFPGAN models","text":"GFPGAN v1.4 (Recommended)GFPGAN v1.3 (Natural)GFPGAN v1.2 (Sharp) <pre><code>gfpgan-infer --input photo.jpg --version 1.4\n</code></pre> <p>Strengths: - Excellent detail preservation - Good identity preservation - Works well on various photo types - Balanced speed/quality trade-off</p> <p>Best for: - General portrait restoration - Mixed photo collections - Production workflows</p> <p>Example use case: Family photo restoration, professional headshots</p> <pre><code>gfpgan-infer --input photo.jpg --version 1.3\n</code></pre> <p>Strengths: - More natural-looking results - Less artificial sharpening - Good for low-quality inputs - Handles repeated restoration well</p> <p>Weaknesses: - Slightly less sharp than v1.4 - Some identity changes possible</p> <p>Best for: - Natural photo restoration - Social media photos - When subtlety is preferred</p> <p>Example use case: Old family photos, vintage portraits</p> <pre><code>gfpgan-infer --input photo.jpg --version 1.2\n</code></pre> <p>Strengths: - Very sharp output - Good detail enhancement - Suitable for beauty/makeup photos</p> <p>Weaknesses: - Can look unnatural - May over-enhance features</p> <p>Best for: - Beauty photography - When maximum sharpness is needed - Fashion/glamour photos</p>"},{"location":"guides/choose-backend/#alternative-backends","title":"Alternative backends","text":"CodeFormer (Fast)RestoreFormer++ (Premium) <pre><code>gfpgan-infer --input photo.jpg --backend codeformer\n</code></pre> <p>Strengths: - Fastest processing - Lower memory usage - Good identity preservation - Controllable restoration strength</p> <p>Weaknesses: - Lower detail quality than GFPGAN - Less sophisticated texture handling</p> <p>Best for: - Large batch processing - Resource-constrained environments - Quick previews</p> <p>Example use case: Processing thousands of photos, real-time applications</p> <pre><code>gfpgan-infer --input photo.jpg --backend restoreformer\n</code></pre> <p>Strengths: - Highest quality results - Excellent identity preservation - Superior texture reconstruction - Advanced face parsing</p> <p>Weaknesses: - Slowest processing - Highest memory usage - May require more powerful hardware</p> <p>Best for: - Professional photo restoration - High-value images - When quality is paramount</p> <p>Example use case: Archive restoration, professional photography, art restoration</p>"},{"location":"guides/choose-backend/#selection-guidelines","title":"Selection guidelines","text":""},{"location":"guides/choose-backend/#by-use-case","title":"By use case","text":"Personal photosProfessional workBatch processingArchive restoration <p>Recommended: GFPGAN v1.4 <pre><code>gfpgan-infer --input family_photo.jpg --version 1.4\n</code></pre> - Good balance of quality and speed - Preserves natural appearance - Handles various lighting conditions</p> <p>Recommended: RestoreFormer++ <pre><code>gfpgan-infer --input portrait.jpg --backend restoreformer --metrics detailed\n</code></pre> - Highest quality output - Excellent for client work - Detailed quality metrics</p> <p>Recommended: CodeFormer <pre><code>gfpgan-infer --input photos/ --backend codeformer --bg_upsampler none\n</code></pre> - Fastest processing - Lower resource usage - Still produces good results</p> <p>Recommended: GFPGAN v1.3 or RestoreFormer++ <pre><code># For natural results\ngfpgan-infer --input old_photo.jpg --version 1.3\n\n# For maximum quality\ngfpgan-infer --input old_photo.jpg --backend restoreformer\n</code></pre></p>"},{"location":"guides/choose-backend/#by-hardware","title":"By hardware","text":"High-end GPU (8GB+ VRAM)Mid-range GPU (4-8GB VRAM)Low-end GPU (&lt;4GB VRAM)CPU only <ul> <li>\u2705 All backends supported</li> <li>Recommended: RestoreFormer++ for quality</li> <li>Alternative: GFPGAN v1.4 for speed</li> </ul> <pre><code>gfpgan-infer --input photo.jpg --backend restoreformer --upscale 2\n</code></pre> <ul> <li>\u2705 GFPGAN (all versions)</li> <li>\u2705 CodeFormer</li> <li>\u26a0\ufe0f RestoreFormer++ (may need CPU fallback)</li> </ul> <pre><code>gfpgan-infer --input photo.jpg --version 1.4 --bg_tile 400\n</code></pre> <ul> <li>\u2705 CodeFormer</li> <li>\u2705 GFPGAN with reduced settings</li> <li>\u274c RestoreFormer++ (use CPU)</li> </ul> <pre><code>gfpgan-infer --input photo.jpg --backend codeformer --bg_upsampler none\n</code></pre> <ul> <li>\u2705 All backends (slower)</li> <li>Recommended: CodeFormer for speed</li> <li>Disable background enhancement</li> </ul> <pre><code>gfpgan-infer --input photo.jpg --device cpu --backend codeformer\n</code></pre>"},{"location":"guides/choose-backend/#performance-benchmarks","title":"Performance benchmarks","text":""},{"location":"guides/choose-backend/#processing-speed-avg-per-image","title":"Processing speed (avg. per image)","text":"Backend GPU (RTX 3080) CPU (Intel i7) CodeFormer 0.8s 12s GFPGAN v1.4 1.2s 18s GFPGAN v1.3 1.1s 17s RestoreFormer++ 2.4s 45s"},{"location":"guides/choose-backend/#memory-usage","title":"Memory usage","text":"Backend GPU Memory System RAM CodeFormer 2GB 4GB GFPGAN v1.4 3GB 6GB GFPGAN v1.3 3GB 6GB RestoreFormer++ 5GB 8GB <p>Benchmarks based on 512x512 input images with background enhancement</p>"},{"location":"guides/choose-backend/#quality-evaluation","title":"Quality evaluation","text":""},{"location":"guides/choose-backend/#objective-metrics","title":"Objective metrics","text":"<p>Use built-in metrics to compare backends:</p> <pre><code># Test multiple backends on the same image\ngfpgan-infer --input test_photo.jpg --version 1.4 --metrics detailed --output v14/\ngfpgan-infer --input test_photo.jpg --backend codeformer --metrics detailed --output cf/\ngfpgan-infer --input test_photo.jpg --backend restoreformer --metrics detailed --output rf/\n</code></pre> <p>Typical metric ranges: - LPIPS: 0.1-0.4 (lower is better) - DISTS: 0.1-0.3 (lower is better) - ArcFace: 0.7-0.95 (higher is better)</p>"},{"location":"guides/choose-backend/#visual-quality-checklist","title":"Visual quality checklist","text":"<p>When comparing results, look for:</p> <ul> <li>Sharpness: Are facial features well-defined?</li> <li>Naturalness: Does the person look realistic?</li> <li>Identity: Is the person still recognizable?</li> <li>Artifacts: Any unnatural textures or distortions?</li> <li>Consistency: Similar quality across multiple faces?</li> </ul>"},{"location":"guides/choose-backend/#advanced-configuration","title":"Advanced configuration","text":""},{"location":"guides/choose-backend/#backend-specific-parameters","title":"Backend-specific parameters","text":"GFPGAN tuningCodeFormer tuningRestoreFormer++ tuning <pre><code># Adjust upsampling\ngfpgan-infer --input photo.jpg --version 1.4 --upscale 1  # No upsampling\ngfpgan-infer --input photo.jpg --version 1.4 --upscale 4  # 4x upsampling\n\n# Background tile size (affects memory)\ngfpgan-infer --input photo.jpg --version 1.4 --bg_tile 200  # Smaller tiles\n</code></pre> <pre><code># Restoration strength (if supported)\ngfpgan-infer --input photo.jpg --backend codeformer --fidelity 0.8\n\n# Face detection threshold\ngfpgan-infer --input photo.jpg --backend codeformer --detection_threshold 0.5\n</code></pre> <pre><code># High quality mode\ngfpgan-infer --input photo.jpg --backend restoreformer --high_quality\n\n# Memory optimization\ngfpgan-infer --input photo.jpg --backend restoreformer --memory_efficient\n</code></pre>"},{"location":"guides/choose-backend/#decision-flowchart","title":"Decision flowchart","text":"<pre><code>Start\n  \u2193\nQuality most important? \u2192 Yes \u2192 Use RestoreFormer++\n  \u2193 No\nSpeed most important? \u2192 Yes \u2192 Use CodeFormer\n  \u2193 No\nNatural results preferred? \u2192 Yes \u2192 Use GFPGAN v1.3\n  \u2193 No\nGeneral use \u2192 Use GFPGAN v1.4\n</code></pre> <p>Next steps: - Measure quality with metrics \u2192 - Process multiple photos \u2192 - Optimize hardware performance \u2192</p>"},{"location":"guides/metrics/","title":"Quality metrics","text":"<p>Measure restoration quality objectively with built-in metrics and learn how to interpret results.</p>"},{"location":"guides/metrics/#quick-start","title":"Quick start","text":"<p>Generate a quality report for your restored images:</p> <pre><code>gfpgan-infer --input photos/ --metrics detailed --report-path quality_report.json\n</code></pre> <p>This creates a comprehensive JSON report with quality scores for each image.</p>"},{"location":"guides/metrics/#available-metrics","title":"Available metrics","text":""},{"location":"guides/metrics/#lpips-learned-perceptual-image-patch-similarity","title":"LPIPS (Learned Perceptual Image Patch Similarity)","text":"<p>What it measures: Perceptual similarity between original and restored images</p> <ul> <li>Range: 0.0 to 1.0</li> <li>Lower is better (0.0 = identical, 1.0 = very different)</li> <li>Typical good range: 0.1 to 0.3</li> </ul> <pre><code>gfpgan-infer --input photo.jpg --metrics lpips\n</code></pre> <p>Interpretation: - <code>&lt; 0.2</code>: Excellent perceptual quality - <code>0.2 - 0.3</code>: Good quality - <code>0.3 - 0.4</code>: Acceptable quality - <code>&gt; 0.4</code>: Poor quality</p>"},{"location":"guides/metrics/#dists-deep-image-structure-and-texture-similarity","title":"DISTS (Deep Image Structure and Texture Similarity)","text":"<p>What it measures: Structural and texture similarity</p> <ul> <li>Range: 0.0 to 1.0</li> <li>Lower is better</li> <li>Typical good range: 0.1 to 0.25</li> </ul> <pre><code>gfpgan-infer --input photo.jpg --metrics dists\n</code></pre> <p>Interpretation: - <code>&lt; 0.15</code>: Excellent structural preservation - <code>0.15 - 0.25</code>: Good structural quality - <code>0.25 - 0.35</code>: Acceptable quality - <code>&gt; 0.35</code>: Poor structural preservation</p>"},{"location":"guides/metrics/#arcface-identity-similarity","title":"ArcFace Identity Similarity","text":"<p>What it measures: How well facial identity is preserved</p> <ul> <li>Range: 0.0 to 1.0</li> <li>Higher is better (1.0 = perfect identity match)</li> <li>Typical good range: 0.7 to 0.95</li> </ul> <pre><code>gfpgan-infer --input photo.jpg --metrics arcface\n</code></pre> <p>Interpretation: - <code>&gt; 0.9</code>: Excellent identity preservation - <code>0.8 - 0.9</code>: Good identity preservation - <code>0.7 - 0.8</code>: Acceptable identity preservation - <code>&lt; 0.7</code>: Poor identity preservation</p>"},{"location":"guides/metrics/#metric-presets","title":"Metric presets","text":""},{"location":"guides/metrics/#fast-metrics-recommended-for-batch","title":"Fast metrics (recommended for batch)","text":"<pre><code>gfpgan-infer --input photos/ --metrics fast\n</code></pre> <p>Includes: LPIPS only Processing time: ~10% overhead Best for: Large batches, quick quality checks</p>"},{"location":"guides/metrics/#detailed-metrics-comprehensive","title":"Detailed metrics (comprehensive)","text":"<pre><code>gfpgan-infer --input photos/ --metrics detailed\n</code></pre> <p>Includes: LPIPS, DISTS, ArcFace, processing time Processing time: ~30% overhead Best for: Quality analysis, backend comparison, research</p>"},{"location":"guides/metrics/#no-metrics-fastest","title":"No metrics (fastest)","text":"<pre><code>gfpgan-infer --input photos/ --metrics none\n</code></pre> <p>Includes: Processing time only Best for: Production workflows where speed matters most</p>"},{"location":"guides/metrics/#understanding-reports","title":"Understanding reports","text":""},{"location":"guides/metrics/#json-report-structure","title":"JSON report structure","text":"<pre><code>{\n    \"summary\": {\n        \"total_images\": 50,\n        \"successful\": 48,\n        \"failed\": 2,\n        \"avg_lpips\": 0.187,\n        \"avg_dists\": 0.142,\n        \"avg_arcface\": 0.863,\n        \"avg_processing_time\": 2.34,\n        \"backend\": \"gfpgan_v1.4\",\n        \"timestamp\": \"2024-01-15T10:30:00Z\"\n    },\n    \"per_image\": {\n        \"family_photo_001.jpg\": {\n            \"lpips\": 0.156,\n            \"dists\": 0.128,\n            \"arcface_similarity\": 0.891,\n            \"processing_time\": 2.1,\n            \"faces_detected\": 2,\n            \"status\": \"success\"\n        },\n        \"portrait_002.jpg\": {\n            \"lpips\": 0.203,\n            \"dists\": 0.167,\n            \"arcface_similarity\": 0.834,\n            \"processing_time\": 2.6,\n            \"faces_detected\": 1,\n            \"status\": \"success\"\n        }\n    },\n    \"failed_images\": {\n        \"corrupted_image.jpg\": {\n            \"error\": \"No faces detected\",\n            \"status\": \"failed\"\n        }\n    }\n}\n</code></pre>"},{"location":"guides/metrics/#key-report-sections","title":"Key report sections","text":"Summary statisticsPer-image detailsFailure analysis <ul> <li>Total/successful/failed counts</li> <li>Average metric scores</li> <li>Overall processing performance</li> <li>Backend and settings used</li> </ul> <ul> <li>Individual quality scores</li> <li>Number of faces detected</li> <li>Processing time per image</li> <li>Success/failure status</li> </ul> <ul> <li>Failed image list</li> <li>Error reasons</li> <li>Troubleshooting hints</li> </ul>"},{"location":"guides/metrics/#comparing-backends","title":"Comparing backends","text":""},{"location":"guides/metrics/#benchmark-multiple-backends","title":"Benchmark multiple backends","text":"<pre><code># Test GFPGAN v1.4\ngfpgan-infer --input test_photos/ --version 1.4 --metrics detailed --output results_v14/ --report-path v14_report.json\n\n# Test CodeFormer\ngfpgan-infer --input test_photos/ --backend codeformer --metrics detailed --output results_cf/ --report-path cf_report.json\n\n# Test RestoreFormer++\ngfpgan-infer --input test_photos/ --backend restoreformer --metrics detailed --output results_rf/ --report-path rf_report.json\n</code></pre>"},{"location":"guides/metrics/#analyze-backend-performance","title":"Analyze backend performance","text":"<p>Create a comparison script:</p> <pre><code>import json\nimport pandas as pd\n\ndef compare_backends(report_files, backend_names):\n    results = []\n\n    for report_file, backend in zip(report_files, backend_names):\n        with open(report_file) as f:\n            data = json.load(f)\n\n        results.append({\n            'backend': backend,\n            'avg_lpips': data['summary']['avg_lpips'],\n            'avg_dists': data['summary']['avg_dists'],\n            'avg_arcface': data['summary']['avg_arcface'],\n            'avg_time': data['summary']['avg_processing_time'],\n            'success_rate': data['summary']['successful'] / data['summary']['total_images']\n        })\n\n    df = pd.DataFrame(results)\n    print(df.to_string(index=False))\n\n# Usage\ncompare_backends(\n    ['v14_report.json', 'cf_report.json', 'rf_report.json'],\n    ['GFPGAN v1.4', 'CodeFormer', 'RestoreFormer++']\n)\n</code></pre>"},{"location":"guides/metrics/#quality-thresholds-and-recommendations","title":"Quality thresholds and recommendations","text":""},{"location":"guides/metrics/#production-quality-gates","title":"Production quality gates","text":"<p>Set quality thresholds for automated workflows:</p> <pre><code>def quality_check(report_path, min_lpips=0.3, min_arcface=0.7):\n    with open(report_path) as f:\n        data = json.load(f)\n\n    passed = []\n    failed = []\n\n    for filename, metrics in data['per_image'].items():\n        if (metrics['lpips'] &lt;= min_lpips and\n            metrics['arcface_similarity'] &gt;= min_arcface):\n            passed.append(filename)\n        else:\n            failed.append(filename)\n\n    return passed, failed\n</code></pre>"},{"location":"guides/metrics/#recommended-thresholds-by-use-case","title":"Recommended thresholds by use case","text":"Professional/CommercialSocial Media/PersonalArchive/Historical <ul> <li>LPIPS: &lt; 0.2</li> <li>DISTS: &lt; 0.15</li> <li>ArcFace: &gt; 0.85</li> </ul> <pre><code>gfpgan-infer --input photos/ --backend restoreformer --metrics detailed\n</code></pre> <ul> <li>LPIPS: &lt; 0.3</li> <li>DISTS: &lt; 0.25</li> <li>ArcFace: &gt; 0.75</li> </ul> <pre><code>gfpgan-infer --input photos/ --version 1.4 --metrics fast\n</code></pre> <ul> <li>LPIPS: &lt; 0.4</li> <li>DISTS: &lt; 0.35</li> <li>ArcFace: &gt; 0.65</li> </ul> <pre><code>gfpgan-infer --input photos/ --version 1.3 --metrics detailed\n</code></pre>"},{"location":"guides/metrics/#advanced-metrics-analysis","title":"Advanced metrics analysis","text":""},{"location":"guides/metrics/#statistical-analysis","title":"Statistical analysis","text":"<pre><code>import json\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analyze_metrics(report_path):\n    with open(report_path) as f:\n        data = json.load(f)\n\n    lpips_scores = [img['lpips'] for img in data['per_image'].values()]\n    arcface_scores = [img['arcface_similarity'] for img in data['per_image'].values()]\n\n    print(f\"LPIPS - Mean: {np.mean(lpips_scores):.3f}, Std: {np.std(lpips_scores):.3f}\")\n    print(f\"ArcFace - Mean: {np.mean(arcface_scores):.3f}, Std: {np.std(arcface_scores):.3f}\")\n\n    # Plot distributions\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    ax1.hist(lpips_scores, bins=20, alpha=0.7)\n    ax1.set_xlabel('LPIPS Score')\n    ax1.set_ylabel('Frequency')\n    ax1.set_title('LPIPS Distribution')\n\n    ax2.hist(arcface_scores, bins=20, alpha=0.7)\n    ax2.set_xlabel('ArcFace Similarity')\n    ax2.set_ylabel('Frequency')\n    ax2.set_title('Identity Preservation Distribution')\n\n    plt.tight_layout()\n    plt.savefig('metrics_analysis.png')\n</code></pre>"},{"location":"guides/metrics/#correlation-analysis","title":"Correlation analysis","text":"<pre><code>def correlation_analysis(report_path):\n    with open(report_path) as f:\n        data = json.load(f)\n\n    metrics_data = []\n    for img_data in data['per_image'].values():\n        metrics_data.append([\n            img_data['lpips'],\n            img_data['dists'],\n            img_data['arcface_similarity'],\n            img_data['processing_time'],\n            img_data['faces_detected']\n        ])\n\n    df = pd.DataFrame(metrics_data,\n                     columns=['LPIPS', 'DISTS', 'ArcFace', 'Time', 'Faces'])\n\n    correlation_matrix = df.corr()\n    print(correlation_matrix)\n</code></pre>"},{"location":"guides/metrics/#troubleshooting-metrics","title":"Troubleshooting metrics","text":""},{"location":"guides/metrics/#common-issues","title":"Common issues","text":"<p>Metrics computation failed</p> <p><pre><code>Error: Failed to compute ArcFace similarity\n</code></pre> Solutions: - Install required dependencies: <code>pip install -e \".[metrics]\"</code> - Check if faces were detected in the image - Try with <code>--metrics fast</code> (LPIPS only)</p> <p>Unexpected metric values</p> <p>Very high LPIPS (&gt; 0.5): - Check if input/output images are properly aligned - Verify the restoration actually improved the image - Consider trying a different backend</p> <p>Very low ArcFace (&lt; 0.5): - Face detection might have failed - Original image quality might be too poor - Identity may have been significantly altered</p> <p>Performance considerations</p> <p>Metrics slow down processing: - Use <code>--metrics fast</code> for large batches - Consider <code>--metrics none</code> for production workflows - Process metrics separately on a subset for quality assessment</p> <p>Next steps: - Choose the right backend \u2192 - Optimize hardware performance \u2192 - Set up batch processing \u2192</p>"},{"location":"guides/restore-a-photo/","title":"Restore a photo","text":"<p>Learn how to restore a single image using GFPGAN's CLI and web interface.</p>"},{"location":"guides/restore-a-photo/#quick-start","title":"Quick start","text":"<p>For a damaged or blurry photo, restoration takes just one command:</p> <pre><code>gfpgan-infer --input damaged_photo.jpg --version 1.4\n</code></pre> <p>Results are saved to <code>results/</code> with before/after comparison images.</p>"},{"location":"guides/restore-a-photo/#cli-workflow","title":"CLI workflow","text":""},{"location":"guides/restore-a-photo/#basic-restoration","title":"Basic restoration","text":"Single imageCustom output locationUpscale while restoring <pre><code>gfpgan-infer --input photo.jpg --version 1.4\n</code></pre> <pre><code>gfpgan-infer --input photo.jpg --output restored/ --version 1.4\n</code></pre> <pre><code>gfpgan-infer --input photo.jpg --version 1.4 --upscale 2\n</code></pre>"},{"location":"guides/restore-a-photo/#choose-your-backend","title":"Choose your backend","text":"<p>Different backends offer different trade-offs:</p> GFPGAN v1.4 (recommended)GFPGAN v1.3 (natural)CodeFormer (fast) <p><pre><code>gfpgan-infer --input photo.jpg --version 1.4\n</code></pre> - Best for: General photos and portraits - Quality: High detail preservation - Speed: Medium</p> <p><pre><code>gfpgan-infer --input photo.jpg --version 1.3\n</code></pre> - Best for: Natural-looking results - Quality: Good, less sharp than v1.4 - Speed: Medium</p> <p><pre><code>gfpgan-infer --input photo.jpg --backend codeformer\n</code></pre> - Best for: Batch processing - Quality: Good - Speed: Fast</p>"},{"location":"guides/restore-a-photo/#advanced-options","title":"Advanced options","text":"Background enhancementFace detection optionsDevice selection <pre><code># Enhance background with Real-ESRGAN\ngfpgan-infer --input photo.jpg --bg_upsampler realesrgan\n\n# Disable background enhancement for speed\ngfpgan-infer --input photo.jpg --bg_upsampler none\n</code></pre> <pre><code># Only restore the center face\ngfpgan-infer --input photo.jpg --only_center_face\n\n# Input is already an aligned face crop\ngfpgan-infer --input face_crop.jpg --aligned\n</code></pre> <pre><code># Force CPU (if GPU has issues)\ngfpgan-infer --input photo.jpg --device cpu\n\n# Auto-detect best device\ngfpgan-infer --input photo.jpg --device auto\n</code></pre>"},{"location":"guides/restore-a-photo/#web-interface","title":"Web interface","text":"<p>Launch the interactive web UI for drag-and-drop restoration:</p> <pre><code>python -m gfpgan.gradio_app\n</code></pre> <p>Then open http://localhost:7860 in your browser.</p>"},{"location":"guides/restore-a-photo/#web-interface-features","title":"Web interface features","text":"<ul> <li>Drag and drop: Upload images directly</li> <li>Real-time preview: See results before saving</li> <li>Parameter adjustment: Change models and settings interactively</li> <li>Download options: Get individual results or batch ZIP</li> </ul>"},{"location":"guides/restore-a-photo/#custom-web-server","title":"Custom web server","text":"<p>For more control, use the FastAPI server:</p> <pre><code>uvicorn services.api.main:app --reload --host 0.0.0.0 --port 8000\n</code></pre> <p>API documentation available at: - Interactive docs: http://localhost:8000/docs - ReDoc format: http://localhost:8000/redoc</p>"},{"location":"guides/restore-a-photo/#understanding-results","title":"Understanding results","text":""},{"location":"guides/restore-a-photo/#output-structure","title":"Output structure","text":"<p>After running restoration, you'll find:</p> <pre><code>results/\n\u251c\u2500\u2500 restored_faces/           # Individual face crops (restored)\n\u251c\u2500\u2500 cropped_faces/           # Original face crops (input)\n\u251c\u2500\u2500 cmp_photo.jpg           # Before/after comparison\n\u2514\u2500\u2500 photo.jpg               # Full restored image\n</code></pre>"},{"location":"guides/restore-a-photo/#quality-assessment","title":"Quality assessment","text":"<p>Visual indicators to check: - Sharpness: Are facial features well-defined? - Identity preservation: Does the person still look like themselves? - Artifacts: Any unnatural textures or distortions? - Background: Is the non-face area properly enhanced?</p>"},{"location":"guides/restore-a-photo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/restore-a-photo/#common-issues","title":"Common issues","text":"<p>No faces detected</p> <p><pre><code>Warning: No faces detected in the input image.\n</code></pre> Solutions: - Ensure the image contains visible faces - Try a different face detection threshold - Check if the image is too small or low quality</p> <p>CUDA out of memory</p> <p><pre><code>RuntimeError: CUDA out of memory\n</code></pre> Solutions: <pre><code># Use CPU instead\ngfpgan-infer --input photo.jpg --device cpu\n\n# Reduce image size first\ngfpgan-infer --input photo.jpg --upscale 1\n</code></pre></p> <p>Poor restoration quality</p> <p>Try different backends: <pre><code># More natural results\ngfpgan-infer --input photo.jpg --version 1.3\n\n# Better identity preservation\ngfpgan-infer --input photo.jpg --backend restoreformer\n</code></pre></p>"},{"location":"guides/restore-a-photo/#getting-help","title":"Getting help","text":"<p>If restoration quality isn't satisfactory:</p> <ol> <li>Check our backend comparison guide</li> <li>Review quality metrics for objective evaluation</li> <li>See troubleshooting for technical issues</li> </ol> <p>Next steps: - Process multiple photos \u2192 - Measure restoration quality \u2192 - Choose the right backend \u2192</p>"},{"location":"product/data-card/","title":"Data Card: GFPGAN Training Datasets","text":""},{"location":"product/data-card/#dataset-overview","title":"Dataset Overview","text":"<p>This data card documents the datasets used for training and evaluating GFPGAN models, following best practices for dataset transparency and responsible AI development.</p> <p>\u26a0\ufe0f Important: This data card is under development. Many sections require additional research and documentation from the original model training process.</p>"},{"location":"product/data-card/#dataset-summary","title":"Dataset Summary","text":""},{"location":"product/data-card/#primary-training-data","title":"Primary Training Data","text":"<p>\u26a0\ufe0f TODO: Comprehensive training data documentation needed</p> Component Description Status Source datasets [NEEDS DOCUMENTATION] TODO Total images [NEEDS DOCUMENTATION] TODO Face count [NEEDS DOCUMENTATION] TODO Resolution range [NEEDS DOCUMENTATION] TODO Collection period [NEEDS DOCUMENTATION] TODO"},{"location":"product/data-card/#evaluation-datasets","title":"Evaluation Datasets","text":"Dataset Size Purpose License Availability CelebA-HQ 30,000 images High-quality evaluation Custom Research only FFHQ 70,000 images Face generation benchmark CC BY-NC-SA 4.0 Public Helen 2,330 images Facial landmark evaluation Academic Research only LFW 13,233 images Identity verification Public Domain Public"},{"location":"product/data-card/#data-collection-and-processing","title":"Data Collection and Processing","text":""},{"location":"product/data-card/#collection-methodology","title":"Collection Methodology","text":"<p>\u26a0\ufe0f TODO: Original data collection process needs documentation</p>"},{"location":"product/data-card/#data-sources","title":"Data Sources","text":"<ul> <li>Web scraping: [DETAILS NEEDED]</li> <li>Public datasets: [LIST NEEDED]</li> <li>Synthetic generation: [METHODS NEEDED]</li> <li>User contributions: [POLICIES NEEDED]</li> </ul>"},{"location":"product/data-card/#collection-criteria","title":"Collection Criteria","text":"<ul> <li>Face visibility: [CRITERIA NEEDED]</li> <li>Image quality: [STANDARDS NEEDED]</li> <li>Resolution requirements: [MINIMUMS NEEDED]</li> <li>Demographic diversity: [TARGETS NEEDED]</li> </ul>"},{"location":"product/data-card/#data-processing-pipeline","title":"Data Processing Pipeline","text":""},{"location":"product/data-card/#preprocessing-steps","title":"Preprocessing Steps","text":"<ol> <li>Face detection: MTCNN or RetinaFace detection</li> <li>Alignment: Facial landmark-based alignment</li> <li>Cropping: Center crop to facial region</li> <li>Resizing: Standardize to 512x512 resolution</li> <li>Quality filtering: Remove low-quality samples</li> </ol>"},{"location":"product/data-card/#data-augmentation","title":"Data Augmentation","text":"<ul> <li>Geometric transforms: Rotation, scaling, flipping</li> <li>Color adjustments: Brightness, contrast, saturation</li> <li>Degradation simulation: Blur, noise, compression</li> <li>Occlusion: Partial face masking</li> </ul>"},{"location":"product/data-card/#quality-control","title":"Quality Control","text":"<ul> <li>Manual review: [PROCESS NEEDED]</li> <li>Automated filtering: [CRITERIA NEEDED]</li> <li>Duplicate detection: [METHODS NEEDED]</li> <li>Privacy screening: [PROTOCOLS NEEDED]</li> </ul>"},{"location":"product/data-card/#dataset-composition","title":"Dataset Composition","text":""},{"location":"product/data-card/#demographic-distribution","title":"Demographic Distribution","text":"<p>\u26a0\ufe0f TODO: Comprehensive demographic analysis needed</p>"},{"location":"product/data-card/#age-groups","title":"Age Groups","text":"<ul> <li>Children (0-17): [PERCENTAGE NEEDED]</li> <li>Young adults (18-35): [PERCENTAGE NEEDED]</li> <li>Middle-aged (36-55): [PERCENTAGE NEEDED]</li> <li>Older adults (55+): [PERCENTAGE NEEDED]</li> </ul>"},{"location":"product/data-card/#gender-representation","title":"Gender Representation","text":"<ul> <li>Male: [PERCENTAGE NEEDED]</li> <li>Female: [PERCENTAGE NEEDED]</li> <li>Non-binary/Other: [PERCENTAGE NEEDED]</li> <li>Not specified: [PERCENTAGE NEEDED]</li> </ul>"},{"location":"product/data-card/#ethnic-and-racial-diversity","title":"Ethnic and Racial Diversity","text":"<ul> <li>White/Caucasian: [PERCENTAGE NEEDED]</li> <li>Black/African American: [PERCENTAGE NEEDED]</li> <li>Asian: [PERCENTAGE NEEDED]</li> <li>Hispanic/Latino: [PERCENTAGE NEEDED]</li> <li>Middle Eastern: [PERCENTAGE NEEDED]</li> <li>Mixed/Other: [PERCENTAGE NEEDED]</li> </ul>"},{"location":"product/data-card/#geographic-distribution","title":"Geographic Distribution","text":"<ul> <li>North America: [PERCENTAGE NEEDED]</li> <li>Europe: [PERCENTAGE NEEDED]</li> <li>Asia: [PERCENTAGE NEEDED]</li> <li>Other regions: [PERCENTAGE NEEDED]</li> </ul>"},{"location":"product/data-card/#technical-characteristics","title":"Technical Characteristics","text":""},{"location":"product/data-card/#image-properties","title":"Image Properties","text":"<ul> <li>Resolution distribution: 128x128 to 1024x1024 (standardized to 512x512)</li> <li>Color space: RGB, sRGB color profile</li> <li>File formats: JPEG, PNG (converted to PNG for training)</li> <li>Compression levels: Various (original quality preserved)</li> </ul>"},{"location":"product/data-card/#face-characteristics","title":"Face Characteristics","text":"<ul> <li>Face size range: 64x64 to 512x512 pixels</li> <li>Pose variation: Primarily frontal (\u00b130 degrees)</li> <li>Expression variety: Neutral to moderate expressions</li> <li>Lighting conditions: Various natural and artificial lighting</li> </ul>"},{"location":"product/data-card/#privacy-ethics","title":"Privacy &amp; Ethics","text":"<p>For information about privacy practices and ethical considerations:</p> <ul> <li>Data handling practices: See our security policy</li> <li>Responsible AI principles: See our contributing guidelines</li> </ul>"},{"location":"product/data-card/#known-limitations-and-biases","title":"Known Limitations and Biases","text":""},{"location":"product/data-card/#identified-biases","title":"Identified Biases","text":"<p>\u26a0\ufe0f TODO: Comprehensive bias analysis needed</p>"},{"location":"product/data-card/#demographic-biases","title":"Demographic Biases","text":"<ul> <li>Age bias: [ANALYSIS NEEDED]</li> <li>Gender bias: [ANALYSIS NEEDED]</li> <li>Racial bias: [ANALYSIS NEEDED]</li> <li>Geographic bias: [ANALYSIS NEEDED]</li> </ul>"},{"location":"product/data-card/#quality-biases","title":"Quality Biases","text":"<ul> <li>High-quality overrepresentation: Professional photos vs. amateur</li> <li>Pose bias: Frontal faces overrepresented</li> <li>Expression bias: Neutral expressions dominant</li> <li>Lighting bias: Well-lit faces overrepresented</li> </ul>"},{"location":"product/data-card/#impact-assessment","title":"Impact Assessment","text":""},{"location":"product/data-card/#model-performance-impact","title":"Model Performance Impact","text":"<ul> <li>Demographic performance gaps: [MEASUREMENT NEEDED]</li> <li>Quality variations: [ASSESSMENT NEEDED]</li> <li>Use case limitations: [DOCUMENTATION NEEDED]</li> <li>Fairness implications: [ANALYSIS NEEDED]</li> </ul>"},{"location":"product/data-card/#mitigation-strategies","title":"Mitigation Strategies","text":"<ul> <li>Diverse evaluation: Multi-demographic test sets</li> <li>Bias monitoring: Regular fairness assessments</li> <li>Data augmentation: Synthetic diversity enhancement</li> <li>Community feedback: Bias reporting mechanisms</li> </ul>"},{"location":"product/data-card/#data-governance","title":"Data Governance","text":""},{"location":"product/data-card/#data-management","title":"Data Management","text":""},{"location":"product/data-card/#storage-and-security","title":"Storage and Security","text":"<ul> <li>Data encryption: At rest and in transit</li> <li>Access controls: Role-based permissions</li> <li>Audit logging: Access and modification tracking</li> <li>Backup procedures: Secure, versioned backups</li> </ul>"},{"location":"product/data-card/#version-control","title":"Version Control","text":"<ul> <li>Dataset versioning: Semantic versioning system</li> <li>Change tracking: Detailed modification logs</li> <li>Reproducibility: Exact dataset recreation capability</li> <li>Documentation: Comprehensive change documentation</li> </ul>"},{"location":"product/data-card/#update-and-maintenance","title":"Update and Maintenance","text":""},{"location":"product/data-card/#regular-updates","title":"Regular Updates","text":"<ul> <li>Bias assessment: Quarterly fairness reviews</li> <li>Quality improvement: Ongoing data curation</li> <li>Coverage expansion: Demographic gap filling</li> <li>Privacy compliance: Regular policy alignment</li> </ul>"},{"location":"product/data-card/#community-involvement","title":"Community Involvement","text":"<ul> <li>Feedback collection: User experience reports</li> <li>Bias reporting: Community bias identification</li> <li>Data contributions: Voluntary diverse data sharing</li> <li>Advisory input: External expert consultation</li> </ul>"},{"location":"product/data-card/#usage-guidelines","title":"Usage Guidelines","text":""},{"location":"product/data-card/#appropriate-uses","title":"Appropriate Uses","text":"<p>\u2705 Recommended applications: - Research: Academic computer vision research - Development: Face restoration algorithm improvement - Evaluation: Model performance benchmarking - Education: Learning about face processing techniques</p>"},{"location":"product/data-card/#restricted-uses","title":"Restricted Uses","text":"<p>\u274c Inappropriate applications: - Surveillance: Identification or tracking individuals - Discrimination: Biased decision-making systems - Commercial exploitation: Unauthorized commercial use - Privacy violation: Processing without consent</p>"},{"location":"product/data-card/#best-practices","title":"Best Practices","text":""},{"location":"product/data-card/#responsible-usage","title":"Responsible Usage","text":"<ol> <li>Bias awareness: Understand and account for dataset limitations</li> <li>Evaluation diversity: Test on diverse populations</li> <li>Transparency: Document data usage in publications</li> <li>Privacy respect: Honor original consent and restrictions</li> </ol>"},{"location":"product/data-card/#technical-recommendations","title":"Technical Recommendations","text":"<ol> <li>Subset selection: Use representative subsets for evaluation</li> <li>Augmentation: Apply appropriate data augmentation</li> <li>Validation: Cross-validate on multiple datasets</li> <li>Documentation: Maintain detailed usage records</li> </ol>"},{"location":"product/data-card/#contact-and-reporting","title":"Contact and Reporting","text":""},{"location":"product/data-card/#data-issues","title":"Data Issues","text":"<ul> <li>Dataset errors: data-issues@gfpgan.ai</li> <li>Privacy concerns: privacy@gfpgan.ai</li> <li>Bias reports: bias@gfpgan.ai</li> <li>General questions: GitHub Discussions</li> </ul>"},{"location":"product/data-card/#contributing","title":"Contributing","text":""},{"location":"product/data-card/#data-contributions","title":"Data Contributions","text":"<ul> <li>Diverse datasets: Help improve demographic coverage</li> <li>Quality assessment: Manual data quality evaluation</li> <li>Bias analysis: Demographic bias identification</li> <li>Documentation: Improve data card completeness</li> </ul>"},{"location":"product/data-card/#review-process","title":"Review Process","text":"<ol> <li>Community review: Public feedback on data practices</li> <li>Expert consultation: External bias and ethics review</li> <li>Regular updates: Quarterly data card revisions</li> <li>Transparency reports: Annual data governance summaries</li> </ol>"},{"location":"product/data-card/#references-and-standards","title":"References and Standards","text":""},{"location":"product/data-card/#standards-compliance","title":"Standards Compliance","text":"<ul> <li>ISO/IEC 23053: Framework for AI risk management</li> <li>NIST AI RMF: AI Risk Management Framework</li> <li>IEEE 2857: Privacy engineering for AI</li> <li>Model Cards: Google's model documentation standard</li> </ul>"},{"location":"product/data-card/#related-documentation","title":"Related Documentation","text":"<ul> <li>Model Card: GFPGAN Model Card</li> <li>Security Policy: Security guidelines</li> <li>Privacy Policy: Data privacy practices</li> <li>Ethical Guidelines: AI ethics framework</li> </ul> <p>Last Updated: September 2024 Next Review: December 2024 Status: Under development - comprehensive data analysis in progress</p> <p>\u26a0\ufe0f Important Notice: This data card is incomplete and under active development. Many critical details about the training data require investigation and documentation. We are committed to improving transparency and welcome community input to make this documentation more complete and accurate.</p>"},{"location":"product/model-card/","title":"Model Card: GFPGAN","text":""},{"location":"product/model-card/#model-overview","title":"Model Overview","text":"<p>Model Name: GFPGAN (Generative Facial Prior GAN) Version: 1.4 Model Type: Generative Adversarial Network for face restoration License: Apache 2.0</p>"},{"location":"product/model-card/#intended-use","title":"Intended Use","text":""},{"location":"product/model-card/#primary-use-cases","title":"Primary Use Cases","text":"<ul> <li>Photo restoration: Enhance damaged, blurred, or low-quality face photos</li> <li>Image enhancement: Improve facial details in compressed or degraded images</li> <li>Historical photo recovery: Restore old or deteriorated photographs</li> <li>Content creation: Enhance facial quality in digital media</li> </ul>"},{"location":"product/model-card/#intended-users","title":"Intended Users","text":"<ul> <li>Photographers: Professional and amateur photo enhancement</li> <li>Archivists: Digital preservation of historical photographs</li> <li>Content creators: Video and image post-production</li> <li>Researchers: Computer vision and image processing studies</li> </ul>"},{"location":"product/model-card/#out-of-scope-uses","title":"Out-of-Scope Uses","text":"<p>\u274c Not intended for: - Real-time video processing (performance limitations) - Non-facial image enhancement (specialized for faces) - Identity modification or deepfake creation - Medical diagnosis or analysis - Surveillance or law enforcement identification</p>"},{"location":"product/model-card/#model-details","title":"Model Details","text":""},{"location":"product/model-card/#architecture","title":"Architecture","text":"<ul> <li>Base Model: StyleGAN2 generator with facial priors</li> <li>Training Framework: PyTorch with custom loss functions</li> <li>Input Resolution: 512x512 pixels (faces automatically detected and cropped)</li> <li>Output Resolution: 512x512 to 2048x2048 (depending on upscale factor)</li> </ul>"},{"location":"product/model-card/#model-versions","title":"Model Versions","text":"Version Release Date Key Features Model Size v1.4 2024-Q4 Best identity preservation ~348MB v1.3 2024-Q2 Improved texture quality ~348MB v1.2 2024-Q1 Enhanced stability ~348MB"},{"location":"product/model-card/#training-data","title":"Training Data","text":"<p>\u26a0\ufe0f TODO: Detailed training data information needed</p> <ul> <li>Dataset composition: [NEEDS DOCUMENTATION]</li> <li>Data sources: [NEEDS DOCUMENTATION]</li> <li>Number of images: [NEEDS DOCUMENTATION]</li> <li>Demographics: [NEEDS ANALYSIS]</li> <li>Geographic coverage: [NEEDS ANALYSIS]</li> </ul>"},{"location":"product/model-card/#performance-and-limitations","title":"Performance and Limitations","text":""},{"location":"product/model-card/#model-performance","title":"Model Performance","text":""},{"location":"product/model-card/#quantitative-metrics","title":"Quantitative Metrics","text":"<p>\u26a0\ufe0f TODO: Comprehensive evaluation needed</p> Metric GFPGAN v1.4 GFPGAN v1.3 Baseline LPIPS \u2193 [TODO] [TODO] [TODO] DISTS \u2193 [TODO] [TODO] [TODO] ArcFace Similarity \u2191 [TODO] [TODO] [TODO] Processing Time (GPU) ~2-3s ~1-2s -"},{"location":"product/model-card/#qualitative-assessment","title":"Qualitative Assessment","text":"<p>\u2705 Strengths: - Excellent identity preservation - Natural-looking texture generation - Robust to various degradation types - Maintains facial structure and features</p> <p>\u26a0\ufe0f Limitations: - Performance degrades with extreme degradation - May struggle with very small faces (&lt;64px) - Requires GPU for reasonable performance - Limited to frontal and near-frontal faces</p>"},{"location":"product/model-card/#known-biases-and-fairness","title":"Known Biases and Fairness","text":"<p>\u26a0\ufe0f TODO: Comprehensive bias analysis needed</p>"},{"location":"product/model-card/#demographic-performance","title":"Demographic Performance","text":"<ul> <li>Age groups: [NEEDS ANALYSIS]</li> <li>Gender representation: [NEEDS ANALYSIS]</li> <li>Ethnic diversity: [NEEDS ANALYSIS]</li> <li>Skin tone coverage: [NEEDS ANALYSIS]</li> </ul>"},{"location":"product/model-card/#mitigation-strategies","title":"Mitigation Strategies","text":"<ul> <li>Regular bias auditing planned</li> <li>Diverse evaluation datasets in development</li> <li>Community feedback collection for bias reporting</li> </ul>"},{"location":"product/model-card/#technical-limitations","title":"Technical Limitations","text":""},{"location":"product/model-card/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>Minimum GPU: 4GB VRAM for basic operation</li> <li>Recommended GPU: 8GB+ VRAM for optimal performance</li> <li>CPU fallback: Available but significantly slower (10-50x)</li> </ul>"},{"location":"product/model-card/#input-constraints","title":"Input Constraints","text":"<ul> <li>Face size: Minimum 32x32 pixels for detection</li> <li>Image formats: JPG, PNG, WebP, BMP</li> <li>Maximum resolution: 4K (auto-resized if larger)</li> <li>Face orientation: Works best with frontal faces</li> </ul>"},{"location":"product/model-card/#ethical-considerations","title":"Ethical Considerations","text":""},{"location":"product/model-card/#responsible-ai-principles","title":"Responsible AI Principles","text":""},{"location":"product/model-card/#transparency","title":"Transparency","text":"<ul> <li>Open-source implementation and weights</li> <li>Documented limitations and failure cases</li> <li>Clear usage guidelines and best practices</li> </ul>"},{"location":"product/model-card/#fairness","title":"Fairness","text":"<ul> <li>\u26a0\ufe0f TODO: Bias evaluation across demographic groups</li> <li>Commitment to addressing identified biases</li> <li>Inclusive evaluation methodologies</li> </ul>"},{"location":"product/model-card/#privacy","title":"Privacy","text":"<ul> <li>Local processing (no cloud upload required)</li> <li>No data retention or telemetry by default</li> <li>User control over all processed images</li> </ul>"},{"location":"product/model-card/#accountability","title":"Accountability","text":"<ul> <li>Clear documentation of intended uses</li> <li>Guidance on inappropriate applications</li> <li>Community reporting mechanisms</li> </ul>"},{"location":"product/model-card/#potential-risks","title":"Potential Risks","text":""},{"location":"product/model-card/#misuse-scenarios","title":"Misuse Scenarios","text":"<p>\u26a0\ufe0f High Risk: - Identity manipulation: Creating misleading enhanced photos - Deepfake preparation: Using enhanced faces for synthetic media - Non-consensual enhancement: Processing photos without permission</p> <p>\u26a0\ufe0f Medium Risk: - Historical revisionism: Inappropriately \"correcting\" historical photos - Surveillance enhancement: Improving low-quality surveillance footage - Bias amplification: Reinforcing beauty standards or demographic preferences</p>"},{"location":"product/model-card/#risk-mitigation","title":"Risk Mitigation","text":"<ul> <li>Clear documentation of appropriate uses</li> <li>Community guidelines and reporting</li> <li>Technical limitations to prevent real-time abuse</li> <li>Education about ethical implications</li> </ul>"},{"location":"product/model-card/#evaluation-and-validation","title":"Evaluation and Validation","text":""},{"location":"product/model-card/#test-datasets","title":"Test Datasets","text":"<p>\u26a0\ufe0f TODO: Standardized evaluation datasets needed</p> <ul> <li>CelebA-HQ: [RESULTS NEEDED]</li> <li>FFHQ: [RESULTS NEEDED]</li> <li>Helen: [RESULTS NEEDED]</li> <li>Custom benchmark: [UNDER DEVELOPMENT]</li> </ul>"},{"location":"product/model-card/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"product/model-card/#technical-quality","title":"Technical Quality","text":"<ul> <li>LPIPS: Perceptual similarity measurement</li> <li>DISTS: Image structural similarity</li> <li>SSIM: Structural similarity index</li> <li>PSNR: Peak signal-to-noise ratio</li> </ul>"},{"location":"product/model-card/#identity-preservation","title":"Identity Preservation","text":"<ul> <li>ArcFace: Identity similarity scores</li> <li>FaceNet: Feature distance measurement</li> <li>Human evaluation: Perceptual identity studies</li> </ul>"},{"location":"product/model-card/#fairness-evaluation","title":"Fairness Evaluation","text":"<ul> <li>\u26a0\ufe0f TODO: Demographic parity analysis</li> <li>\u26a0\ufe0f TODO: Equal opportunity assessment</li> <li>\u26a0\ufe0f TODO: Individual fairness evaluation</li> </ul>"},{"location":"product/model-card/#human-evaluation","title":"Human Evaluation","text":"<p>\u26a0\ufe0f TODO: User study needed</p> <ul> <li>Quality assessment: Professional photographer evaluation</li> <li>Identity preservation: Human rater studies</li> <li>Bias detection: Diverse evaluator panels</li> <li>Use case validation: Target user feedback</li> </ul>"},{"location":"product/model-card/#environmental-impact","title":"Environmental Impact","text":""},{"location":"product/model-card/#carbon-footprint","title":"Carbon Footprint","text":"<p>\u26a0\ufe0f TODO: Environmental impact assessment needed</p> <ul> <li>Training emissions: [NEEDS CALCULATION]</li> <li>Inference efficiency: ~2-3 seconds per image on modern GPUs</li> <li>Hardware efficiency: Optimized for consumer GPUs</li> </ul>"},{"location":"product/model-card/#sustainability-measures","title":"Sustainability Measures","text":"<ul> <li>Model optimization for efficiency</li> <li>Support for various hardware configurations</li> <li>Local processing to reduce data transfer</li> <li>Open-source to prevent duplicate training</li> </ul>"},{"location":"product/model-card/#model-governance","title":"Model Governance","text":""},{"location":"product/model-card/#version-control","title":"Version Control","text":"<ul> <li>Semantic versioning: Clear versioning for compatibility</li> <li>Model registry: Centralized model distribution</li> <li>Reproducibility: Locked dependency versions</li> <li>Rollback capability: Previous versions maintained</li> </ul>"},{"location":"product/model-card/#monitoring-and-updates","title":"Monitoring and Updates","text":"<ul> <li>Performance monitoring: Automated quality checks</li> <li>Bias monitoring: Regular fairness evaluations</li> <li>Security updates: Vulnerability patching</li> <li>Community feedback: Issue tracking and resolution</li> </ul>"},{"location":"product/model-card/#access-and-distribution","title":"Access and Distribution","text":"<ul> <li>Open access: Free download for research and development</li> <li>Commercial use: Permitted under Apache 2.0 license</li> <li>Distribution channels: GitHub releases and model hubs</li> <li>Version compatibility: Backward compatibility guarantees</li> </ul>"},{"location":"product/model-card/#contact-and-feedback","title":"Contact and Feedback","text":""},{"location":"product/model-card/#reporting-issues","title":"Reporting Issues","text":"<ul> <li>Technical issues: GitHub Issues</li> <li>Ethical concerns: ethics@gfpgan.ai</li> <li>Security vulnerabilities: Security Policy</li> <li>Bias reports: bias@gfpgan.ai</li> </ul>"},{"location":"product/model-card/#contributing","title":"Contributing","text":"<ul> <li>Model improvements: Community contributions welcome</li> <li>Evaluation data: Diverse evaluation datasets needed</li> <li>Bias testing: Fairness evaluation contributions</li> <li>Documentation: Help improve this model card</li> </ul>"},{"location":"product/model-card/#references-and-citations","title":"References and Citations","text":""},{"location":"product/model-card/#academic-citations","title":"Academic Citations","text":"<pre><code>@InProceedings{wang2021gfpgan,\n    author = {Xintao Wang and Yu Li and Honglun Zhang and Ying Shan},\n    title = {Towards Real-World Blind Face Restoration with Generative Facial Prior},\n    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n    year = {2021}\n}\n</code></pre>"},{"location":"product/model-card/#related-work","title":"Related Work","text":"<ul> <li>StyleGAN2: Foundation architecture for face generation</li> <li>Real-ESRGAN: Background enhancement technology</li> <li>ArcFace: Identity preservation evaluation</li> <li>Face detection: MTCNN and RetinaFace integration</li> </ul> <p>Last Updated: September 2024 Next Review: December 2024 Status: Active development - some evaluations pending</p> <p>\u26a0\ufe0f Note: This model card is under active development. Sections marked with \"TODO\" require additional research and evaluation. We welcome community contributions to improve the completeness and accuracy of this documentation.</p>"},{"location":"usage/api/","title":"API Usage (FastAPI)","text":"<ul> <li>Install extras: <code>pip install -e .[api]</code></li> <li>Run locally: <code>gfpgan-api</code> (serves on <code>http://127.0.0.1:8000</code>)</li> <li>Health: <code>GET /healthz</code></li> <li>Restore: <code>POST /restore</code> (multipart/form-data)</li> <li>form field <code>files</code>: one or more images</li> <li>query params: <code>version=1.4</code>, <code>upscale=2</code>, <code>backend=gfpgan</code>, <code>device=auto</code>, <code>dry_run=true</code></li> </ul> <p>Notes - Default is <code>dry_run=true</code> for smoke-safety; set <code>dry_run=false</code> to perform actual restoration. - In non-dry-run, the server resolves weights via env/Hugging Face (see CLI docs) and runs the GFPGAN engine. - CI runs an API smoke check as part of the workflow.</p>"},{"location":"usage/cli/","title":"CLI Usage","text":"<pre><code>usage: gfpgan-infer [-i INPUT] [-o OUTPUT] [-v VERSION] [-s UPSCALE]\n                    [--bg_upsampler {realesrgan,none}] [--bg_tile BG_TILE]\n                    [--bg_precision {auto,fp16,fp32}] [--backend {gfpgan,restoreformer,codeformer}]\n                    [--suffix SUFFIX] [--only_center_face] [--aligned]\n                    [--ext EXT] [-w WEIGHT] [--sweep-weight SWEEP_WEIGHT]\n                    [--jpg-quality JPG_QUALITY] [--png-compress PNG_COMPRESS]\n                    [--webp-quality WEBP_QUALITY]\n                    [--detector {retinaface_resnet50,retinaface_mobile0.25,scrfd}] [--no-parse]\n                    [--device {auto,cpu,cuda}] [--dry-run] [--no-download]\n                    [--model-path MODEL_PATH] [--seed SEED] [--no-cmp]\n                    [--manifest MANIFEST] [--compile {none,default,max}] [--metrics {none,id,lpips,both}] [--metrics-out METRICS_OUT] [--print-env]\n                    [--deterministic-cuda] [--eye-dist-threshold EYE_DIST_THRESHOLD]\n                    [--max-images MAX_IMAGES] [--skip-existing] [--workers WORKERS]\n                    [--auto] [--auto-hw] [--select-by {sharpness,identity}]\n                    [--verbose]\n</code></pre> <ul> <li>Common examples</li> <li><code>gfpgan-infer -i inputs/whole_imgs -o results -v 1.4 -s 2 --device auto</code></li> <li><code>gfpgan-infer -i \"inputs/whole_imgs/*.png\" -v 1.3 --bg_upsampler none</code></li> <li><code>gfpgan-infer -i my.jpg -v 1.3 --no-download --model-path ./gfpgan/weights/GFPGANv1.3.pth</code></li> <li><code>gfpgan-infer --dry-run -v 1.4 --verbose</code> (validate and exit)</li> <li>Weight sweep: <code>gfpgan-infer -i img.jpg -v 1.4 --sweep-weight 0.3,0.5,0.7 --manifest out.json</code></li> <li>Deterministic CUDA: <code>gfpgan-infer -i img.jpg -v 1.4 --deterministic-cuda --seed 123</code></li> <li>Torch compile: <code>gfpgan-infer -i img.jpg -v 1.4 --compile default</code></li> <li>Metrics: <code>gfpgan-infer -i img.jpg -v 1.4 --metrics both --metrics-out results/metrics.json</code></li> <li>Quality controls: <code>--jpg-quality 95 --png-compress 3 --webp-quality 90</code></li> <li>Autopilot: <code>gfpgan-infer -i img.jpg -v 1.4 --auto --select-by sharpness</code></li> <li> <p>Hardware-aware defaults: <code>gfpgan-infer -i img.jpg -v 1.3 --auto-hw</code></p> </li> <li> <p>Notes</p> </li> <li>On CPU, Real-ESRGAN background upsampling is disabled automatically.</li> <li>Use <code>--bg_precision fp32</code> to force full precision, or <code>fp16</code> to half on CUDA.</li> <li><code>--seed</code> sets seeds for random, numpy, and torch.</li> <li>Use <code>--detector</code> to switch facexlib detectors; <code>--no-parse</code> disables face parsing.</li> <li>Backends: choose <code>--backend gfpgan|restoreformer|restoreformerpp|codeformer</code>. RestoreFormer uses built-in arch; <code>restoreformerpp</code> aliases to RestoreFormer for now; CodeFormer requires installing its repo.</li> <li><code>--manifest out.json</code> writes a manifest of inputs and outputs for automation.</li> <li><code>--skip-existing</code> avoids re-writing outputs; <code>--max-images</code> caps processing.</li> <li><code>--print-env</code> prints torch/torchvision/basicsr/facexlib versions and CUDA availability.</li> <li><code>--workers N</code> enables experimental CPU-only parallelization across images (spawns multiple processes).</li> <li>Quality controls apply by file extension (jpg/png/webp) when saving outputs.</li> <li><code>--auto</code> tries a couple of model/weight combos and picks the best by <code>--select-by</code>.</li> <li><code>--auto-hw</code> sets sensible tiles/precision/workers based on detected hardware.</li> </ul> <p>Model weights - Download utility: <code>gfpgan-download-weights --list</code> to view; <code>gfpgan-download-weights -v 1.4</code> to fetch. - Destination: downloads to <code>gfpgan/weights/</code> by default. Set <code>GFPGAN_WEIGHTS_DIR</code> to override. - The runtime can resolve weights from Hugging Face Hub if <code>GFPGAN_HF_REPO</code> is defined. Set <code>HF_HUB_OFFLINE=1</code> for cache-only.</p>"},{"location":"usage/colab/","title":"Colab Guide","text":"<p>Open the notebook:</p> <ul> <li>GFPGAN Colab Demo</li> </ul> <p>Features - Install cell sets up Torch + Basicsr master (for torchvision compatibility) - Interactive UI for:   - Uploading images   - Fetching images from URLs   - Optional Drive mount   - Selecting version, upscale, weight, and options   - Running inference and previewing results   - ZIP download of the results directory - First-image before/after slider when original is available</p> <p>Tips - Use a GPU runtime to enable background upsampling (Real-ESRGAN) and for speed. - The notebook prints versions of Torch, Torchvision, and Basicsr to help debugging.</p>"},{"location":"usage/colab/#ci-smoke-mode","title":"CI Smoke Mode","text":"<ul> <li>The Colab notebook supports a lightweight smoke run for CI.</li> <li>Set the environment variable <code>NB_CI_SMOKE=1</code> to skip heavy installation, cloning, and inference cells.</li> <li>Example (local):</li> <li><code>NB_CI_SMOKE=1 pytest -c /dev/null --nbmake --nbmake-kernel=python3 --ignore=tests notebooks/GFPGAN_Colab.ipynb</code></li> </ul>"},{"location":"usage/gradio/","title":"Local Gradio App","text":"<p>Run a lightweight local UI for GFPGAN.</p> <ul> <li>Install (editable): <code>pip install -e .[dev]</code></li> <li>Launch: <code>gfpgan-gradio --server-port 7860 --share</code></li> <li>Options: <code>--server-name 0.0.0.0</code> (default), <code>--share</code> for a public link.</li> </ul> <p>Features - Upload multiple images, pick version, device (auto/cpu/cuda), upscale, weight. - Choose detector, enable/disable face parsing. - Optional background upsampler with precision and tile controls (GPU). - Displays restored images and device info.</p> <p>Notes - The app lazily loads models and uses the same inference logic as the CLI. - For better results, download weights first: <code>gfpgan-download-weights -v 1.4</code>.</p>"},{"location":"usage/gradio/#docker-local-app","title":"Docker (local app)","text":"<p>Build and run the Gradio app via Docker (CPU):</p> <ul> <li>Build: <code>docker build -t gfpgan-app .</code></li> <li>Run: <code>docker run --rm -p 7860:7860 gfpgan-app</code></li> </ul> <p>Then open http://localhost:7860.</p> <p>Notes - The image installs Torch 2.x CPU wheels and BasicSR master for compatibility. - For GPU, consider a CUDA base image and matching Torch wheels (not included here).</p>"},{"location":"usage/recipes/","title":"CLI Recipes","text":"<ul> <li>CPU batch of images, disable background, cap to 20 images</li> <li> <p><code>gfpgan-infer -i inputs/whole_imgs -o results -v 1.4 --device cpu --bg_upsampler none --max-images 20</code></p> </li> <li> <p>GPU quality run with weight sweep and manifest</p> </li> <li> <p><code>gfpgan-infer -i my.jpg -o results -v 1.4 --device cuda --sweep-weight 0.3,0.5,0.7 --manifest results/manifest.json</code></p> </li> <li> <p>Deterministic CUDA with seed (potentially slower)</p> </li> <li> <p><code>gfpgan-infer -i my.jpg -v 1.4 --device cuda --deterministic-cuda --seed 123</code></p> </li> <li> <p>Change detector and parsing behavior</p> </li> <li> <p><code>gfpgan-infer -i img.jpg -v 1.3 --detector scrfd --no-parse</code></p> </li> <li> <p>Print environment versions and run</p> </li> <li><code>gfpgan-infer -i img.jpg -v 1.4 --print-env --verbose</code></li> </ul>"},{"location":"usage/recipes/#performance-autopilot-tips","title":"Performance &amp; Autopilot Tips","text":"<ul> <li>Autopilot (<code>--auto</code>):</li> <li>For common photos, try: <code>--auto --select-by sharpness</code>.</li> <li>For portraits where identity matters: <code>--auto --select-by identity</code> (falls back to sharpness if identity backend is unavailable).</li> <li> <p>Autopilot tries a small set of model/weight combos (e.g., 1.2/1.3 with 0.3/0.5) and picks the best by the metric on the restored image.</p> </li> <li> <p>Hardware-aware defaults (<code>--auto-hw</code>):</p> </li> <li>On CUDA: sets <code>--bg_precision fp16</code> and tiles based on VRAM (0/600/400 for high/med/low VRAM).</li> <li> <p>On CPU: sets a sensible <code>--workers</code> value up to 4.</p> </li> <li> <p>CPU concurrency (<code>--workers N</code>):</p> </li> <li>Parallelizes images across processes; each worker maintains its own restorer.</li> <li> <p>Start with 2\u20134 workers depending on core count and memory; avoid high values.</p> </li> <li> <p>Quality/perf trade-offs:</p> </li> <li>Increase <code>--bg_tile</code> or disable background (<code>--bg_upsampler none</code>) if you encounter OOM.</li> <li>Lower <code>--upscale</code> for faster throughput on large batches.</li> </ul>"}]}