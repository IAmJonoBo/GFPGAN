{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility imports used throughout the notebook\n",
    "from PIL import Image  # noqa: F401\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFPGAN Colab Demo (Fork)\n",
    "\n",
    "This Colab notebook installs dependencies, clones this fork, downloads pretrained weights automatically, and runs the GFPGAN inference script on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d9709",
   "metadata": {
    "id": "env-setup"
   },
   "outputs": [],
   "source": [
    "#@title Install dependencies\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def run(cmd, shell=False, check=True):\n",
    "    print(\">>\", cmd if shell else \" \".join(cmd))\n",
    "    if shell:\n",
    "        return subprocess.run(cmd, shell=True, check=check)\n",
    "    else:\n",
    "        return subprocess.run(cmd, check=check)\n",
    "\n",
    "print(sys.version)\n",
    "\n",
    "# CI smoke: robust env var parsing (treat '0'/'false' as False)\n",
    "_ci_val = os.environ.get(\"NB_CI_SMOKE\", \"\").strip().lower()\n",
    "CI_SMOKE = _ci_val in (\"1\", \"true\", \"yes\", \"y\")\n",
    "if CI_SMOKE:\n",
    "    print(\"NB_CI_SMOKE=1: skipping dependency install step\")\n",
    "\n",
    "if not CI_SMOKE:\n",
    "    # Upgrade pip tooling quietly\n",
    "    run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--quiet\", \"pip\", \"setuptools\", \"wheel\"])\n",
    "    # Ensure IPython deps are satisfied to avoid resolver warnings\n",
    "    run([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"-U\", \"ipython\", \"jedi\"])\n",
    "\n",
    "    # Install torch pinned to repo-compatible versions\n",
    "    has_nvidia = shutil.which(\"nvidia-smi\") is not None\n",
    "    torch_spec = \"torch==2.1.*\"\n",
    "    vision_spec = \"torchvision==0.16.*\"\n",
    "    audio_spec = \"torchaudio==2.1.*\"\n",
    "    if has_nvidia:\n",
    "        print(\"Detected NVIDIA GPU; installing CUDA wheels (cu121)\")\n",
    "        run([\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--quiet\",\n",
    "            torch_spec,\n",
    "            vision_spec,\n",
    "            audio_spec,\n",
    "            \"--index-url\",\n",
    "            \"https://download.pytorch.org/whl/cu121\",\n",
    "        ])\n",
    "    else:\n",
    "        print(\"No NVIDIA GPU; installing CPU wheels\")\n",
    "        run([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", torch_spec, vision_spec, audio_spec])\n",
    "\n",
    "    # Core deps: match repo constraints (basicsr <= 1.4.2)\n",
    "    run(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--quiet\",\n",
    "            \"--no-cache-dir\",\n",
    "            \"--upgrade\",\n",
    "            \"basicsr<=1.4.2\",\n",
    "            \"facexlib\",\n",
    "            \"realesrgan\",\n",
    "            \"opencv-python\",\n",
    "            \"tqdm\",\n",
    "            \"numpy\",\n",
    "            \"PyYAML\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56a79c",
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "REPO_URL = \"https://github.com/IAmJonoBo/Restoria.git\"\n",
    "REPO_DIR = \"GFPGAN\"  # Keep folder name for compatibility with notebook paths\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"Removing existing {REPO_DIR} directory...\")\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "\n",
    "print(f\"Cloning repository from {REPO_URL}...\")\n",
    "subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\n",
    "print(\"Clone complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bced545",
   "metadata": {
    "id": "run-inference"
   },
   "outputs": [],
   "source": [
    "#@title Run inference on sample images\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# CI smoke: skip heavy inference when NB_CI_SMOKE is set\n",
    "if os.environ.get(\"NB_CI_SMOKE\"):\n",
    "    print(\"NB_CI_SMOKE=1: skipping sample inference step\")\n",
    "else:\n",
    "    repo_dir = \"GFPGAN\"\n",
    "    cmd = [sys.executable, \"inference_gfpgan.py\", \"-i\", \"inputs/whole_imgs\", \"-o\", \"results\", \"-v\", \"1.4\", \"-s\", \"2\"]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    res = subprocess.run(cmd, cwd=repo_dir, check=False)\n",
    "    print(\"Exit code:\", res.returncode)\n",
    "    print(\"Results written to ./GFPGAN/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b7758",
   "metadata": {
    "id": "display-results"
   },
   "outputs": [],
   "source": [
    "# @title Display a few restored images\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# CI smoke: do not try to display images in headless CI\n",
    "if os.environ.get(\"NB_CI_SMOKE\"):\n",
    "    print(\"NB_CI_SMOKE=1: skipping image display\")\n",
    "else:\n",
    "    base = \"GFPGAN/results/restored_imgs\"\n",
    "    imgs = sorted(glob.glob(os.path.join(base, \"*\")))[:4]\n",
    "    print(f\"Displaying {len(imgs)} images from\", base)\n",
    "    for p in imgs:\n",
    "        try:\n",
    "            display(Image(filename=p))\n",
    "        except Exception:\n",
    "            print(\"Could not display:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b78da",
   "metadata": {
    "id": "upload-run"
   },
   "outputs": [],
   "source": [
    "# @title Upload your own images and run GFPGAN\n",
    "# @markdown Upload one or more images; they will be processed with the settings below.\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Guard: if not in Colab, skip gracefully; also skip in CI smoke\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if os.environ.get(\"NB_CI_SMOKE\"):\n",
    "    print(\"NB_CI_SMOKE=1: skipping upload/inference UI\")\n",
    "else:\n",
    "    upload_dir = \"GFPGAN/uploads\"\n",
    "    os.makedirs(upload_dir, exist_ok=True)\n",
    "\n",
    "    if IN_COLAB:\n",
    "        uploaded = files.upload()\n",
    "        for name, data in uploaded.items():\n",
    "            with open(name, \"wb\") as f:\n",
    "                f.write(data)\n",
    "            shutil.move(name, os.path.join(upload_dir, os.path.basename(name)))\n",
    "    else:\n",
    "        print(\"Not running in Colab; skipping upload UI.\")\n",
    "\n",
    "    # Inference parameters\n",
    "    version = \"1.4\"  # @param ['1', '1.2', '1.3', '1.4']\n",
    "    scale = 2  # @param {type: 'integer'}\n",
    "    only_center_face = False  # @param {type: 'boolean'}\n",
    "    aligned = False  # @param {type: 'boolean'}\n",
    "    autopilot = False  # @param {type: 'boolean'}\n",
    "    hardware_aware = True  # @param {type: 'boolean'}\n",
    "    select_by = \"sharpness\"  # @param ['sharpness', 'identity']\n",
    "\n",
    "    print(\"Running inference on uploads...\")\n",
    "    cmd = [sys.executable, \"inference_gfpgan.py\", \"-i\", \"uploads\", \"-o\", \"results\", \"-v\", str(version), \"-s\", str(scale)]\n",
    "    if only_center_face:\n",
    "        cmd.append(\"--only_center_face\")\n",
    "    if aligned:\n",
    "        cmd.append(\"--aligned\")\n",
    "    if autopilot:\n",
    "        cmd.append(\"--auto\")\n",
    "        cmd.extend([\"--select-by\", select_by])\n",
    "    if hardware_aware:\n",
    "        cmd.append(\"--auto-hw\")\n",
    "    subprocess.run(cmd, cwd=\"GFPGAN\", check=False)\n",
    "    print(\"Done. See GFPGAN/results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- GPU acceleration is optional; uncomment the CUDA wheel index in the install cell to use GPU on Colab.\n",
    "- The inference script will automatically download the GFPGAN v1.4 weights if not present.\n",
    "- For large batches, consider enabling the Real-ESRGAN background upsampler by keeping default settings (it is auto-disabled on CPU)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
