{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GFPGAN (Fork) \u2014 Documentation","text":"<p>Welcome to the fork documentation. This fork focuses on modern developer ergonomics, a smoother Colab experience, and practical inference.</p> <ul> <li>What\u2019s new vs upstream:</li> <li>Modern CI and linting</li> <li>Colab with interactive UI and compatibility fixes (BasicSR master + modern torchvision)</li> <li>CLI quality-of-life flags and console entrypoint</li> <li>Safer repository defaults and optional light tests</li> </ul> <p>Quick links - Quickstart: ./quickstart.md - CLI Usage: ./usage/cli.md - Colab Guide: ./usage/colab.md - Compatibility Matrix: ./COMPATIBILITY.md - Contributing: ./contributing.md</p> <p>Upstream reference: https://github.com/TencentARC/GFPGAN</p>"},{"location":"COMPATIBILITY/","title":"Compatibility Matrix","text":"<p>This fork supports two primary tracks to balance stability and modern stacks.</p> <ul> <li>Torch 1.x (stable, Python 3.10): use <code>extras = [dev, torch1]</code> or <code>-c constraints-3.10-compat.txt</code>.</li> <li>Torch 2.x (modern, Python 3.11+): use <code>extras = [dev, torch2]</code>. Install BasicSR from master if needed.</li> </ul> <p>Known-good combinations - Python 3.10 + Torch 1.13.1 + torchvision 0.14.1 + basicsr 1.4.2 - Python 3.11 + Torch 2.4.1 + torchvision 0.19.1 + basicsr (master)</p> <p>Notes - Basicsr 1.4.2 uses <code>torchvision.transforms.functional_tensor</code> which was removed in torchvision 0.15+. For Torch 2.x stacks, install BasicSR from GitHub master. - CPU runs disable Real-ESRGAN background upsampling by default for speed. - Apple Silicon: prefer Python 3.10 + constraints for prebuilt wheels; Torch 2.x on arm64 is improving but may pull heavier builds.</p> <p>Quick recipes - 3.10 stable (Torch 1.x): <code>scripts/setup_uv.sh --python 3.10 --track torch1</code> - 3.11 modern (Torch 2.x): <code>scripts/setup_uv.sh --python 3.11 --track torch2</code> - Pip + constraints (3.10): <code>pip install -e .[dev] -c constraints-3.10-compat.txt</code></p>"},{"location":"DEV_ENV/","title":"DEV ENV","text":"<p>Development Environment (uv)</p> <p>Overview - Uses uv for fast, reproducible Python environments and locking. - Default target: Python 3.10 with a CPU-only stack compatible with BasicSR 1.4.2.</p> <p>Prereqs - Install uv: macOS <code>brew install uv</code>, Linux <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code>. - Python 3.10 recommended. 3.11+ is experimental (see below).</p> <p>Quick Start (3.11, Torch 2.x default) 1) Sync deps (with dev tools):    - <code>scripts/setup_uv.sh --python 3.11 --track torch2</code> 2) Run tests:    - <code>scripts/test.sh</code> 3) Lint / format:    - <code>scripts/lint.sh</code>    - <code>scripts/fmt.sh</code></p> <p>Notes - The runtime depends on <code>torch</code> and <code>torchvision</code>. For compatibility with <code>basicsr==1.4.2</code>, we supply a Torch 1.x extra (<code>torch1</code>) and version markers to keep NumPy/Skimage/OpenCV in a compatible range for Python 3.10. - If you install via pip directly, you can use the constraints file: <code>pip install -r requirements.txt -c constraints-3.10-compat.txt</code>.</p> <p>Python 3.10 (Torch 1.x track; stable) - For maximum compatibility with <code>basicsr==1.4.2</code> and CPU-only usage:   - <code>scripts/setup_uv.sh --python 3.10 --track torch1</code>   - Or with pip + constraints: <code>pip install -r requirements.txt -c constraints-3.10-compat.txt</code></p> <p>Torch 2.x notes - Torch 2.x (3.11+) may require Basicsr master:   - <code>uv pip install --no-cache-dir --upgrade \"git+https://github.com/xinntao/BasicSR@master\"</code>   - Alternatively use <code>constraints-3.11-experimental.txt</code> with pip.</p> <p>Apple Silicon - All constraints are chosen to have prebuilt wheels on macOS arm64 where possible. If you hit build issues, ensure you\u2019re on Python 3.10 and use the 3.10 constraints.</p>"},{"location":"TORCH2/","title":"TORCH2","text":"<p>Torch 2.x Migration Path (Experimental)</p> <p>Goals - Support Python 3.11+ and Torch/Torchvision 2.x while keeping the stable 3.10 + Torch 1.x path intact.</p> <p>Status - GFPGAN code imports from <code>torchvision.ops</code> and <code>torchvision.transforms.functional</code>, which are compatible with 0.17\u20130.19. - Known blocker was in <code>basicsr==1.4.2</code> using <code>torchvision.transforms.functional_tensor</code> removed in torchvision 0.15+. Upstream master addresses this.</p> <p>How to Try 1) Create a uv env for Python 3.11 with Torch 2.x extra:    - <code>uv sync -p 3.11 -E dev -E torch2</code> 2) If you hit Basicsr import errors, install master:    - <code>uv pip install --no-cache-dir --upgrade \"git+https://github.com/xinntao/BasicSR@master\"</code> 3) Run tests:    - <code>uv run pytest -q</code></p> <p>Notes - Some torchvision ops (e.g., <code>roi_align</code>) rely on compiled C++/CUDA ops. CPU wheels include C++ kernels; CUDA requires matching CUDA wheels. - If you need CUDA, install the appropriate Torch/Torchvision CUDA wheels and re-sync the environment. - If downstream regressions appear (numerics, tolerance), we can add conditional branches or loosen test tolerances for the Torch 2.x track.</p> <p>Next Steps (if adopting Torch 2.x by default) - Change the default scripts to <code>--track torch2</code> and set CI matrix to Python 3.11. - Remove Torch 1.x constraints when Basicsr master (or a tagged release) is required and stable.</p>"},{"location":"contributing/","title":"Contributing (Fork)","text":"<p>Thanks for considering a contribution! This fork aims to remain close to upstream while improving practical usage.</p> <ul> <li>Dev setup</li> <li><code>pip install -e .[dev]</code></li> <li>Enable pre-commit hooks: <code>pre-commit install</code></li> <li>Lint locally: <code>ruff check . &amp;&amp; black .</code></li> <li> <p>Run light tests: <code>pytest -q tests_light</code></p> </li> <li> <p>PR guidelines</p> </li> <li>Keep diffs focused and well-described</li> <li>Prefer small, reviewable changes</li> <li> <p>Add or update tests when changing behavior</p> </li> <li> <p>Syncing with upstream</p> </li> <li>We periodically pull from upstream <code>TencentARC/GFPGAN</code> and resolve conflicts.</li> </ul> <p>Code of Conduct: see <code>CODE_OF_CONDUCT.md</code>.</p>"},{"location":"faq/","title":"FAQ","text":"<ul> <li>Which Python/Torch should I use?</li> <li> <p>Python 3.11 with Torch 2.x is recommended. See docs/COMPATIBILITY.md for details.</p> </li> <li> <p>Why is background upsampling disabled on CPU?</p> </li> <li> <p>Real-ESRGAN is heavy on CPU and often slower than desired; this fork disables it by default on CPU for a smoother UX. Use GPU for best results.</p> </li> <li> <p>How can I run inference without downloads?</p> </li> <li> <p>Use <code>--no-download</code> and pass <code>--model-path /path/to/GFPGANv1.4.pth</code>.</p> </li> <li> <p>How do I quickly validate the CLI works locally?</p> </li> <li> <p><code>gfpgan-infer --dry-run -v 1.4</code> parses arguments and exits.</p> </li> <li> <p>Where is the docs site?</p> </li> <li>Once enabled via GitHub Pages on gh-pages, the site will be available at the repo Pages URL.</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":"<ul> <li>Install (editable):</li> <li><code>pip install -e .[dev]</code></li> <li>Download weights (optional, local cache):</li> <li>List: <code>gfpgan-download-weights --list</code></li> <li>Fetch v1.4: <code>gfpgan-download-weights -v 1.4</code></li> <li>Inference on CPU or GPU:</li> <li><code>gfpgan-infer --input inputs/whole_imgs --version 1.4 --upscale 2 --device auto</code></li> <li>Helpful flags:</li> <li><code>--dry-run</code> (validate args), <code>--no-download</code> (require local weights), <code>--model-path</code> (override weights)</li> <li><code>--bg_upsampler none</code> (disable background upsampling), <code>--bg_precision fp32|fp16|auto</code></li> <li><code>--detector scrfd|retinaface_*</code> (switch detector), <code>--no-parse</code> (disable parsing)</li> <li><code>--manifest outputs.json</code> to save a simple results manifest</li> <li><code>--sweep-weight 0.3,0.5,0.7</code> to run multiple weights</li> <li><code>--print-env</code> to display versions and CUDA availability</li> <li>Quality: <code>--jpg-quality 95</code>, <code>--png-compress 3</code>, <code>--webp-quality 90</code></li> <li>Colab notebook (UI):</li> <li>https://colab.research.google.com/github/IAmJonoBo/GFPGAN/blob/main/notebooks/GFPGAN_Colab.ipynb</li> </ul> <p>See: CLI Usage (./usage/cli.md) and Colab Guide (./usage/colab.md).</p> <p>Known good versions - Python 3.11 with Torch 2.x track (e.g., torch 2.4.1, torchvision 0.19.1) - Basicsr from master when using modern torchvision (CI/Colab default)</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<ul> <li>Basicsr import error with <code>functional_tensor</code></li> <li>Symptom: <code>ModuleNotFoundError: No module named 'torchvision.transforms.functional_tensor'</code></li> <li> <p>Fix: Install Basicsr from master (works with modern torchvision)</p> <ul> <li>Pip: <code>pip uninstall -y basicsr &amp;&amp; pip install --no-cache-dir --force-reinstall \"git+https://github.com/xinntao/BasicSR@master\"</code></li> <li>Colab: already handled in the install cell.</li> </ul> </li> <li> <p>Colab is slow or runs out of memory</p> </li> <li>Use a GPU runtime (Runtime \u2192 Change runtime type \u2192 GPU)</li> <li>Keep background upsampling disabled on CPU (default in this fork)</li> <li> <p>Reduce <code>--upscale</code> or batch size (fewer/lighter images)</p> </li> <li> <p>Heavy installs on CI/Colab</p> </li> <li>Prefer pinned Python (3.11) with Torch 2.x track</li> <li> <p>The tests (light) job avoids heavyweight tests and validates CLI and imports</p> </li> <li> <p>Deterministic results</p> </li> <li>Use <code>--seed</code> for reproducibility (random, numpy, torch)</li> <li> <p>Consider CPU for stricter determinism (or set CUDA deterministic options)</p> </li> <li> <p>Missing model weights</p> </li> <li>Use <code>--model-path</code> to point to local weights</li> <li>Or remove <code>--no-download</code> to allow fetching from the release URLs</li> </ul>"},{"location":"usage/cli/","title":"CLI Usage","text":"<pre><code>usage: gfpgan-infer [-i INPUT] [-o OUTPUT] [-v VERSION] [-s UPSCALE]\n                    [--bg_upsampler {realesrgan,none}] [--bg_tile BG_TILE]\n                    [--bg_precision {auto,fp16,fp32}]\n                    [--suffix SUFFIX] [--only_center_face] [--aligned]\n                    [--ext EXT] [-w WEIGHT] [--sweep-weight SWEEP_WEIGHT]\n                    [--jpg-quality JPG_QUALITY] [--png-compress PNG_COMPRESS]\n                    [--webp-quality WEBP_QUALITY]\n                    [--detector {retinaface_resnet50,retinaface_mobile0.25,scrfd}] [--no-parse]\n                    [--device {auto,cpu,cuda}] [--dry-run] [--no-download]\n                    [--model-path MODEL_PATH] [--seed SEED] [--no-cmp]\n                    [--manifest MANIFEST] [--print-env]\n                    [--deterministic-cuda] [--eye-dist-threshold EYE_DIST_THRESHOLD]\n                    [--max-images MAX_IMAGES] [--skip-existing] [--workers WORKERS]\n                    [--verbose]\n</code></pre> <ul> <li>Common examples</li> <li><code>gfpgan-infer -i inputs/whole_imgs -o results -v 1.4 -s 2 --device auto</code></li> <li><code>gfpgan-infer -i \"inputs/whole_imgs/*.png\" -v 1.3 --bg_upsampler none</code></li> <li><code>gfpgan-infer -i my.jpg -v 1.3 --no-download --model-path ./gfpgan/weights/GFPGANv1.3.pth</code></li> <li><code>gfpgan-infer --dry-run -v 1.4 --verbose</code> (validate and exit)</li> <li>Weight sweep: <code>gfpgan-infer -i img.jpg -v 1.4 --sweep-weight 0.3,0.5,0.7 --manifest out.json</code></li> <li>Deterministic CUDA: <code>gfpgan-infer -i img.jpg -v 1.4 --deterministic-cuda --seed 123</code></li> <li> <p>Quality controls: <code>--jpg-quality 95 --png-compress 3 --webp-quality 90</code></p> </li> <li> <p>Notes</p> </li> <li>On CPU, Real-ESRGAN background upsampling is disabled automatically.</li> <li>Use <code>--bg_precision fp32</code> to force full precision, or <code>fp16</code> to half on CUDA.</li> <li><code>--seed</code> sets seeds for random, numpy, and torch.</li> <li>Use <code>--detector</code> to switch facexlib detectors; <code>--no-parse</code> disables face parsing.</li> <li><code>--manifest out.json</code> writes a manifest of inputs and outputs for automation.</li> <li><code>--skip-existing</code> avoids re-writing outputs; <code>--max-images</code> caps processing.</li> <li><code>--print-env</code> prints torch/torchvision/basicsr/facexlib versions and CUDA availability.</li> <li><code>--workers N</code> enables experimental CPU-only parallelization across images (spawns multiple processes).</li> <li>Quality controls apply by file extension (jpg/png/webp) when saving outputs.</li> </ul> <p>Model weights - Download utility: <code>gfpgan-download-weights --list</code> to view; <code>gfpgan-download-weights -v 1.4</code> to fetch. - Destination: downloads to <code>gfpgan/weights/</code> by default.</p>"},{"location":"usage/colab/","title":"Colab Guide","text":"<p>Open the notebook: - https://colab.research.google.com/github/IAmJonoBo/GFPGAN/blob/main/notebooks/GFPGAN_Colab.ipynb</p> <p>Features - Install cell sets up Torch + Basicsr master (for torchvision compatibility) - Interactive UI for:   - Uploading images   - Fetching images from URLs   - Optional Drive mount   - Selecting version, upscale, weight, and options   - Running inference and previewing results   - ZIP download of the results directory - First-image before/after slider when original is available</p> <p>Tips - Use a GPU runtime to enable background upsampling (Real-ESRGAN) and for speed. - The notebook prints versions of Torch, Torchvision, and Basicsr to help debugging.</p>"},{"location":"usage/gradio/","title":"Local Gradio App","text":"<p>Run a lightweight local UI for GFPGAN.</p> <ul> <li>Install (editable): <code>pip install -e .[dev]</code></li> <li>Launch: <code>gfpgan-gradio --server-port 7860 --share</code></li> <li>Options: <code>--server-name 0.0.0.0</code> (default), <code>--share</code> for a public link.</li> </ul> <p>Features - Upload multiple images, pick version, device (auto/cpu/cuda), upscale, weight. - Choose detector, enable/disable face parsing. - Optional background upsampler with precision and tile controls (GPU). - Displays restored images and device info.</p> <p>Notes - The app lazily loads models and uses the same inference logic as the CLI. - For better results, download weights first: <code>gfpgan-download-weights -v 1.4</code>.</p>"},{"location":"usage/gradio/#docker-local-app","title":"Docker (local app)","text":"<p>Build and run the Gradio app via Docker (CPU):</p> <ul> <li>Build: <code>docker build -t gfpgan-app .</code></li> <li>Run: <code>docker run --rm -p 7860:7860 gfpgan-app</code></li> </ul> <p>Then open http://localhost:7860.</p> <p>Notes - The image installs Torch 2.x CPU wheels and BasicSR master for compatibility. - For GPU, consider a CUDA base image and matching Torch wheels (not included here).</p>"},{"location":"usage/recipes/","title":"CLI Recipes","text":"<ul> <li>CPU batch of images, disable background, cap to 20 images</li> <li> <p><code>gfpgan-infer -i inputs/whole_imgs -o results -v 1.4 --device cpu --bg_upsampler none --max-images 20</code></p> </li> <li> <p>GPU quality run with weight sweep and manifest</p> </li> <li> <p><code>gfpgan-infer -i my.jpg -o results -v 1.4 --device cuda --sweep-weight 0.3,0.5,0.7 --manifest results/manifest.json</code></p> </li> <li> <p>Deterministic CUDA with seed (potentially slower)</p> </li> <li> <p><code>gfpgan-infer -i my.jpg -v 1.4 --device cuda --deterministic-cuda --seed 123</code></p> </li> <li> <p>Change detector and parsing behavior</p> </li> <li> <p><code>gfpgan-infer -i img.jpg -v 1.3 --detector scrfd --no-parse</code></p> </li> <li> <p>Print environment versions and run</p> </li> <li><code>gfpgan-infer -i img.jpg -v 1.4 --print-env --verbose</code></li> </ul>"}]}